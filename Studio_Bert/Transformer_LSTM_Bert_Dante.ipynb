{"cells":[{"cell_type":"markdown","source":["## Pacchetti da installare"],"metadata":{"id":"yDKuSNBd92YI"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hJy-juNOpUOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681465949423,"user_tz":-120,"elapsed":24124,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"71eec7fe-2fd0-41f0-f114-3a153f4e737b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"1eALqJmsL4C_","executionInfo":{"status":"ok","timestamp":1681465952408,"user_tz":-120,"elapsed":2989,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U 'tensorflow-text==2.8.*'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NE4enZGpvMRX","outputId":"046b271c-fe52-48e4-9a46-3f3819c7bc71","executionInfo":{"status":"ok","timestamp":1681466023559,"user_tz":-120,"elapsed":71153,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q tf-models-official"],"metadata":{"id":"FPtWz_qHuofc","executionInfo":{"status":"ok","timestamp":1681466092265,"user_tz":-120,"elapsed":68713,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c47e700-88bd-493c-bbbd-409fc3712529"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.1/630.1 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["## Import notebook"],"metadata":{"id":"xXYm-Qqw-ANh"}},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":true,"id":"UaAiWsEuC_4K","executionInfo":{"status":"ok","timestamp":1681466096338,"user_tz":-120,"elapsed":4100,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"outputs":[],"source":["import os\n","import re\n","import datetime\n","import pathlib\n","import json\n","from pathlib import Path\n","'''\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","'''\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","import matplotlib.pyplot as plt\n","\n","from typing import Tuple\n"]},{"cell_type":"code","source":["tf.get_logger().setLevel('ERROR')\n","tf.config.run_functions_eagerly(True)"],"metadata":{"id":"uKEqRlKowOQS","executionInfo":{"status":"ok","timestamp":1681466096339,"user_tz":-120,"elapsed":17,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Variabili Globali"],"metadata":{"id":"HRe16D-rUBLA"}},{"cell_type":"code","source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'train_data.csv'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_data_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# PATH LOG Tensorboard\n","PATH_LOG = 'logs/fit/transformer_multi_bert_dante'\n","PATH_LOG = os.path.abspath(os.path.join(root_folder, PATH_LOG))\n","log_dir =  os.path.abspath(os.path.join(PATH_LOG, datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))) \n","log_history = os.path.abspath(os.path.join(PATH_LOG, 'histrory.json'))\n","\n","# PATH WEIGHTS Tensorboard\n","PATH_WEIGHTS = 'weights/transformer_multi_bert_dante'\n","PATH_WEIGHTS = os.path.abspath(os.path.join(root_folder, PATH_WEIGHTS))\n","checkpoint_path = os.path.abspath(os.path.join(PATH_WEIGHTS, 'cp.ckpt'))\n","\n","# VOCABOLARIO\n","vocab_folder = 'vocab'\n","multilingual_vocab_finalname = 'multilingual_vocab.txt'\n","ita_vocab_finalname = 'ita_dante_vocab.txt'\n","\n","VOCAB_PATH = os.path.abspath(os.path.join(root_folder, vocab_folder))\n","multilingual_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, multilingual_vocab_finalname))\n","ita_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, ita_vocab_finalname))\n","\n","# parametri per il modello\n","ORIGINAL_COLUMN = 'Original'\n","TRANSLATE_COLUMN = 'Translate'\n","TYPE_COLUMN ='Type'"],"metadata":{"id":"ewLgCIuEpczO","executionInfo":{"status":"ok","timestamp":1681466096339,"user_tz":-120,"elapsed":15,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Iper Parametri Modello"],"metadata":{"id":"LCiP6wT05k6j"}},{"cell_type":"code","source":["NUM_SAMPLES = 25000\n","TEST = 200\n","TEST_SIZE = 0.3\n","\n","MAX_VOCAB_SIZE = 30000 \n","EMBEDDING_DIM = 128\n","HIDDEN_DIM = 1024 # numero di celle nei layer ricorrenti nascosti\n","\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 2000\n","MAX_SEQ_LENGTH = 128\n","\n","NUM_LAYERS = 1 # Numero di layer di Decoder del Transformer\n","NUM_HEADS = 8 # Numero di meccanismi di multi-head attention\n","FF_DIM = 32 # Numero di celle dei Layer Feed Forward\n","DROPUOT = 0.5\n","\n","# Ottimizzatore Adam\n","LEARNING_RATE_ADAM = 1e-4\n","BETA_1 = 0.66\n","BETA_2 = 0.999\n","EPOCHS_ADAM = 20\n","\n","# IMPOSTO IL DEBUG A TRUE \n","debug = True\n","trainable = False\n","training = True"],"metadata":{"id":"8CN-4Uzoqbjl","executionInfo":{"status":"ok","timestamp":1681466096339,"user_tz":-120,"elapsed":14,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Parametri BERT"],"metadata":{"id":"BehZY4rETECN"}},{"cell_type":"code","source":["bert_model_name = 'distilbert_multi_cased_L-6_H-768_A-12/1'  \n","tfhub_handle_preprocess = 'https://tfhub.dev/jeongukjae/distilbert_multi_cased_preprocess/2'\n","tfhub_handle_encoder =  'https://tfhub.dev/jeongukjae/distilbert_multi_cased_L-6_H-768_A-12/1'\n","\n","if debug:\n","  print('BERT model name                    : ', bert_model_name)\n","  print('BERT model selected                : ', tfhub_handle_encoder)\n","  print('BERT preprocess                    : ', tfhub_handle_preprocess)"],"metadata":{"id":"fodDcY6sm392","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466096340,"user_tz":-120,"elapsed":11,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"ea53f9d6-0d69-43da-fea1-dcbe0548b1d4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT model name                    :  distilbert_multi_cased_L-6_H-768_A-12/1\n","BERT model selected                :  https://tfhub.dev/jeongukjae/distilbert_multi_cased_L-6_H-768_A-12/1\n","BERT preprocess                    :  https://tfhub.dev/jeongukjae/distilbert_multi_cased_preprocess/2\n"]}]},{"cell_type":"markdown","source":["## DATASET"],"metadata":{"id":"5DPeN9Vanbvv"}},{"cell_type":"markdown","source":["### Caricamento Dati"],"metadata":{"id":"LU7AorKXT8K7"}},{"cell_type":"code","source":["def preprocess_sentence(w):\n","  '''\n","  Preprocessing dei testi di input, impostando tutti i caratteri\n","  minuscoli, aggiungendo uno spazio prima di ogni punto e sostituendo\n","  qualsiasi carattere con uno spazio se non è compreso nel seguente elenco:\n","  (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\", \"’\")\n","  '''\n","  # inserimento di uno spazio tra ogni parola e il successivo punto,\n","  # punto esclamativo, punto interrogativo e virgola\n","  # esempio: \"ciao, come và?\" => \"ciao , come và ?\"\n","  w = re.sub(r\"([?.!,])\", r\" \\1 \", w) # inserimento di uno spazio\n","\n","  # sostituzione dei caratteri apostrofo\n","  w = re.sub(r\"([’]+)\", \"'\", w)\n","\n","  w = w.replace(\"á\", \"à\")\n","  w = w.replace(\"é\", \"è\")\n","  w = w.replace(\"í\", \"ì\")\n","  w = w.replace(\"ó\", \"ò\")\n","  w = w.replace(\"ú\", \"ù\")\n","  w = w.replace('\"', \" \")\n","  w = w.replace(':', \" \")\n","  w = w.replace('«', \" \")\n","  w = w.replace('»', \" \")\n","  w = w.replace('‘', \" \")\n","  w = w.replace('-', \" \")\n","  w = w.replace('[', \" \")\n","  w = w.replace(']', \" \")\n","  w = w.replace('(', \" \")\n","  w = w.replace(')', \" \")\n","  w = w.replace(\"•\", \" \")\n","  w = w.replace(\"..\", \".\")\n","  w = w.replace(\"...\", \".\")\n","  w = w.replace(\"\\xa0\", \" \")\n","  w = w.replace(\"\\xc3\\xa8\", \" \")\n","  w = w.replace(\"\\xe2\\x80\\xaf\", \" \")\n","  w = w.replace(\"   \", \" \")\n","  w = w.replace(\"–\", \" \")\n","  w = w.replace(\"“\", \" \")\n","  w = w.replace(\"”\", \" \")\n","  w = w.replace(\"„\", \" \")\n","  w = w.replace(\"─\", \" \")\n","  w = w.replace(\"♪\", \" \")\n","  w = w.replace(\"#\", \" \")\n","  w = w.replace(\"/\", \" \")\n","  w = w.replace(\"=\", \" \")\n","  w = w.replace(\">\", \" \")\n","  w = w.replace(\"\\\\\", \" \")\n","  w = w.replace(\"`\", \" \")\n","  w = w.replace(\"¡\", \" \")\n","  w = w.replace(\"¿\", \" \")\n","  w = w.replace(\"œ\", \" \")\n","  w = w.replace(\"♗\", \" \")\n","  w = w.replace(\"♘\", \" \")\n","  w = w.replace(\"《\", \" \")\n","  w = w.replace(\"》\", \" \")\n","  # w = w.replace(\"\", \" \")\n","  # w = w.replace(\"\", \" \")\n","\n","  # inserimento di uno spazio dopo apostrofo\n","  w = re.sub(r\"(['])\", r\"\\1 \", w) \n","\n","  w = w.replace(\" ' \", \" '\")\n","\n","  w = re.sub(r'[\" \"]+', \" \", w) # rimozione di più spazi consecutivi\n","  return w"],"metadata":{"id":"Jm_Up6gyOTgW","executionInfo":{"status":"ok","timestamp":1681466096340,"user_tz":-120,"elapsed":8,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\n","  train_data_filenamepath,\n","  usecols=[ORIGINAL_COLUMN, TRANSLATE_COLUMN, TYPE_COLUMN],\n",")\n","\n","# Preprocessing dei dati di Input\n","df[ORIGINAL_COLUMN] = df[ORIGINAL_COLUMN].apply(lambda x : preprocess_sentence(x))\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","df[TRANSLATE_COLUMN] = df[TRANSLATE_COLUMN].apply(lambda x : preprocess_sentence(x))\n","\n","if debug:\n","  print(f'Dati totali presenti nel Dataset               : {len(df)}')\n","  print('----------------------------------- TRAIN SET -----------------------------------------')\n","  print((df[ORIGINAL_COLUMN].tolist())[-1:])\n","  print((df[TRANSLATE_COLUMN].tolist())[-1:])\n","  print((df[ORIGINAL_COLUMN].tolist())[:1])\n","  print((df[TRANSLATE_COLUMN].tolist())[:1])"],"metadata":{"id":"duGPhZ_jgPVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466124569,"user_tz":-120,"elapsed":28237,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"0f3e19f8-0754-4ad0-e87a-138cccd001b7"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Dati totali presenti nel Dataset               : 379582\n","----------------------------------- TRAIN SET -----------------------------------------\n","['Min önskan och vilja styrdes av gudomlig kärlek som driver solen och andra stjärnor framåt som ett hjul flyttade regelbundet . ']\n","[\" ma già volgeva il mio disio e 'l velle , sì come rota ch' igualmente è mossa , l' amor che move il sole e l' altre stelle\"]\n","[\"De même que , en temps de guerre , officiers et soldats se sentent autorisès par l' opinion gènèrale à commettre des actes qui , en temps de paix , sont tenus pour criminels , de même les rèvolutionnaires , dans leur lutte , se regardaient comme couverts par l' opinion de leur cercle , en vertu de laquelle les actes de cruautè qu' ils commettaient ètaient nobles et moraux , ètant commis par eux au prix de leur libertè , de leur vie , de tout ce qui est cher à la plupart des hommes . Ainsi s' expliquait , que des personnes excellentes , incapables non seulement de causer une souffrance , mais même d' en supporter la vue , pussent se prèparer tranquillement à la violence et au meurtre , et professer la saintetè de tels actes , considèrès comme moyens de dèfense , ou encore comme instrument utile à la rèalisation d' un idèal de bonheur pour l' humanitè . \"]\n","[\"Così come in tempo di guerra , ufficiali e soldati si sentono responsabilizzati dall' opinione generale a commettere atti che , in tempo di pace , sono necessari per i criminali , anche rivoluzionari nella loro lotta , considerati coperti dal parere del loro circolo , secondo cui gli atti di crudeltà che hanno commesso erano nobili e morali , essendo commessi da loro nel prezzo della loro libertà , della loro vita , di tutto ciò che è caro alla maggior parte degli uomini . Ciò ha spiegato che persone eccellenti , in grado non solo di causare sofferenza , ma anche di sopportarne la vista , potrebbero felicemente prepararsi alla violenza e all' omicidio , e professare la santità di tali atti , considerati come un mezzo di difesa , o come utili per la realizzazione di un ideale di felicità per l' umanità . \"]\n"]}]},{"cell_type":"markdown","source":["## Tokenizer\n","\n","Creo due differenti tokenizer che mi servizranno per la predisposizione dei dati di input:\n","\n","\n","*   EncTokenizer classe custom per la tokenizzazione dei dati di input al Layer di Encoder di Bert\n","*   DecTokenizer classe custom per la tokenizzazione dei dati di input al Layer di Decoder\n","\n"],"metadata":{"id":"njyY9RWlFMWu"}},{"cell_type":"code","source":["input_data_vocab = df[ORIGINAL_COLUMN].tolist()\n","target_data_vocab = df[TRANSLATE_COLUMN].tolist()\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_data_vocab, target_data_vocab))\n","dataset = dataset.shuffle(len(input_data_vocab)).batch(BATCH_SIZE, drop_remainder=True)\n","\n","train_multilingual = dataset.map(lambda multilingual, ita: multilingual)\n","train_ita = dataset.map(lambda multilingual, ita: ita)"],"metadata":{"id":"fUG1fTAYekOy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466133264,"user_tz":-120,"elapsed":8725,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"8a89fc32-01c7-4a74-97a2-7b1a5df144ad"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"],"metadata":{"id":"xWO-LrXJe0cF","executionInfo":{"status":"ok","timestamp":1681466133265,"user_tz":-120,"elapsed":33,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def cleanup_text(reserved_tokens, token_txt):\n","\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result"],"metadata":{"id":"yGdsrOoKiYUK","executionInfo":{"status":"ok","timestamp":1681466133265,"user_tz":-120,"elapsed":31,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["tokenizers = tf.Module()"],"metadata":{"id":"qbKNS_uQhHLz","executionInfo":{"status":"ok","timestamp":1681466133265,"user_tz":-120,"elapsed":29,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["### Classe EncTokenizer\n","\n","Classe custom per la tokenizzazione dei dati di italiano e che crea i tre vettori necessari al layer di Encoder \n","Bert:\n","\n","\n","*   input_word_ids\n","*   input_type_ids\n","*   input_mask\n","\n","\n","\n"],"metadata":{"id":"0KUcCnjXVjt3"}},{"cell_type":"code","source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens = {\n","  'start_of_sequence_id': 101,\n","  'end_of_segment_id': 102,\n","  'padding_id': 0,\n","  'mask_id': 103\n","}\n","\n","bert_vocab_args = dict(\n","  # The target vocabulary size\n","  vocab_size = MAX_VOCAB_SIZE,\n","  # Reserved tokens that must be included in the vocabulary\n","  reserved_tokens=reserved_tokens,\n","  # Arguments for `text.BertTokenizer`\n","  bert_tokenizer_params=bert_tokenizer_params,\n","  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","  learn_params={},\n",")"],"metadata":{"id":"Yr0izOZLembx","executionInfo":{"status":"ok","timestamp":1681466133266,"user_tz":-120,"elapsed":29,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["exist_vocab = Path(multilingual_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  multilingual_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_multilingual.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(multilingual_vocab_filenamepath, multilingual_vocab)"],"metadata":{"id":"BwSKtlLSe7bH","executionInfo":{"status":"ok","timestamp":1681466133619,"user_tz":-120,"elapsed":381,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class EncTokenizer(tf.Module):\n","  def __init__(self, tfhub_handle_preprocess):\n","    self.preprocessor = hub.KerasLayer(tfhub_handle_preprocess)\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    return self.preprocessor(strings)"],"metadata":{"id":"WmsNdDLNf6vr","executionInfo":{"status":"ok","timestamp":1681466133620,"user_tz":-120,"elapsed":3,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["tokenizers.multilingual = EncTokenizer(tfhub_handle_preprocess)"],"metadata":{"id":"-4B-HWWcmsmz","executionInfo":{"status":"ok","timestamp":1681466142800,"user_tz":-120,"elapsed":9183,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### Classe DecTokenizer\n","\n","Classe custom per la tokenizzazione dei dati in lingua italiana per il layer di Decoder\n"],"metadata":{"id":"mICEGEzJVnvx"}},{"cell_type":"code","source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens_vocab=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","  # The target vocabulary size\n","  vocab_size = MAX_VOCAB_SIZE,\n","  # Reserved tokens that must be included in the vocabulary\n","  reserved_tokens=reserved_tokens_vocab,\n","  # Arguments for `text.BertTokenizer`\n","  bert_tokenizer_params=bert_tokenizer_params,\n","  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","  learn_params={},\n",")"],"metadata":{"id":"abBEnJJGV0AD","executionInfo":{"status":"ok","timestamp":1681466142801,"user_tz":-120,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["exist_vocab = Path(ita_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  ita_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_ita.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(ita_vocab_filenamepath, ita_vocab)"],"metadata":{"id":"dGsP1V4nVz6S","executionInfo":{"status":"ok","timestamp":1681466142801,"user_tz":-120,"elapsed":7,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["START = tf.argmax(tf.constant(reserved_tokens_vocab) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens_vocab) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape(out_type=tf.int32)[0]\n","\n","  starts = tf.fill([count,1], START)\n","  starts = tf.cast(starts, tf.int32)\n","\n","  ends = tf.fill([count,1], END)\n","  ends = tf.cast(ends, tf.int32)\n","\n","  x = tf.concat([starts, ragged, ends], axis=1)\n","  return x"],"metadata":{"id":"BeaD2-uLWT50","executionInfo":{"status":"ok","timestamp":1681466142802,"user_tz":-120,"elapsed":7,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["class DecTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens_vocab, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True, token_out_type=tf.int32)\n","    self._reserved_tokens_vocab = reserved_tokens_vocab\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","    \n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int32))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int32))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens_vocab, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens_vocab)"],"metadata":{"id":"iaAW-xm5WT1_","executionInfo":{"status":"ok","timestamp":1681466142802,"user_tz":-120,"elapsed":7,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["tokenizers.ita = DecTokenizer(reserved_tokens_vocab, ita_vocab_filenamepath)"],"metadata":{"id":"svlLobM4WTzC","executionInfo":{"status":"ok","timestamp":1681466146355,"user_tz":-120,"elapsed":3559,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["### Analisi Dati Tokenizzati"],"metadata":{"id":"pKZxiQ5_Whmw"}},{"cell_type":"code","source":["print(f'Vocabolario Italiano : {tokenizers.ita.get_vocab_size()}')"],"metadata":{"id":"Jrg6TwQzW5LN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466146356,"user_tz":-120,"elapsed":15,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"62856db6-0975-4770-c833-00ec3b1ea735"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabolario Italiano : 23120\n"]}]},{"cell_type":"markdown","source":["## Creazione dataset\n","Utilizzo della libreria tf.data per la gestione del dataset da utilizzare.\n","Verranno creati batch di esempi che verranno utilizzati durante l'addestramento."],"metadata":{"id":"5QIDajkEsVU1"}},{"cell_type":"code","source":["def split_dataset(df: pd.DataFrame,\n","                  filter_column: str, \n","                  debug: bool = False) -> Tuple:\n","\n","  print(f'Lunghezza df {len(df)}')\n","  dataset = (df.loc[df[TYPE_COLUMN] == filter_column]).copy() \n","  print(f'Lunghezza dataset {len(dataset)}')\n","  \n","  if NUM_SAMPLES > 0:\n","    dataset = dataset[:NUM_SAMPLES]\n","\n","  input_data = dataset[ORIGINAL_COLUMN].tolist()\n","  target_data = dataset[TRANSLATE_COLUMN].tolist()\n","\n","  train_input_data, validation_input_data, train_target_data, validation_target_data = train_test_split(\n","    input_data[:-TEST], \n","    target_data[:-TEST], \n","    test_size=TEST_SIZE, \n","    random_state=42,\n","    shuffle=True\n","  ) \n","\n","  train_input_data = train_input_data[:(int((len(train_input_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","  train_target_data = train_target_data[:(int((len(train_target_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","  \n","  validation_input_data = validation_input_data[:(int((len(validation_input_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","  validation_target_data = validation_target_data[:(int((len(validation_target_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","\n","  test_input_data = input_data[len(train_input_data)+len(validation_input_data):]\n","  test_target_data = target_data[len(train_target_data)+len(validation_target_data):]\n","\n","  if debug:\n","    print(f'Dati totali presenti nel Dataset               : {len(df)}')\n","    print(f'Dati totali presenti nel Dataset di Train      : {len(train_input_data)}')\n","    print(f'Dati totali presenti nel Dataset di Validation : {len(validation_input_data)}')\n","    print(f'Dati totali presenti nel Dataset di Test       : {len(test_input_data)}\\n')\n","\n","\n","    print('----------------------------------- TRAIN SET -----------------------------------------')\n","    print(train_input_data[-4:])\n","    print(train_target_data[-4:])\n","    print('--------------------------------- VALIDATION SET --------------------------------------')\n","    print(validation_input_data[-4:])\n","    print(validation_target_data[-4:])\n","    print('----------------------------------- TEST SET ------------------------------------------')\n","    print(test_input_data[-4:])\n","    print(test_target_data[-4:])\n","\n","    print('-------------------------------- ANALISI DATI -----------------------------------------')\n","    print(f'Esempi nel Dataset di Train                            : {len(train_input_data)}')\n","    print(f'Frase più corta nel Dataset Input di Train             : {min(train_input_data, key = len)}')\n","    print(f'Frase più corta nel Dataset Target di Train            : {min(train_target_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Input di Train             : {max(train_input_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Target di Train            : {max(train_target_data, key = len)}')\n","    print('---------------------------------------------------------------------------------------')\n","    print(f'Esempi nel Dataset di Validation                       : {len(validation_input_data)}')\n","    print(f'Frase più corta nel Dataset Input di Validation        : {min(validation_input_data, key = len)}')\n","    print(f'Frase più corta nel Dataset Target di Validation       : {min(validation_target_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Input di Validation        : {max(validation_input_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Target di Validation       : {max(validation_target_data, key = len)}')\n","    print('---------------------------------------------------------------------------------------')\n","    print(f'Esempi nel Dataset di Test                             : {len(test_input_data)}')\n","    print(f'Frase più corta nel Dataset Input di Test              : {min(test_input_data, key = len)}')\n","    print(f'Frase più corta nel Dataset Target di Test             : {min(test_target_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Input di Test              : {max(test_input_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Target di Test             : {max(test_target_data, key = len)}')    \n","\n","    print('\\n--------------------------------- EXAMPLE ---------------------------------------------')\n","    print([min(train_input_data, key = len)])\n","    print(tokenizers.multilingual.tokenize([min(train_input_data, key = len)])['input_word_ids'][:, :32])\n","    print('------------------------------------------------------------------')\n","    print([min(train_target_data, key = len)])\n","    print(tokenizers.ita.tokenize([min(train_target_data, key = len)]))\n","    print('\\n')\n","    print([max(train_input_data, key = len)])\n","    print(tokenizers.multilingual.tokenize([max(train_input_data, key = len)])['input_word_ids'])\n","    print('------------------------------------------------------------------')\n","    print([max(train_target_data, key = len)])\n","    print(tokenizers.ita.tokenize([max(train_target_data, key = len)]))  \n","  \n","  return train_input_data, validation_input_data, test_input_data, train_target_data, validation_target_data, test_target_data "],"metadata":{"id":"ESHUcQtthhd2","executionInfo":{"status":"ok","timestamp":1681466146357,"user_tz":-120,"elapsed":12,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def prepare_batch(multilingual, ita):\n","  zero = tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH], tf.int32)\n","\n","  # Tokenizzo l'input per l'Encoder\n","  encoder = tokenizers.multilingual.tokenize(multilingual)          \n","\n","  # Tokenizzo l'input per il Decder e creo la variabile Target\n","  ita = tokenizers.ita.tokenize(ita)\n","  decoder = ita[:, :-1].to_tensor()  # Drop the [END] tokens\n","  target = ita[:, 1:].to_tensor()   # Drop the [START] tokens\n","  \n","  decoder = tf.concat([decoder, zero], 1)\n","  decoder = decoder[:, :(MAX_SEQ_LENGTH)]\n","\n","  target = tf.concat([target, zero], 1)\n","  target = target[:, :(MAX_SEQ_LENGTH)]\n","\n","  return (encoder, decoder), target"],"metadata":{"id":"ccH3jHoABPzV","executionInfo":{"status":"ok","timestamp":1681466146357,"user_tz":-120,"elapsed":11,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def make_batches(ds):\n","  return (\n","    ds\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE)\n","    .map(prepare_batch, tf.data.AUTOTUNE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE))"],"metadata":{"id":"l_dswlCiBTdR","executionInfo":{"status":"ok","timestamp":1681466146357,"user_tz":-120,"elapsed":10,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def train_val_test_dataset(df: pd.DataFrame, \n","                          filter_column: str, \n","                          debug: bool = False) -> Tuple:\n","\n","  # Recupero il dataset \n","  train_input_data, validation_input_data, test_input_data, train_target_data, validation_target_data, test_target_data = split_dataset(df=df, filter_column=filter_column, debug=debug)\n","\n","  # Definizione del dataset\n","  # [from_tensor_slices] permette di recuperare batch\n","  # di esempi dai dataset di riferimento\n","  train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_target_data))\n","  validation_dataset = tf.data.Dataset.from_tensor_slices((validation_input_data, validation_target_data))\n","  test_dataset = tf.data.Dataset.from_tensor_slices((test_input_data, test_target_data))\n","\n","  # impostazione del recupero di esempi presi in maniera\n","  # casuale in gruppi di [BATCH_SIZE] tra quelli disponibili\n","  train_dataset = make_batches(train_dataset)\n","  validation_dataset = make_batches(validation_dataset)\n","\n","  return train_dataset, validation_dataset, test_dataset"],"metadata":{"id":"tktJ5YuIsYe3","executionInfo":{"status":"ok","timestamp":1681466146358,"user_tz":-120,"elapsed":11,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["train_dataset_ita, validation_dataset_ita, test_dataset = train_val_test_dataset(df=df,\n","                                                                                 filter_column='ITA',\n","                                                                                 debug=debug)"],"metadata":{"id":"nkcyLV1qqQHL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466152795,"user_tz":-120,"elapsed":6050,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"00d00259-f8bb-4344-db72-a07a50896e20"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Lunghezza df 379582\n","Lunghezza dataset 244398\n","Dati totali presenti nel Dataset               : 379582\n","Dati totali presenti nel Dataset di Train      : 17344\n","Dati totali presenti nel Dataset di Validation : 7424\n","Dati totali presenti nel Dataset di Test       : 232\n","\n","----------------------------------- TRAIN SET -----------------------------------------\n","[\"C' est le braquage de banque le plus sanglant qu' a connu ce pays . \", \"Il s' est converti au christianisme . \", 'Vous pouvez ècrire dans la langue que vous voulez . Sur Tatoeba , toutes les langues sont ègales . ', 'Elle a trois s urs une est infirmière et les autres sont enseignantes . ']\n","['È la rapina in banca più sanguinosa della storia di questo paese . ', 'Si è convertito al Cristianesimo . ', 'Puoi scrivere in qualsiasi lingua desideri . Su Tatoeba tutte le lingue sono uguali . ', 'Lei ha tre sorelle una è infermiera e le altre sono insegnanti . ']\n","--------------------------------- VALIDATION SET --------------------------------------\n","['Pourquoi la chance vous haït elle ? ', \"Le gouvernement des États Unis a taxè plusieurs pays d' États voyous , mais l' ironie du sort est qu' aujourd' hui , et après des dècennies de politiques agressives , d' interventions et d' invasions , ce sont les États Unis eux mêmes qui sont considèrès , dans le monde , comme l' État voyou par excellence . \", \"Je m' inquiète des rèsultats de l' examen . \", 'Vous allez marcher tous les matins . ']\n","['Perchè la fortuna vi odia ? ', \"Il governo americano ha tacciato diversi paesi come Stati canaglia , ma l' ironia è che oggi , dopo decenni di politiche aggressive , interventi e invasioni , sono gli Stati Uniti sono considerati in tutto il mondo come lo Stato canaglia per eccellenza . \", \"Mi preoccupo per i risultati dell' esame . \", 'Va a camminare ogni mattina . ']\n","----------------------------------- TEST SET ------------------------------------------\n","['Je ne vois aucun problème avec ça . ', 'Vous devez travailler , pas penser . ', \"Tatoeba n' est pas un dictionnaire . \", \"L' orthographe est très importante . \"]\n","['Non vedo dove stia il problema . ', 'Dovete lavorare , non pensare . ', 'Tatoeba non è un dizionario . ', \"L' ortografia è molto importante . \"]\n","-------------------------------- ANALISI DATI -----------------------------------------\n","Esempi nel Dataset di Train                            : 17344\n","Frase più corta nel Dataset Input di Train             :  Veux tu l' acheter ? Oui . \n","Frase più corta nel Dataset Target di Train            : Oggi offro io . \n","Frase più lunga nel Dataset Input di Train             : De même que , en temps de guerre , officiers et soldats se sentent autorisès par l' opinion gènèrale à commettre des actes qui , en temps de paix , sont tenus pour criminels , de même les rèvolutionnaires , dans leur lutte , se regardaient comme couverts par l' opinion de leur cercle , en vertu de laquelle les actes de cruautè qu' ils commettaient ètaient nobles et moraux , ètant commis par eux au prix de leur libertè , de leur vie , de tout ce qui est cher à la plupart des hommes . Ainsi s' expliquait , que des personnes excellentes , incapables non seulement de causer une souffrance , mais même d' en supporter la vue , pussent se prèparer tranquillement à la violence et au meurtre , et professer la saintetè de tels actes , considèrès comme moyens de dèfense , ou encore comme instrument utile à la rèalisation d' un idèal de bonheur pour l' humanitè . \n","Frase più lunga nel Dataset Target di Train            : Così come in tempo di guerra , ufficiali e soldati si sentono responsabilizzati dall' opinione generale a commettere atti che , in tempo di pace , sono necessari per i criminali , anche rivoluzionari nella loro lotta , considerati coperti dal parere del loro circolo , secondo cui gli atti di crudeltà che hanno commesso erano nobili e morali , essendo commessi da loro nel prezzo della loro libertà , della loro vita , di tutto ciò che è caro alla maggior parte degli uomini . Ciò ha spiegato che persone eccellenti , in grado non solo di causare sofferenza , ma anche di sopportarne la vista , potrebbero felicemente prepararsi alla violenza e all' omicidio , e professare la santità di tali atti , considerati come un mezzo di difesa , o come utili per la realizzazione di un ideale di felicità per l' umanità . \n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Validation                       : 7424\n","Frase più corta nel Dataset Input di Validation        :  Veux tu l' acheter ? Oui . \n","Frase più corta nel Dataset Target di Validation       : Vuoi farlo ora ? \n","Frase più lunga nel Dataset Input di Validation        : Les États Unis ont plusieurs fois justifiè des interventions dans d' autres pays au nom de la protection des sacro saints intèrêts amèricains ou des citoyens amèricains à travers le monde . Le jour où , en 2008 , la Gèorgie avait attaquè des civils et des militaires russes en Ossètie du Sud , les Russes sont ègalement intervenus pour la protection lègitime de ses citoyens et militaires qui n' ètaient pas seulement victimes d' une attaque , mais d' un massacre . Mais lorsque ce sont les autres pays qui dèfendent leurs intèrêts et leurs civils au delà de leurs frontières , ceci ne plaît èvidemment pas aux États Unis . \n","Frase più lunga nel Dataset Target di Validation       : Gli Stati Uniti hanno ripetutamente giustificato degli interventi in altri paesi in nome della tutela degli interessi sacrosanti americani o dei cittadini americani in tutto il mondo . Il giorno in cui , nel 2008 , la Georgia ha attaccato civili e soldati russi in Ossezia del Sud , i russi sono ugualmente intervenuti per la legittima tutela dei loro cittadini e soldati che non erano solo le vittime di un attacco , ma di un massacro . Ma quando gli altri paesi stanno difendendo i loro interessi e civili oltre i loro confini , questo ovviamente non piace agli Stati Uniti . \n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Test                             : 232\n","Frase più corta nel Dataset Input di Test              :  J' aime voyager . Moi aussi . \n","Frase più corta nel Dataset Target di Test             : Oggi è venerdì . \n","Frase più lunga nel Dataset Input di Test              : L' un est rouge , l' autre est blanc . \n","Frase più lunga nel Dataset Target di Test             : Molte persone hanno partecipato alla riunione . \n","\n","--------------------------------- EXAMPLE ---------------------------------------------\n","[\" Veux tu l' acheter ? Oui . \"]\n","tf.Tensor(\n","[[  101 19561 11855 13055   180   112 33478 28647   136 47060 10116   119\n","    102     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(1, 32), dtype=int32)\n","------------------------------------------------------------------\n","['Oggi offro io . ']\n","<tf.RaggedTensor [[2, 260, 21120, 85, 11, 3]]>\n","\n","\n","[\"De même que , en temps de guerre , officiers et soldats se sentent autorisès par l' opinion gènèrale à commettre des actes qui , en temps de paix , sont tenus pour criminels , de même les rèvolutionnaires , dans leur lutte , se regardaient comme couverts par l' opinion de leur cercle , en vertu de laquelle les actes de cruautè qu' ils commettaient ètaient nobles et moraux , ètant commis par eux au prix de leur libertè , de leur vie , de tout ce qui est cher à la plupart des hommes . Ainsi s' expliquait , que des personnes excellentes , incapables non seulement de causer une souffrance , mais même d' en supporter la vue , pussent se prèparer tranquillement à la violence et au meurtre , et professer la saintetè de tels actes , considèrès comme moyens de dèfense , ou encore comme instrument utile à la rèalisation d' un idèal de bonheur pour l' humanitè . \"]\n","tf.Tensor(\n","[[   101  10190  11594  10121    117  10110  12358  10104  14158    117\n","   59973  10131  25734  10126  97705  10368  37882  10107  13230  10248\n","     180    112  32282    175  20276  37833  10284    254  10986  11527\n","   10246  10139  37481  10355    117  10110  12358  10104  41795    117\n","   10647  69323  10107  10322    171 102422  58798    117  10104  11594\n","   10152    186  13340  34381  30861  38260    117  10260  11807  43927\n","     117  10126  42047  32247  10986  11170  98095  10107  10248    180\n","     112  32282  10104  11807  57775    117  10110  20900  10138  10104\n","   20600  10152  37481  10104    171  60021  11159  13340  10608    112\n","   13178  10986  12201  15617    262  26812  11405  43657  10131  25528\n","   11855    117    262  19533  10212  15240  10248  22502  10257  18236\n","   10104  11807  72517  10123  13340    117  10104  11807  13772    117\n","   10104  13003  10794  10355  10176  10262  10129    102]], shape=(1, 128), dtype=int32)\n","------------------------------------------------------------------\n","[\"Così come in tempo di guerra , ufficiali e soldati si sentono responsabilizzati dall' opinione generale a commettere atti che , in tempo di pace , sono necessari per i criminali , anche rivoluzionari nella loro lotta , considerati coperti dal parere del loro circolo , secondo cui gli atti di crudeltà che hanno commesso erano nobili e morali , essendo commessi da loro nel prezzo della loro libertà , della loro vita , di tutto ciò che è caro alla maggior parte degli uomini . Ciò ha spiegato che persone eccellenti , in grado non solo di causare sofferenza , ma anche di sopportarne la vista , potrebbero felicemente prepararsi alla violenza e all' omicidio , e professare la santità di tali atti , considerati come un mezzo di difesa , o come utili per la realizzazione di un ideale di felicità per l' umanità . \"]\n","<tf.RaggedTensor [[2, 125, 99, 83, 172, 78, 575, 10, 11697, 5987, 31, 3991, 81, 5792, 6586,\n","  5661, 532, 8, 1488, 2750, 27, 6224, 1923, 76, 10, 83, 172, 78, 604, 10,\n","  91, 21111, 82, 35, 4450, 10, 281, 18720, 22435, 20890, 926, 206, 122,\n","  5619, 10, 11841, 6829, 158, 2737, 97, 122, 20964, 10, 409, 163, 128,\n","  1923, 78, 10810, 76, 134, 2985, 245, 10121, 31, 12939, 10, 3117, 11007,\n","  95, 122, 119, 1374, 141, 122, 1739, 10, 141, 122, 192, 10, 78, 130, 167,\n","  76, 31, 962, 153, 497, 189, 333, 708, 11, 167, 89, 4918, 76, 270, 20590,\n","  10, 83, 414, 79, 195, 78, 12719, 21200, 10, 104, 281, 78, 209, 20483,\n","  10185, 77, 329, 10, 10297, 417, 978, 11350, 153, 2474, 31, 229, 8, 2764,\n","  10, 31, 18116, 377, 77, 1241, 384, 78, 2481, 1923, 10, 11841, 99, 84,\n","  451, 78, 3976, 10, 41, 99, 6098, 82, 77, 13024, 78, 84, 7265, 78, 2322,\n","  82, 38, 8, 5381, 11, 3]]>\n"]}]},{"cell_type":"code","source":["# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for (enc_input, dec_input), target in train_dataset_ita.take(1):\n","  print('----------------------- ENCODER  -------------------------------')\n","  print(f'Shape                    : {enc_input[\"input_word_ids\"].shape}')\n","  print(f'Word Ids                 : {enc_input[\"input_word_ids\"][0, :MAX_SEQ_LENGTH]}')\n","  print(f'Input Mask               : {enc_input[\"input_mask\"][0, :MAX_SEQ_LENGTH]}')\n","  print('--------------------- DECODER ----------------------------------')\n","  print(f'Shape it input           : {dec_input.shape}')\n","  print(f'Example it input         : {dec_input[0]}')  \n","  print('--------------------- TARGET -----------------------------------')\n","  print(f'Shape it input           : {target.shape}')\n","  print(f'Example it target        : {target[0]}')  "],"metadata":{"id":"VH_aKPlV_AWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466154145,"user_tz":-120,"elapsed":1372,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"42b1d7c7-6e87-4dec-f306-86fd1e6b81d2"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------- ENCODER  -------------------------------\n","Shape                    : (32, 128)\n","Word Ids                 : [  101   140   112 10176 63651 10608   112 11117 10176 12541 17656   117\n"," 10614 11117 10176   262 10797 84066 10216   119   102     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0     0     0     0     0\n","     0     0     0     0     0     0     0     0]\n","Input Mask               : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","--------------------- DECODER ----------------------------------\n","Shape it input           : (32, 128)\n","Example it input         : [   2   31  171   76   31 3280   10  173   31 6756   11    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0]\n","--------------------- TARGET -----------------------------------\n","Shape it input           : (32, 128)\n","Example it target        : [  31  171   76   31 3280   10  173   31 6756   11    3    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0]\n"]}]},{"cell_type":"markdown","source":["## Encoder BERT\n","\n","Predispondo la classe necessaria per la costruzione di BERT\n"],"metadata":{"id":"8dtVuZGJpvXl"}},{"cell_type":"code","source":["class MLPMixerLayer(tf.keras.layers.Layer):\n","  def __init__(self, num_patches, dropout_rate, *args, **kwargs):\n","    super().__init__(*args, **kwargs)\n","\n","    self.mlp = tf.keras.Sequential(\n","        [\n","            tf.keras.layers.Dense(units=num_patches, activation='relu'),\n","            tf.keras.layers.Dense(units=num_patches),\n","            tf.keras.layers.Dropout(rate=dropout_rate),\n","        ]\n","    )\n","    self.normalize = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","  def call(self, inputs):\n","    # Apply layer normalization.\n","    x = self.normalize(inputs)\n","    # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n","    x_channels = tf.linalg.matrix_transpose(x)\n","    # Apply mlp on each channel independently.\n","    mlp_outputs = self.mlp(x_channels)\n","    # Transpose mlp_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n","    mlp_outputs = tf.linalg.matrix_transpose(mlp_outputs)\n","    return mlp_outputs"],"metadata":{"id":"L0OniBSyW-rF","executionInfo":{"status":"ok","timestamp":1681466154146,"user_tz":-120,"elapsed":6,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["class EncoderBert(tf.keras.layers.Layer):\n","  def __init__(self, bert_encoder, embedding_dim, max_len, rate=0.5, trainable=False):\n","    super(EncoderBert, self).__init__()\n","\n","    self.encoder = hub.KerasLayer(bert_encoder, name='BERT_encoder', trainable=trainable)\n","\n","    self.conv_1 = tf.keras.layers.Conv1D(embedding_dim * 4, 1, activation='relu') \n","    self.conv_2 = tf.keras.layers.Conv1D(embedding_dim * 2, 1, activation='relu') \n","    self.conv_3 = tf.keras.layers.Conv1D(embedding_dim, 1, activation='relu') \n","    # self.lambda_layer = tf.keras.layers.Lambda(lambda x: x[:,:max_len])\n","    self.mlp_mixer = MLPMixerLayer(num_patches=embedding_dim // 4, dropout_rate=rate)\n","    self.max_len = max_len\n","\n","  def call(self, x, debug=False):\n","\n","    if debug:\n","      print(f'****************** DEBUG ENCODER BERT ******************')\n","      print(f\"First example\")\n","      print(f'Keys                         : {list(x.keys())}')\n","      print(f'Shape                        : {x[\"input_word_ids\"].shape}')\n","      print(f'Word Ids                     : {x[\"input_word_ids\"][0, :16]}')\n","      print(f'Input Mask                   : {x[\"input_mask\"][0, :16]}')\n","      \n","    x = self.encoder(x)['sequence_output'] \n","    # encoder_outputs stato intermedio di BERT prima che esegua la traduzione recuperare la metà della lunghezza\n","    # x = self.encoder(x)['encoder_outputs'] \n","    # x = x[int(len(x) / 2) - 1]\n","\n","    if debug:\n","      print()\n","      print(f'Encoder Outputs BERT Shape   : {x.shape}')\n","      print(f'Encoder Outputs BERT Values  : {x[0, :1, :16]}')\n","\n","    x = self.conv_1(x)\n","    if debug:\n","      print()\n","      print(f'Sequence Conv1 Shape         : {x.shape}')\n","\n","    x = self.conv_2(x)\n","    if debug:\n","      print(f'Sequence Conv2 Shape         : {x.shape}')\n","\n","    x = self.conv_3(x)\n","    if debug:\n","      print(f'Sequence Conv3 Shape         : {x.shape}')\n","      \n","    x = self.mlp_mixer(x)\n","    if debug:\n","      print(f'Sequence MLP-Mixer Shape     : {x.shape}')\n","      print()\n","      print(f'Sequence Outputs Values      : {x[0, 0, :16]}')      \n","      print('*********************************************************') \n","\n","    # x = self.lambda_layer(x)\n","    # if debug:\n","    #   print(f'Sequence Lambda Layer        : {x.shape}')\n","    #   print()\n","    #   print(f'Sequence Outputs Values      : {x[0, 0, :16]}')      \n","    #   print('*********************************************************') \n","\n","    return x"],"metadata":{"id":"m7v9Y-Lep4CD","executionInfo":{"status":"ok","timestamp":1681466154146,"user_tz":-120,"elapsed":5,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["encoder_bert = EncoderBert(bert_encoder=tfhub_handle_encoder, \n","                           embedding_dim=EMBEDDING_DIM, \n","                           max_len=MAX_SEQ_LENGTH,\n","                           rate=DROPUOT,\n","                           trainable=trainable)\n","\n","bert_outputs = encoder_bert(enc_input, debug) "],"metadata":{"id":"Q08luTkusEfn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466173826,"user_tz":-120,"elapsed":19685,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"ff52af78-3700-4258-ce70-9323248d45c1"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["****************** DEBUG ENCODER BERT ******************\n","First example\n","Keys                         : ['input_word_ids', 'input_mask']\n","Shape                        : (32, 128)\n","Word Ids                     : [  101   140   112 10176 63651 10608   112 11117 10176 12541 17656   117\n"," 10614 11117 10176   262]\n","Input Mask                   : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","\n","Encoder Outputs BERT Shape   : (32, 128, 768)\n","Encoder Outputs BERT Values  : [[-0.0093045   0.10829777 -0.04312363  0.0431083  -0.23877075  0.17301273\n","  -0.04629159  0.20771399  0.0983489   0.5058314   0.24097222 -0.16872738\n","  -0.3057204   0.32812995 -0.5659584   0.09819015]]\n","\n","Sequence Conv1 Shape         : (32, 128, 512)\n","Sequence Conv2 Shape         : (32, 128, 256)\n","Sequence Conv3 Shape         : (32, 128, 128)\n","Sequence MLP-Mixer Shape     : (32, 32, 128)\n","\n","Sequence Outputs Values      : [-0.58419627 -0.4106043  -0.39505738 -0.21475509 -0.6917261  -0.21466152\n"," -0.92724204  0.1105772  -0.2275325  -0.80182934 -0.82815486 -0.23576094\n"," -0.31855994 -0.07493272 -0.0864995  -0.33371243]\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## Decoder\n","\n","Predispondo la classe necessaria per la costruzione di un Layer di Decoder"],"metadata":{"id":"ReEQ5rX7aGtl"}},{"cell_type":"markdown","source":["### TOKEN AND POSITION EMBEDDING\n","\n","Implementazione del blocco Embedding per l'utilizzo di vettori posizionali insieme ai vettori di token di parole tramite estensione della classe Layer di Keras. "],"metadata":{"id":"gAu1IXlRZzlq"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, maxlen, vocab_size, embed_dim):\n","    super(TokenAndPositionEmbedding, self).__init__()\n","    self.maxlen = maxlen\n","    self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","    self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","  def call(self, x, debug=False):\n","    x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=self.maxlen, padding='post')\n","    maxlen = tf.shape(x)[-1]\n","\n","    if debug:\n","      print('********** DEBUG TOKEN AND POSITION EMBEDDING ***********')\n","      print(f'Sequence Max len                          : {maxlen}')\n","      print(f'Sequence Shape                            : {tf.shape(x)}')\n","\n","    positions = tf.range(start=0, limit=maxlen, delta=1)\n","    positions = self.pos_emb(positions)\n","    x = self.token_emb(x)\n","    output = x + positions\n","\n","    if debug:\n","      print(f'Shape TokenAndPositionEmbedding           : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"o9-RSKTqsmUC","executionInfo":{"status":"ok","timestamp":1681466173827,"user_tz":-120,"elapsed":28,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["token_position_it = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.ita.get_vocab_size(), EMBEDDING_DIM)\n","\n","inputs_decoder = token_position_it(dec_input, debug)"],"metadata":{"id":"rr_EWQUX8EWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466173827,"user_tz":-120,"elapsed":24,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"64cb6ab3-fc1c-44d7-8070-bf3f569a2eca"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 128\n","Sequence Shape                            : [ 32 128]\n","Shape TokenAndPositionEmbedding           : (32, 128, 128)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### LAYER DECODER\n","\n","Implementazione di un blocco di DecoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"XdLv-6nidKGK"}},{"cell_type":"markdown","source":["#### DecodeBert\n","\n","Implmentazione di un blocco di  decodifica custom per decodificare l'output dal layer EncoderBert prima di passarlo al Decoder del Transformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"_iq7Y-d4eRd8"}},{"cell_type":"code","source":["class DecodeBert(tf.keras.layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DecodeBert'):\n","    super(DecodeBert, self).__init__()\n","    self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = tf.keras.Sequential(\n","      [tf.keras.layers.Dense(ff_dim, activation='relu'), \n","       tf.keras.layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = tf.keras.layers.LayerNormalization()\n","    self.layernorm2 = tf.keras.layers.LayerNormalization()\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, bert_outputs, training=False, debug=False):\n","    attn_output = self.att(query=bert_outputs,\n","                           value=bert_outputs, \n","                           key=bert_outputs)\n","    \n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(bert_outputs + attn_output)\n","\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","\n","    output = self.layernorm2(out1 + ffn_output)\n","\n","    if debug:\n","      print('********************* DEBUG DECODE-BERT *********************')\n","      print(f'Shape Input Layer Decode-Bert       : {bert_outputs.shape}')\n","      print(f'Shape Output Layer Decode-Bert      : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"joTBTlWF8ETD","executionInfo":{"status":"ok","timestamp":1681466173827,"user_tz":-120,"elapsed":18,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["encoder = DecodeBert(MAX_SEQ_LENGTH, \n","                     EMBEDDING_DIM, \n","                     NUM_HEADS, \n","                     FF_DIM, \n","                     DROPUOT)\n","\n","outputs_encoder = encoder(bert_outputs=bert_outputs,\n","                          training=training, \n","                          debug=debug)"],"metadata":{"id":"JaIzBxFCfKe9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466174957,"user_tz":-120,"elapsed":1148,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"afc71475-4852-457a-ef15-12b9bec5bee0"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (32, 32, 128)\n","Shape Output Layer Decode-Bert      : (32, 32, 128)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["#### Layer Decoder"],"metadata":{"id":"dMTKLwd3dRw5"}},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DEC'):\n","    super(Decoder, self).__init__()\n","    self.decode_bert = DecodeBert(max_len=max_len, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, rate=rate)\n","    self.att1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.att2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = tf.keras.Sequential(\n","      [tf.keras.layers.Dense(ff_dim, activation='relu'), \n","       tf.keras.layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = tf.keras.layers.LayerNormalization()\n","    self.layernorm2 = tf.keras.layers.LayerNormalization()\n","    self.layernorm3 = tf.keras.layers.LayerNormalization()\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","    self._name = name\n","\n","  def call(self, inputs, bert_outputs, training=False, debug=False):\n","    attn_output1 = self.att1(query=inputs,\n","                             value=inputs, \n","                             key=inputs, \n","                             use_causal_mask=True)\n","    \n","    attn_output1 = self.dropout1(attn_output1)\n","    out1 = self.layernorm1(inputs + attn_output1)\n","\n","    dec_bert = self.decode_bert(bert_outputs=bert_outputs, training=training, debug=debug)\n","\n","    attn_output2 = self.att2(key=dec_bert, \n","                             value=dec_bert, \n","                             query=out1)\n","    \n","    attn_output2 = self.dropout2(attn_output2, training=training)\n","    out2 = self.layernorm2(out1 + attn_output2)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","    \n","    output = self.layernorm3(out2 + ffn_output)\n","\n","    if debug:\n","      print('******************* DEBUG DECODER ***********************')\n","      print(f'Input Shape                       : {inputs.shape}')\n","      print(f'Shape Outputs Decoder             : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"SO5rYsFpfFS_","executionInfo":{"status":"ok","timestamp":1681466174957,"user_tz":-120,"elapsed":18,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_decoder = decoder(inputs=inputs_decoder, \n","                          bert_outputs=bert_outputs,  \n","                          training=training,\n","                          debug=debug)"],"metadata":{"id":"yysVdkHH8EPH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466174958,"user_tz":-120,"elapsed":15,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"904a35e6-7ecc-493a-9974-cf0214156d66"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (32, 32, 128)\n","Shape Output Layer Decode-Bert      : (32, 32, 128)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 128, 128)\n","Shape Outputs Decoder             : (32, 128, 128)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## TRANSFORMER\n","\n","Implementazione del blocco Transformer tramite estensione della classe Layer di Keras."],"metadata":{"id":"ne4zTOG_NKfV"}},{"cell_type":"code","execution_count":41,"metadata":{"pycharm":{"name":"#%%\n"},"id":"lw2xMCAMC_4M","executionInfo":{"status":"ok","timestamp":1681466174958,"user_tz":-120,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"outputs":[],"source":["class TransformerBlock(tf.keras.Model):\n","  def __init__(self, \n","               num_layers, \n","               embed_dim,\n","               hidden_dim, \n","               num_heads, \n","               ff_dim, \n","               max_len,\n","               vocab_size,\n","               tfhub_handle_encoder,\n","               trainable,\n","               rate=0.5):\n","    \n","    super(TransformerBlock, self).__init__()\n","\n","    self.num_layers = num_layers\n","\n","    self.token_pos_dec = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n","\n","    self.encoder = EncoderBert(tfhub_handle_encoder, embed_dim, max_len, trainable=trainable)\n","    self.decoder = [Decoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","    self.final_layer = tf.keras.Sequential(\n","        [\n","            tf.keras.layers.LSTM(hidden_dim, return_sequences=True, name = 'LSTM1'),\n","            tf.keras.layers.LSTM(hidden_dim*2, return_sequences=True, name = 'LSTM2'),\n","            tf.keras.layers.Dense(vocab_size, name='final_layer'),\n","        ]\n","    )\n","\n","  def call(self, inputs, training=False, debug=False):\n","    inputs_encoder, inputs_decoder  = inputs\n","\n","    encoder_output = self.encoder(inputs_encoder, debug) \n","\n","    inputs_decoder = self.token_pos_dec(inputs_decoder, debug)\n","\n","    if debug:\n","      print(f'---------------- DEBUG TRANSFORMER BLOCK ----------------')\n","      print(f'inputs_encoder       : {inputs_encoder[\"input_word_ids\"].shape}')\n","      print(f'inputs_decoder       : {inputs_decoder.shape}')      \n","\n","    transformer_output = inputs_decoder\n","      \n","    for i in range(self.num_layers):\n","      transformer_output = self.decoder[i](inputs=transformer_output, \n","                                           bert_outputs=encoder_output, \n","                                           training=training,\n","                                           debug=debug)\n","\n","    transformer_output = self.dropout(transformer_output)   \n","    transformer_output = self.final_layer(transformer_output)\n","\n","    if debug:\n","      print(f'Output Shape       : {transformer_output.shape}')\n","      print(f'Output Transformer : {transformer_output[0, :1, :12]}')    \n","      print(f'---------------------------------------------------------')\n","\n","    return transformer_output"]},{"cell_type":"code","source":["transformer = TransformerBlock(NUM_LAYERS, \n","                               EMBEDDING_DIM, \n","                               HIDDEN_DIM,\n","                               NUM_HEADS, \n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.ita.get_vocab_size(),\n","                               tfhub_handle_encoder,\n","                               trainable,\n","                               DROPUOT)\n","\n","transformer_output = transformer((enc_input, dec_input), \n","                                 training=training,\n","                                 debug=debug)"],"metadata":{"id":"pr--G0ZZVAMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466192783,"user_tz":-120,"elapsed":17833,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"92337183-65d5-49f9-c3e2-cb57f4a9bd37"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["****************** DEBUG ENCODER BERT ******************\n","First example\n","Keys                         : ['input_word_ids', 'input_mask']\n","Shape                        : (32, 128)\n","Word Ids                     : [  101   140   112 10176 63651 10608   112 11117 10176 12541 17656   117\n"," 10614 11117 10176   262]\n","Input Mask                   : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n","\n","Encoder Outputs BERT Shape   : (32, 128, 768)\n","Encoder Outputs BERT Values  : [[-0.0093045   0.10829777 -0.04312363  0.0431083  -0.23877075  0.17301273\n","  -0.04629159  0.20771399  0.0983489   0.5058314   0.24097222 -0.16872738\n","  -0.3057204   0.32812995 -0.5659584   0.09819015]]\n","\n","Sequence Conv1 Shape         : (32, 128, 512)\n","Sequence Conv2 Shape         : (32, 128, 256)\n","Sequence Conv3 Shape         : (32, 128, 128)\n","Sequence MLP-Mixer Shape     : (32, 32, 128)\n","\n","Sequence Outputs Values      : [-1.9728296 -3.0371788  0.         0.        -2.2953012  0.\n","  0.         0.         0.        -1.3227853  0.         0.\n"," -2.2762825  0.         0.        -2.2852237]\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 128\n","Sequence Shape                            : [ 32 128]\n","Shape TokenAndPositionEmbedding           : (32, 128, 128)\n","*********************************************************\n","---------------- DEBUG TRANSFORMER BLOCK ----------------\n","inputs_encoder       : (32, 128)\n","inputs_decoder       : (32, 128, 128)\n","********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (32, 32, 128)\n","Shape Output Layer Decode-Bert      : (32, 32, 128)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 128, 128)\n","Shape Outputs Decoder             : (32, 128, 128)\n","*********************************************************\n","Output Shape       : (32, 128, 23120)\n","Output Transformer : [[ 0.00514602 -0.00941351  0.00545708  0.00296523  0.00327502  0.00449253\n","  -0.00649774 -0.00017409  0.00113525  0.00114662 -0.0005044  -0.00333002]]\n","---------------------------------------------------------\n"]}]},{"cell_type":"code","source":["transformer.summary()"],"metadata":{"id":"0kYt6ehvh-8B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466192784,"user_tz":-120,"elapsed":47,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"66d3b062-aefc-462e-fe40-f2239c3bb3d2"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer_block\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," token_and_position_embeddin  multiple                 2975744   \n"," g_1 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," encoder_bert_1 (EncoderBert  multiple                 135297472 \n"," )                                                               \n","                                                                 \n"," DEC (Decoder)               multiple                  1600448   \n","                                                                 \n"," dropout_18 (Dropout)        multiple                  0         \n","                                                                 \n"," sequential_7 (Sequential)   (32, 128, 23120)          77269584  \n","                                                                 \n","=================================================================\n","Total params: 217,143,248\n","Trainable params: 82,409,168\n","Non-trainable params: 134,734,080\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Addestramento Modello"],"metadata":{"id":"IFmcHTSDTvYk"}},{"cell_type":"markdown","source":["### Compilazione"],"metadata":{"id":"tiuqPlHo0Z0n"}},{"cell_type":"code","source":["transformer.compile(\n","  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_ADAM, \n","                                     beta_1=BETA_1, \n","                                     beta_2=BETA_2),\n","  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"],"metadata":{"id":"bOyqCyjIr-L2","executionInfo":{"status":"ok","timestamp":1681466192784,"user_tz":-120,"elapsed":26,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["### Callbacks"],"metadata":{"id":"-z6qj1uclHRa"}},{"cell_type":"code","source":["# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 save_best_only=True)\n","\n","# Create a callback Tensorboard\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","\n","# Create a callback save the log history\n","json_logging_callback = tf.keras.callbacks.LambdaCallback(\n","  on_epoch_end=lambda epoch, logs: json_log.write(\n","    json.dumps({'epoch': epoch, \n","                'loss': logs['loss'],\n","                'sparse_categorical_accuracy': logs['sparse_categorical_accuracy'],\n","                'val_loss': logs['val_loss'],\n","                'val_sparse_categorical_accuracy': logs['val_sparse_categorical_accuracy']}) + '\\n'),\n","  on_train_end=lambda logs: json_log.close()\n",")"],"metadata":{"id":"3hurmpSjJ_dT","executionInfo":{"status":"ok","timestamp":1681466192784,"user_tz":-120,"elapsed":24,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["### Train Ita"],"metadata":{"id":"Day7C7Qh0b4G"}},{"cell_type":"code","source":["start = datetime.datetime.now()\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_ita,\n","                initial_epoch=0,\n","                epochs=EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset_ita,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"etOGtBcer9yi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3db5e838-845b-4011-f31e-6ce697b4ec56","executionInfo":{"status":"ok","timestamp":1681424550088,"user_tz":-120,"elapsed":8573792,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","542/542 [==============================] - 856s 2s/step - loss: 0.6656 - sparse_categorical_accuracy: 0.9239 - val_loss: 0.4914 - val_sparse_categorical_accuracy: 0.9321\n","Epoch 2/10\n","542/542 [==============================] - 849s 2s/step - loss: 0.4751 - sparse_categorical_accuracy: 0.9326 - val_loss: 0.4627 - val_sparse_categorical_accuracy: 0.9332\n","Epoch 3/10\n","542/542 [==============================] - 851s 2s/step - loss: 0.4457 - sparse_categorical_accuracy: 0.9342 - val_loss: 0.4367 - val_sparse_categorical_accuracy: 0.9353\n","Epoch 4/10\n","542/542 [==============================] - 851s 2s/step - loss: 0.4204 - sparse_categorical_accuracy: 0.9357 - val_loss: 0.4172 - val_sparse_categorical_accuracy: 0.9369\n","Epoch 5/10\n","542/542 [==============================] - 811s 1s/step - loss: 0.3984 - sparse_categorical_accuracy: 0.9373 - val_loss: 0.4038 - val_sparse_categorical_accuracy: 0.9380\n","Epoch 6/10\n","542/542 [==============================] - 801s 1s/step - loss: 0.3783 - sparse_categorical_accuracy: 0.9387 - val_loss: 0.3869 - val_sparse_categorical_accuracy: 0.9398\n","Epoch 7/10\n","542/542 [==============================] - 844s 2s/step - loss: 0.3583 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.3732 - val_sparse_categorical_accuracy: 0.9412\n","Epoch 8/10\n","542/542 [==============================] - 843s 2s/step - loss: 0.3396 - sparse_categorical_accuracy: 0.9418 - val_loss: 0.3639 - val_sparse_categorical_accuracy: 0.9421\n","Epoch 9/10\n","542/542 [==============================] - 845s 2s/step - loss: 0.3217 - sparse_categorical_accuracy: 0.9433 - val_loss: 0.3514 - val_sparse_categorical_accuracy: 0.9436\n","Epoch 10/10\n","542/542 [==============================] - 846s 2s/step - loss: 0.3057 - sparse_categorical_accuracy: 0.9446 - val_loss: 0.3423 - val_sparse_categorical_accuracy: 0.9449\n","Tempo necessario per l'addestramento: 2:22:53.432223\n"]}]},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PM1WykpPjkHe","executionInfo":{"status":"ok","timestamp":1681456042614,"user_tz":-120,"elapsed":13556,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"12cfb256-75a4-4b21-b645-63305d3774b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f2b57d06d60>"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_ita,\n","                initial_epoch=EPOCHS_ADAM,\n","                epochs=EPOCHS_ADAM+EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset_ita,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"IfuqU0Yt8J7q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681465161047,"user_tz":-120,"elapsed":9118461,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"574e1c6a-8cab-42a4-e9af-031ec660fcf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 11/20\n","542/542 [==============================] - 927s 2s/step - loss: 0.2906 - sparse_categorical_accuracy: 0.9462 - val_loss: 0.3341 - val_sparse_categorical_accuracy: 0.9460\n","Epoch 12/20\n","542/542 [==============================] - 913s 2s/step - loss: 0.2770 - sparse_categorical_accuracy: 0.9475 - val_loss: 0.3294 - val_sparse_categorical_accuracy: 0.9467\n","Epoch 13/20\n","542/542 [==============================] - 875s 2s/step - loss: 0.2639 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.3229 - val_sparse_categorical_accuracy: 0.9478\n","Epoch 14/20\n","542/542 [==============================] - 914s 2s/step - loss: 0.2521 - sparse_categorical_accuracy: 0.9503 - val_loss: 0.3175 - val_sparse_categorical_accuracy: 0.9486\n","Epoch 15/20\n","542/542 [==============================] - 887s 2s/step - loss: 0.2405 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.3127 - val_sparse_categorical_accuracy: 0.9495\n","Epoch 16/20\n","542/542 [==============================] - 881s 2s/step - loss: 0.2296 - sparse_categorical_accuracy: 0.9533 - val_loss: 0.3086 - val_sparse_categorical_accuracy: 0.9504\n","Epoch 17/20\n","542/542 [==============================] - 909s 2s/step - loss: 0.2193 - sparse_categorical_accuracy: 0.9549 - val_loss: 0.3058 - val_sparse_categorical_accuracy: 0.9510\n","Epoch 18/20\n","542/542 [==============================] - 872s 2s/step - loss: 0.2098 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.3016 - val_sparse_categorical_accuracy: 0.9518\n","Epoch 19/20\n","542/542 [==============================] - 858s 2s/step - loss: 0.2006 - sparse_categorical_accuracy: 0.9580 - val_loss: 0.2984 - val_sparse_categorical_accuracy: 0.9524\n","Epoch 20/20\n","542/542 [==============================] - 911s 2s/step - loss: 0.1920 - sparse_categorical_accuracy: 0.9592 - val_loss: 0.2971 - val_sparse_categorical_accuracy: 0.9528\n","Tempo necessario per l'addestramento: 2:31:59.949184\n"]}]},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"uEYbpTwKFhZY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681466203923,"user_tz":-120,"elapsed":11161,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"2f72d573-499f-42ae-9451-590e55f04957"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f74c64e6fd0>"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_ita,\n","                initial_epoch=20,\n","                epochs=20+EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset_ita,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"Ge0s5d8QFgvk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a4232dae-5b4b-4054-c17b-33f2e6df09da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 21/40\n","542/542 [==============================] - 888s 2s/step - loss: 0.1839 - sparse_categorical_accuracy: 0.9606 - val_loss: 0.2941 - val_sparse_categorical_accuracy: 0.9537\n","Epoch 22/40\n","542/542 [==============================] - 876s 2s/step - loss: 0.1757 - sparse_categorical_accuracy: 0.9619 - val_loss: 0.2917 - val_sparse_categorical_accuracy: 0.9542\n","Epoch 23/40\n","542/542 [==============================] - 866s 2s/step - loss: 0.1684 - sparse_categorical_accuracy: 0.9632 - val_loss: 0.2904 - val_sparse_categorical_accuracy: 0.9548\n","Epoch 24/40\n","542/542 [==============================] - 860s 2s/step - loss: 0.1614 - sparse_categorical_accuracy: 0.9645 - val_loss: 0.2896 - val_sparse_categorical_accuracy: 0.9550\n","Epoch 25/40\n","542/542 [==============================] - 891s 2s/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9658 - val_loss: 0.2888 - val_sparse_categorical_accuracy: 0.9557\n","Epoch 26/40\n","542/542 [==============================] - 825s 2s/step - loss: 0.1483 - sparse_categorical_accuracy: 0.9670 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.9558\n","Epoch 27/40\n","542/542 [==============================] - 817s 2s/step - loss: 0.1422 - sparse_categorical_accuracy: 0.9682 - val_loss: 0.2878 - val_sparse_categorical_accuracy: 0.9560\n","Epoch 28/40\n","542/542 [==============================] - 879s 2s/step - loss: 0.1364 - sparse_categorical_accuracy: 0.9692 - val_loss: 0.2862 - val_sparse_categorical_accuracy: 0.9569\n","Epoch 29/40\n","542/542 [==============================] - 893s 2s/step - loss: 0.1308 - sparse_categorical_accuracy: 0.9703 - val_loss: 0.2842 - val_sparse_categorical_accuracy: 0.9572\n","Epoch 30/40\n","542/542 [==============================] - 864s 2s/step - loss: 0.1255 - sparse_categorical_accuracy: 0.9713 - val_loss: 0.2851 - val_sparse_categorical_accuracy: 0.9575\n","Epoch 31/40\n","542/542 [==============================] - 838s 2s/step - loss: 0.1201 - sparse_categorical_accuracy: 0.9725 - val_loss: 0.2841 - val_sparse_categorical_accuracy: 0.9578\n","Epoch 32/40\n","542/542 [==============================] - 871s 2s/step - loss: 0.1154 - sparse_categorical_accuracy: 0.9735 - val_loss: 0.2842 - val_sparse_categorical_accuracy: 0.9580\n","Epoch 33/40\n","542/542 [==============================] - 829s 2s/step - loss: 0.1106 - sparse_categorical_accuracy: 0.9745 - val_loss: 0.2828 - val_sparse_categorical_accuracy: 0.9585\n","Epoch 34/40\n","542/542 [==============================] - 838s 2s/step - loss: 0.1061 - sparse_categorical_accuracy: 0.9754 - val_loss: 0.2825 - val_sparse_categorical_accuracy: 0.9588\n","Epoch 35/40\n","542/542 [==============================] - 828s 2s/step - loss: 0.1017 - sparse_categorical_accuracy: 0.9764 - val_loss: 0.2828 - val_sparse_categorical_accuracy: 0.9591\n","Epoch 36/40\n","542/542 [==============================] - 832s 2s/step - loss: 0.0975 - sparse_categorical_accuracy: 0.9772 - val_loss: 0.2836 - val_sparse_categorical_accuracy: 0.9592\n","Epoch 37/40\n","276/542 [==============>...............] - ETA: 5:55 - loss: 0.0947 - sparse_categorical_accuracy: 0.9778"]}]},{"cell_type":"markdown","source":["### Train Dante"],"metadata":{"id":"GhBGzbvrh2Rw"}},{"cell_type":"code","source":["train_dataset_dante, validation_dataset_dante, test_dataset = train_val_test_dataset(df=df, filter_column='DANTE', debug=debug)"],"metadata":{"id":"kndzuk5XVnmM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"4ejTGcN8h4we"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_dante,\n","                initial_epoch=20,\n","                epochs=20+EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset_dante,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"UZ0iTUOXh70W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_dante,\n","                initial_epoch=EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM,\n","                epochs=EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset_dante,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"3yZ-Zq0xJIZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_dante,\n","                initial_epoch=EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM,\n","                epochs=EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset_dante,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"7yibFj-xkgRP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_dante,\n","                initial_epoch=EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM,\n","                epochs=EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM+EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset_dante,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"9HtFRuW6dDEE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Valutazione dell'addestramento\n","Avendo in output il log ed i risultati dell'addestramento, possiamo visualizzare\n","queste informazioni relativamente alle metriche di interesse."],"metadata":{"id":"L0w4wF79UhAp"}},{"cell_type":"code","source":["# Recupero il log di addestramento\n","df_history = pd.read_json(log_history, lines=True)\n","\n","# visualizzazione andamento addestramento\n","# su un grafico composto da due sub-plot\n","# uno per il loss, l'altro per l'accuracy\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n","\n","# Errore durante l'addestramento\n","ax1.plot(df_history['loss'], label='Loss')\n","ax1.plot(df_history['val_loss'], label='Validation Loss')\n","ax1.set_title('Training Loss')\n","ax1.legend()\n","\n","# Accuratezza durante l'addestramento\n","ax2.plot(df_history['sparse_categorical_accuracy'], label='Accuracy')\n","ax2.plot(df_history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n","ax2.set_title('Training Accuracy')\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"RpXR2p5VAdoG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test del modello\n","La seguente cella permette di caricare l'ultimo checkpoint dell'addestramento\n","precedentemente salvato."],"metadata":{"id":"ReOkcBp2WHWW"}},{"cell_type":"code","source":["trainable = False\n","\n","transformer = TransformerBlock(NUM_LAYERS, \n","                               EMBEDDING_DIM, \n","                               HIDDEN_DIM,\n","                               NUM_HEADS, \n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.ita.get_vocab_size(),\n","                               tfhub_handle_encoder,\n","                               trainable,\n","                               DROPUOT)"],"metadata":{"id":"RN0mnV8Wd92H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"5PIf_6-RSBb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Translate:\n","  def __init__(self, transformer_block, tokenizers):\n","    self.transformer = transformer_block\n","    self.tokenizers = tokenizers\n","\n","  def predict(self, input_text, max_length):\n","    if input_text is None:\n","      input_text = (df[ORIGINAL_COLUMN].tolist())[np.random.choice(len(df[ORIGINAL_COLUMN].tolist()))]\n","      print(input_text)\n","\n","    inputs_bert = self.tokenizers.multilingual.tokenize(input_text)\n","\n","    start_end = self.tokenizers.ita.tokenize([''])[0]\n","    start = (start_end[0][tf.newaxis]).numpy()[0]\n","    end = (start_end[1][tf.newaxis]).numpy()[0]\n","\n","    output_array = tf.TensorArray(dtype=tf.int32, size=max_length, dynamic_size=True)\n","    output_array = output_array.write(0, tf.constant([start]))     \n","\n","    out_words = []\n","\n","    for i in tf.range(max_length):\n","      # decodifica e recupero probabilità di output\n","      output = tf.transpose(output_array.stack())\n","\n","      transformer_output = transformer((inputs_bert, output), \n","                                        training=False,\n","                                        debug=False)\n","\n","      predictions = transformer_output[:, -1:, :]\n","\n","      # selezione della parola più probabile\n","      predict = tf.argmax(predictions, -1)\n","      pred_values = (tf.keras.backend.argmax(transformer_output, axis=-1)).numpy()\n","    \n","      # inserimento della parola nella sequenza di output\n","      output_array = output_array.write(i+1, [pred_values[0][i]])\n","\n","      if pred_values[0][i] == end:\n","        break\n","\n","    output = tf.transpose(output_array.stack())\n","    text = tokenizers.ita.detokenize(output)[0]  \n","    tokens = tokenizers.ita.lookup(output)[0]\n","\n","    return text, tokens"],"metadata":{"id":"L2PEoJVb1V8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translate = Translate(transformer_block=transformer,\n","                      tokenizers=tokenizers)\n","\n","# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for test_input_data, test_target_data in test_dataset.take(50):\n","  test_input_data = test_input_data.numpy().decode()\n","  test_target_data = test_target_data.numpy().decode()\n","\n","  text, token = translate.predict(tf.constant([test_input_data]), MAX_SEQ_LENGTH)\n","\n","  print(f'{\"Input\":15s}: {test_input_data}')\n","  print(f'{\"Target\":15s}: {test_target_data}')\n","  print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')  \n","  print('---------------------------------------------')"],"metadata":{"id":"udIjI2jZWR6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["  text_input_data = 'Ieri sono andato al supermercato'\n","\n","  text, token = translate.predict(tf.constant([text_input_data]), MAX_SEQ_LENGTH)\n","  print(f'{\"Input\":15s}: {text_input_data}')\n","  print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')  \n","  print('---------------------------------------------')"],"metadata":{"id":"Qex8JVqvJxzp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680072300037,"user_tz":-120,"elapsed":7618,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"58ca9ac9-d032-45f1-8515-e44305d4b9cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input          : Ieri sono andato al supermercato\n","Prediction     : i ' van cherendo lor vita ,\n","---------------------------------------------\n"]}]},{"cell_type":"code","source":["  text_input_data = 'ho comprato la cocacolo'\n","\n","  text, token = translate.predict(tf.constant([text_input_data]), MAX_SEQ_LENGTH)\n","  print(f'{\"Input\":15s}: {text_input_data}')\n","  print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')  \n","  print('---------------------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9Dun8NnGaXd","executionInfo":{"status":"ok","timestamp":1680072336039,"user_tz":-120,"elapsed":8370,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"efd28f2d-c1c2-47a0-8b68-126447c92c56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input          : ho comprato la cocacolo\n","Prediction     : i ' potre ' ben tosto confortato ,\n","---------------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"UONyLudWHYCr"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}