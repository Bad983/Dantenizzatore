{"cells":[{"cell_type":"markdown","metadata":{"id":"yDKuSNBd92YI"},"source":["# DANTENIZZATORE"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17467,"status":"ok","timestamp":1687081649778,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"hJy-juNOpUOY","outputId":"a68e0808-7788-4e27-dee5-d8becdec94af"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"xXYm-Qqw-ANh"},"source":["## Import notebook"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6601,"status":"ok","timestamp":1687081656372,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"1eALqJmsL4C_"},"outputs":[],"source":["import os\n","import re\n","import datetime\n","import pathlib\n","import json\n","from pathlib import Path\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","import matplotlib.pyplot as plt\n","\n","from typing import Tuple\n","from tensorboard.plugins.hparams import api as hp"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":55521,"status":"ok","timestamp":1687081711889,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"NE4enZGpvMRX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d29705ee-eb39-4d00-b9fb-0c13a250921a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n","tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -q -U 'tensorflow-text==2.8.*'"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":77851,"status":"ok","timestamp":1687081789733,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"FPtWz_qHuofc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a1e82298-4157-4d7d-b4a6-791700870b92"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q tf-models-official"]},{"cell_type":"code","source":["import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"],"metadata":{"id":"6_ZXm1Jpys6A","executionInfo":{"status":"ok","timestamp":1687081789734,"user_tz":-120,"elapsed":20,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1687081789734,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"uKEqRlKowOQS"},"outputs":[],"source":["tf.get_logger().setLevel('ERROR')\n","tf.config.run_functions_eagerly(True)"]},{"cell_type":"markdown","metadata":{"id":"HRe16D-rUBLA"},"source":["## Variabili Globali"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1687081789734,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"ewLgCIuEpczO"},"outputs":[],"source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/Progetti/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'train_data.csv'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_data_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# PATH LOG Tensorboard\n","PATH_LOG = 'logs/fit/dantenizzatore'\n","PATH_LOG = os.path.abspath(os.path.join(root_folder, PATH_LOG))\n","log_dir =  os.path.abspath(os.path.join(PATH_LOG, datetime.datetime.now().strftime('%Y%m%d-%H%M%S')))\n","log_history = os.path.abspath(os.path.join(PATH_LOG, 'histrory.json'))\n","\n","# PATH WEIGHTS Tensorboard\n","PATH_WEIGHTS = 'weights/dantenizzatore'\n","PATH_WEIGHTS = os.path.abspath(os.path.join(root_folder, PATH_WEIGHTS))\n","checkpoint_path = os.path.abspath(os.path.join(PATH_WEIGHTS, 'cp.ckpt'))\n","\n","# VOCABOLARIO\n","vocab_folder = 'vocab'\n","multilingual_vocab_finalname = 'multilingual_vocab.txt'\n","dante_vocab_finalname = 'dante_vocab.txt'\n","\n","VOCAB_PATH = os.path.abspath(os.path.join(root_folder, vocab_folder))\n","multilingual_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, multilingual_vocab_finalname))\n","dante_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, dante_vocab_finalname))\n","\n","# parametri per il modello\n","ORIGINAL_COLUMN = 'Original'\n","TRANSLATE_COLUMN = 'Translate'\n","TYPE_COLUMN ='Type'"]},{"cell_type":"markdown","metadata":{"id":"LCiP6wT05k6j"},"source":["## Iper Parametri Modello"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1687081789735,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"8CN-4Uzoqbjl"},"outputs":[],"source":["NUM_SAMPLES = 0  # 0 == all dataset\n","TEST = 200\n","TEST_SIZE = 0.3\n","BATCH_SIZE = 64\n","\n","# Vocabolario e lunghezza max frase\n","MAX_VOCAB_SIZE = 30000\n","MAX_SEQ_LENGTH = 128\n","# Shuffle dei dati usato in preparazione del batch\n","BUFFER_SIZE = 2000\n","\n","# Numero di layer di Decoder del Transformer\n","NUM_LAYERS = 3\n","# Numero di meccanismi di multi-head attention\n","NUM_HEADS = 4\n","# Numero di celle dei Layer Feed Forward\n","FF_DIM = 512\n","EMBEDDING_DIM = 128\n","\n","# Altri iperparametri\n","LEARNING_RATE = 3e-4\n","BETA_1 = 0.66\n","BETA_2 = 0.999\n","EPSILON=1e-9\n","DROPUOT = 0.1\n","\n","EPOCHS_DANTE = 20\n","\n","debug = True\n","trainable = False\n","training = True"]},{"cell_type":"markdown","metadata":{"id":"BehZY4rETECN"},"source":["## Parametri BERT"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1687081789735,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"fodDcY6sm392","colab":{"base_uri":"https://localhost:8080/"},"outputId":"975a5218-c62f-409c-b092-76b185f6ca47"},"outputs":[{"output_type":"stream","name":"stdout","text":["BERT model name                    :  distilbert_multi_cased_L-6_H-768_A-12/1\n","BERT model selected                :  https://tfhub.dev/jeongukjae/distilbert_multi_cased_L-6_H-768_A-12/1\n","BERT preprocess                    :  https://tfhub.dev/jeongukjae/distilbert_multi_cased_preprocess/2\n"]}],"source":["bert_model_name = 'distilbert_multi_cased_L-6_H-768_A-12/1'\n","tfhub_handle_preprocess = 'https://tfhub.dev/jeongukjae/distilbert_multi_cased_preprocess/2'\n","tfhub_handle_encoder =  'https://tfhub.dev/jeongukjae/distilbert_multi_cased_L-6_H-768_A-12/1'\n","\n","if debug:\n","  print('BERT model name                    : ', bert_model_name)\n","  print('BERT model selected                : ', tfhub_handle_encoder)\n","  print('BERT preprocess                    : ', tfhub_handle_preprocess)"]},{"cell_type":"markdown","metadata":{"id":"5DPeN9Vanbvv"},"source":["## DATASET"]},{"cell_type":"markdown","metadata":{"id":"LU7AorKXT8K7"},"source":["### Caricamento Dati e Preprocessing"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1687081789736,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"Jm_Up6gyOTgW"},"outputs":[],"source":["def preprocess_sentence(w):\n","  '''\n","  Preprocessing dei testi di input, impostando tutti i caratteri\n","  minuscoli, aggiungendo uno spazio prima di ogni punto e sostituendo\n","  qualsiasi carattere con uno spazio se non è compreso nel seguente elenco:\n","  (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\", \"’\")\n","  '''\n","  w = re.sub(r\"([?.!,])\", r\" \\1 \", w)\n","  w = re.sub(r\"([’]+)\", \"'\", w)\n","\n","  w = w.replace(\"á\", \"à\")\n","  w = w.replace(\"é\", \"è\")\n","  w = w.replace(\"í\", \"ì\")\n","  w = w.replace(\"ó\", \"ò\")\n","  w = w.replace(\"ú\", \"ù\")\n","  w = w.replace('\"', \" \")\n","  w = w.replace(':', \" \")\n","  w = w.replace('«', \" \")\n","  w = w.replace('»', \" \")\n","  w = w.replace('‘', \" \")\n","  w = w.replace('-', \" \")\n","  w = w.replace('[', \" \")\n","  w = w.replace(']', \" \")\n","  w = w.replace('(', \" \")\n","  w = w.replace(')', \" \")\n","  w = w.replace(\"•\", \" \")\n","  w = w.replace(\"..\", \".\")\n","  w = w.replace(\"...\", \".\")\n","  w = w.replace(\"\\xa0\", \" \")\n","  w = w.replace(\"\\xc3\\xa8\", \" \")\n","  w = w.replace(\"\\xe2\\x80\\xaf\", \" \")\n","  w = w.replace(\"   \", \" \")\n","  w = w.replace(\"–\", \" \")\n","  w = w.replace(\"“\", \" \")\n","  w = w.replace(\"”\", \" \")\n","  w = w.replace(\"„\", \" \")\n","  w = w.replace(\"─\", \" \")\n","  w = w.replace(\"♪\", \" \")\n","  w = w.replace(\"#\", \" \")\n","  w = w.replace(\"/\", \" \")\n","  w = w.replace(\"=\", \" \")\n","  w = w.replace(\">\", \" \")\n","  w = w.replace(\"\\\\\", \" \")\n","  w = w.replace(\"`\", \" \")\n","  w = w.replace(\"¡\", \" \")\n","  w = w.replace(\"¿\", \" \")\n","  w = w.replace(\"œ\", \" \")\n","  w = w.replace(\"♗\", \" \")\n","  w = w.replace(\"♘\", \" \")\n","  w = w.replace(\"《\", \" \")\n","  w = w.replace(\"》\", \" \")\n","\n","  w = re.sub(r\"(['])\", r\"\\1 \", w)\n","\n","  w = w.replace(\" ' \", \" '\")\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","\n","  return w"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"duGPhZ_jgPVI","executionInfo":{"status":"ok","timestamp":1687081801876,"user_tz":-120,"elapsed":12147,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"442bca7b-bed5-46f3-ce59-d8bcac535936"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dati totali presenti nel Dataset               : 135184\n","----------------------------------- TRAIN SET -----------------------------------------\n","['Min önskan och vilja styrdes av gudomlig kärlek som driver solen och andra stjärnor framåt som ett hjul flyttade regelbundet . ']\n","[\" ma già volgeva il mio disio e 'l velle , sì come rota ch' igualmente è mossa , l' amor che move il sole e l' altre stelle\"]\n","[\"Con il suo arco il Dio dell' Amore mi trafisse\"]\n","[\"Lo Dio d' Amor con su' arco mi trasse\"]\n"]}],"source":["df = pd.read_csv(\n","  train_data_filenamepath,\n","  usecols=[ORIGINAL_COLUMN, TRANSLATE_COLUMN, TYPE_COLUMN],\n",")\n","\n","# Preprocessing dei dati di Input\n","df[ORIGINAL_COLUMN] = df[ORIGINAL_COLUMN].apply(lambda x : preprocess_sentence(x))\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","df[TRANSLATE_COLUMN] = df[TRANSLATE_COLUMN].apply(lambda x : preprocess_sentence(x))\n","\n","if debug:\n","  print(f'Dati totali presenti nel Dataset               : {len(df)}')\n","  print('----------------------------------- TRAIN SET -----------------------------------------')\n","  print((df[ORIGINAL_COLUMN].tolist())[-1:])\n","  print((df[TRANSLATE_COLUMN].tolist())[-1:])\n","  print((df[ORIGINAL_COLUMN].tolist())[:1])\n","  print((df[TRANSLATE_COLUMN].tolist())[:1])"]},{"cell_type":"markdown","metadata":{"id":"njyY9RWlFMWu"},"source":["## Tokenizer\n","\n","Due differenti tokenizer per la predisposizione dei dati di input:\n","\n","\n","*   EncTokenizer classe custom per la tokenizzazione dei dati di input al Layer di Encoder di Bert\n","*   DecTokenizer classe custom per la tokenizzazione dei dati di input al Layer di Decoder\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"fUG1fTAYekOy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687081807913,"user_tz":-120,"elapsed":6058,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"outputId":"3e3f4595-fe1c-4177-b822-867872145011"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]}],"source":["input_data_vocab = df[ORIGINAL_COLUMN].tolist()\n","target_data_vocab = df[TRANSLATE_COLUMN].tolist()\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_data_vocab, target_data_vocab))\n","dataset = dataset.shuffle(len(input_data_vocab)).batch(BATCH_SIZE, drop_remainder=True)\n","\n","train_multilingual = dataset.map(lambda multilingual, dante: multilingual)\n","train_dante = dataset.map(lambda multilingual, dante: dante)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"xWO-LrXJe0cF","executionInfo":{"status":"ok","timestamp":1687081807913,"user_tz":-120,"elapsed":17,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"yGdsrOoKiYUK","executionInfo":{"status":"ok","timestamp":1687081807914,"user_tz":-120,"elapsed":16,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["def cleanup_text(reserved_tokens, token_txt):\n","\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"qbKNS_uQhHLz","executionInfo":{"status":"ok","timestamp":1687081807914,"user_tz":-120,"elapsed":15,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["tokenizers = tf.Module()"]},{"cell_type":"markdown","metadata":{"id":"0KUcCnjXVjt3"},"source":["### Classe EncTokenizer\n","\n","Classe custom per la tokenizzazione dei dati di input e che crea i tre vettori necessari al layer di Encoder\n","Bert:\n","\n","\n","*   input_word_ids\n","*   input_type_ids\n","*   input_mask\n","\n","\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"Yr0izOZLembx","executionInfo":{"status":"ok","timestamp":1687081807914,"user_tz":-120,"elapsed":15,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens = {\n","  'start_of_sequence_id': 101,\n","  'end_of_segment_id': 102,\n","  'padding_id': 0,\n","  'mask_id': 103\n","}\n","\n","bert_vocab_args = dict(\n","  # The target vocabulary size\n","  vocab_size = MAX_VOCAB_SIZE,\n","  # Reserved tokens that must be included in the vocabulary\n","  reserved_tokens=reserved_tokens,\n","  # Arguments for `text.BertTokenizer`\n","  bert_tokenizer_params=bert_tokenizer_params,\n","  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","  learn_params={},\n",")"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"BwSKtlLSe7bH","executionInfo":{"status":"ok","timestamp":1687081807915,"user_tz":-120,"elapsed":15,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["exist_vocab = Path(multilingual_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  multilingual_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_multilingual.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(multilingual_vocab_filenamepath, multilingual_vocab)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"WmsNdDLNf6vr","executionInfo":{"status":"ok","timestamp":1687081807916,"user_tz":-120,"elapsed":16,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["class EncTokenizer(tf.Module):\n","  def __init__(self, tfhub_handle_preprocess):\n","    self.preprocessor = hub.KerasLayer(tfhub_handle_preprocess)\n","\n","  @tf.function\n","  def tokenize(self, strings):\n","    return self.preprocessor(strings)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"-4B-HWWcmsmz","executionInfo":{"status":"ok","timestamp":1687081829905,"user_tz":-120,"elapsed":22005,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["tokenizers.multilingual = EncTokenizer(tfhub_handle_preprocess)"]},{"cell_type":"markdown","metadata":{"id":"mICEGEzJVnvx"},"source":["### Classe DecTokenizer\n","\n","Classe custom per la tokenizzazione dei dati in lingua dantesca per il layer di Decoder\n"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"abBEnJJGV0AD","executionInfo":{"status":"ok","timestamp":1687081829905,"user_tz":-120,"elapsed":14,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens_vocab=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","  # The target vocabulary size\n","  vocab_size = MAX_VOCAB_SIZE,\n","  # Reserved tokens that must be included in the vocabulary\n","  reserved_tokens=reserved_tokens_vocab,\n","  # Arguments for `text.BertTokenizer`\n","  bert_tokenizer_params=bert_tokenizer_params,\n","  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","  learn_params={},\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"dGsP1V4nVz6S","executionInfo":{"status":"ok","timestamp":1687081829906,"user_tz":-120,"elapsed":13,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["exist_vocab = Path(dante_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  dante_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_dante.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(dante_vocab_filenamepath, dante_vocab)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"BeaD2-uLWT50","executionInfo":{"status":"ok","timestamp":1687081829907,"user_tz":-120,"elapsed":12,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["START = tf.argmax(tf.constant(reserved_tokens_vocab) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens_vocab) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape(out_type=tf.int32)[0]\n","\n","  starts = tf.fill([count,1], START)\n","  starts = tf.cast(starts, tf.int32)\n","\n","  ends = tf.fill([count,1], END)\n","  ends = tf.cast(ends, tf.int32)\n","\n","  x = tf.concat([starts, ragged, ends], axis=1)\n","  return x"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"iaAW-xm5WT1_","executionInfo":{"status":"ok","timestamp":1687081829907,"user_tz":-120,"elapsed":11,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["class DecTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens_vocab, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True, token_out_type=tf.int32)\n","    self._reserved_tokens_vocab = reserved_tokens_vocab\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:\n","\n","    # Include a tokenize signature for a batch of strings.\n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","\n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int32))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int32))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","\n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens_vocab, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens_vocab)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"svlLobM4WTzC","executionInfo":{"status":"ok","timestamp":1687081833406,"user_tz":-120,"elapsed":3509,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["tokenizers.dante = DecTokenizer(reserved_tokens_vocab, dante_vocab_filenamepath)"]},{"cell_type":"markdown","metadata":{"id":"pKZxiQ5_Whmw"},"source":["### Analisi Dati Tokenizzati"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"Jrg6TwQzW5LN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687081833407,"user_tz":-120,"elapsed":16,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"outputId":"f247a720-0095-43d1-e588-fbe4ab4fa627"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabolario Dantesco : 15269\n"]}],"source":["print(f'Vocabolario Dantesco : {tokenizers.dante.get_vocab_size()}')"]},{"cell_type":"markdown","metadata":{"id":"5QIDajkEsVU1"},"source":["## Creazione dataset\n","Utilizzo della libreria tf.data per la gestione del dataset da utilizzare.\n","Verranno creati batch di esempi che verranno utilizzati durante l'addestramento."]},{"cell_type":"code","execution_count":26,"metadata":{"id":"ESHUcQtthhd2","executionInfo":{"status":"ok","timestamp":1687081833408,"user_tz":-120,"elapsed":12,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["def split_dataset(df: pd.DataFrame,\n","                  filter_column: str,\n","                  debug: bool = False) -> Tuple:\n","\n","  dataset = (df.loc[df[TYPE_COLUMN] == filter_column]).copy()\n","\n","  if NUM_SAMPLES > 0:\n","    dataset = dataset[:NUM_SAMPLES]\n","\n","  dataset = dataset.sample(frac = 1)\n","  input_data = dataset[ORIGINAL_COLUMN].tolist()\n","  target_data = dataset[TRANSLATE_COLUMN].tolist()\n","\n","  train_input_data, validation_input_data, train_target_data, validation_target_data = train_test_split(\n","    input_data[:-TEST],\n","    target_data[:-TEST],\n","    test_size=TEST_SIZE,\n","    random_state=42,\n","    shuffle=True\n","  )\n","\n","  train_input_data = train_input_data[:(int((len(train_input_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","  train_target_data = train_target_data[:(int((len(train_target_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","\n","  validation_input_data = validation_input_data[:(int((len(validation_input_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","  validation_target_data = validation_target_data[:(int((len(validation_target_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","\n","  test_input_data = input_data[len(train_input_data)+len(validation_input_data):]\n","  test_target_data = target_data[len(train_target_data)+len(validation_target_data):]\n","\n","  if debug:\n","    print(f'Dati totali presenti nel Dataset               : {len(df)}')\n","    print(f'Dati totali presenti nel Dataset di Train      : {len(train_input_data)}')\n","    print(f'Dati totali presenti nel Dataset di Validation : {len(validation_input_data)}')\n","    print(f'Dati totali presenti nel Dataset di Test       : {len(test_input_data)}\\n')\n","\n","\n","    print('----------------------------------- TRAIN SET -----------------------------------------')\n","    print(train_input_data[-4:])\n","    print(train_target_data[-4:])\n","    print('--------------------------------- VALIDATION SET --------------------------------------')\n","    print(validation_input_data[-4:])\n","    print(validation_target_data[-4:])\n","    print('----------------------------------- TEST SET ------------------------------------------')\n","    print(test_input_data[-4:])\n","    print(test_target_data[-4:])\n","\n","    print('-------------------------------- ANALISI DATI -----------------------------------------')\n","    print(f'Esempi nel Dataset di Train                            : {len(train_input_data)}')\n","    print(f'Frase più corta nel Dataset Input di Train             : {min(train_input_data, key = len)}')\n","    print(f'Frase più corta nel Dataset Target di Train            : {min(train_target_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Input di Train             : {max(train_input_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Target di Train            : {max(train_target_data, key = len)}')\n","    print('---------------------------------------------------------------------------------------')\n","    print(f'Esempi nel Dataset di Validation                       : {len(validation_input_data)}')\n","    print(f'Frase più corta nel Dataset Input di Validation        : {min(validation_input_data, key = len)}')\n","    print(f'Frase più corta nel Dataset Target di Validation       : {min(validation_target_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Input di Validation        : {max(validation_input_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Target di Validation       : {max(validation_target_data, key = len)}')\n","    print('---------------------------------------------------------------------------------------')\n","    print(f'Esempi nel Dataset di Test                             : {len(test_input_data)}')\n","    print(f'Frase più corta nel Dataset Input di Test              : {min(test_input_data, key = len)}')\n","    print(f'Frase più corta nel Dataset Target di Test             : {min(test_target_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Input di Test              : {max(test_input_data, key = len)}')\n","    print(f'Frase più lunga nel Dataset Target di Test             : {max(test_target_data, key = len)}')\n","\n","    print('\\n--------------------------------- EXAMPLE ---------------------------------------------')\n","    print([min(train_input_data, key = len)])\n","    print(tokenizers.multilingual.tokenize([min(train_input_data, key = len)])['input_word_ids'][:, :32])\n","    print('------------------------------------------------------------------')\n","    print([min(train_target_data, key = len)])\n","    print(tokenizers.dante.tokenize([min(train_target_data, key = len)]))\n","    print('\\n')\n","    print([max(train_input_data, key = len)])\n","    print(tokenizers.multilingual.tokenize([max(train_input_data, key = len)])['input_word_ids'])\n","    print('------------------------------------------------------------------')\n","    print([max(train_target_data, key = len)])\n","    print(tokenizers.dante.tokenize([max(train_target_data, key = len)]))\n","\n","  return train_input_data, validation_input_data, test_input_data, train_target_data, validation_target_data, test_target_data"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ccH3jHoABPzV","executionInfo":{"status":"ok","timestamp":1687081833408,"user_tz":-120,"elapsed":11,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["def prepare_batch(multilingual, dante):\n","  zero = tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH], tf.int32)\n","\n","  # Tokenizzo l'input per l'Encoder\n","  encoder = tokenizers.multilingual.tokenize(multilingual)\n","\n","  # Tokenizzo l'input per il Decoder e creo la variabile Target\n","  dante = tokenizers.dante.tokenize(dante)\n","  decoder = dante[:, :-1].to_tensor()\n","  target = dante[:, 1:].to_tensor()\n","\n","  decoder = tf.concat([decoder, zero], 1)\n","  decoder = decoder[:, :(MAX_SEQ_LENGTH)]\n","\n","  target = tf.concat([target, zero], 1)\n","  target = target[:, :(MAX_SEQ_LENGTH)]\n","\n","  return (encoder, decoder), target"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"l_dswlCiBTdR","executionInfo":{"status":"ok","timestamp":1687081833409,"user_tz":-120,"elapsed":11,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["def make_batches(ds):\n","  return (\n","    ds\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE)\n","    .map(prepare_batch, tf.data.AUTOTUNE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE))"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"tktJ5YuIsYe3","executionInfo":{"status":"ok","timestamp":1687081833914,"user_tz":-120,"elapsed":515,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["def train_val_test_dataset(df: pd.DataFrame,\n","                          filter_column: str,\n","                          debug: bool = False) -> Tuple:\n","\n","  # Recupero il dataset\n","  train_input_data, validation_input_data, test_input_data, train_target_data, validation_target_data, test_target_data = split_dataset(df=df, filter_column=filter_column, debug=debug)\n","\n","  # Definizione del dataset\n","  train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_target_data))\n","  validation_dataset = tf.data.Dataset.from_tensor_slices((validation_input_data, validation_target_data))\n","  test_dataset = tf.data.Dataset.from_tensor_slices((test_input_data, test_target_data))\n","\n","  train_dataset = make_batches(train_dataset)\n","  validation_dataset = make_batches(validation_dataset)\n","\n","  return train_dataset, validation_dataset, test_dataset"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"nkcyLV1qqQHL","executionInfo":{"status":"ok","timestamp":1687081841565,"user_tz":-120,"elapsed":7660,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9710961-88f2-4a79-b58f-88f2916f39bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dati totali presenti nel Dataset               : 135184\n","Dati totali presenti nel Dataset di Train      : 94464\n","Dati totali presenti nel Dataset di Validation : 40448\n","Dati totali presenti nel Dataset di Test       : 272\n","\n","----------------------------------- TRAIN SET -----------------------------------------\n","['so erschrocken durch seine Einschüchterung', 'Ma Falsenbiante non si azzarda a inviare', ' De banken waren op deze manier gebouwd , hoewel hun schepper niet van plan was ze zo massaal en groots te maken . ', \"Ceux qui sont honorès par la grâce divine devraient prendre par exemple la mythologie pour l' expèrimenter directement . \"]\n","['Sì forte ridottava suo minaccia . ', \"Ma Falsenbiante trametter non s' osa\", ' a tale imagine eran fatti quelli , tutto che nè sì alti nè sì grossi , qual che si fosse , lo maestro fèlli', \" però l' essemplo basti a cui esperïenza grazia serba\"]\n","--------------------------------- VALIDATION SET --------------------------------------\n","['che credette di avermi tolto le mie gioie . ', \"Bèatrice conduit d' un Ciel à l' autre si vite , que son acte est pratiquement instantanè\", 'Auf diese Weise tröstete mich mein gütiger Freund . ', 'Dopo che le altre donne hanno ceduto il campo , facendole posto per parlare , lei si alzò in piedi con aria indignata e rispose . ']\n","['Che lle mie gioie mi credette aver tolte , ', \" È Bëatrice quella che sì scorge di bene in meglio , sì subitamente che l' atto suo per tempo non si sporge\", 'Così mi confortò il buon Amico , ', \" Ma poi che l' altre vergini dier loco a lei di dir , levata dritta in pè , rispuose , colorata come foco\"]\n","----------------------------------- TEST SET ------------------------------------------\n","['och gjorde himmelen ovantill , full av fukt , så att den förvandlades till regn . ', 'Op deze manier zul je zijn genegenheid en dapperheid winnen . ', 'From which your gaze returns to me , it is the brightness of a soul that had been oppressed by deep worries , to the point that death seemed never to come . ', \"Der Engel , der zuerst angekommen war , öffnete seine Flügel vor ihr und sang 'Hallo , Maria voller Gnade' . \"]\n","[\" e 'l ciel di sopra fece intento , sì che 'l pregno aere in acqua si converse\", \"Così avrai il su' amor , e 'l su' coragio . \", \" Questi onde a me ritorna il tuo riguardo , è 'l lume d' uno spirto che 'n pensieri gravi a morir li parve venir tardo\", \" e quello amor che primo lì discese , cantando Ave , Maria , gratïa plena' , dinanzi a lei le sue ali distese\"]\n","-------------------------------- ANALISI DATI -----------------------------------------\n","Esempi nel Dataset di Train                            : 94464\n","Frase più corta nel Dataset Input di Train             : zei\n","Frase più corta nel Dataset Target di Train            :  E io\n","Frase più lunga nel Dataset Input di Train             : Sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal ! \n","Frase più lunga nel Dataset Target di Train            :  E già venìa su per le torbide onde un fracasso d' un suon , pien di spavento , per cui tremavano amendue le sponde , non altrimenti fatto che d' un vento impetüoso per li avversi ardori , che fier la selva e sanz' alcun rattento li rami schianta , abbatte e porta fori\n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Validation                       : 40448\n","Frase più corta nel Dataset Input di Validation        : zei\n","Frase più corta nel Dataset Target di Validation       :  E io\n","Frase più lunga nel Dataset Input di Validation        : Meinst du , wenn ich im ehrwürdigen Alter eines Ältesten gestorben bin , wäre dein Ruhm größer als das , was du gehabt hättest , wenn du als Kind gestorben wärst ? Tausend Jahre ist eine unbedeutende Zeitspanne im Vergleich zur Ewigkeit und ist weniger dauerhaft als der Blitz eines Flügelblitzs im Vergleich zu der sehr langsamen himmlischen Bewegung ? \n","Frase più lunga nel Dataset Target di Validation       :  E già venìa su per le torbide onde un fracasso d' un suon , pien di spavento , per cui tremavano amendue le sponde , non altrimenti fatto che d' un vento impetüoso per li avversi ardori , che fier la selva e sanz' alcun rattento li rami schianta , abbatte e porta fori\n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Test                             : 272\n","Frase più corta nel Dataset Input di Test              : Poi mi disse\n","Frase più corta nel Dataset Target di Test             :  Indi spirò\n","Frase più lunga nel Dataset Input di Test              :  El Omnisciente que trasciende cada lìmite creò los cielos y estableciò las esferas de los àngeles para gobernarlos , de modo que su luz pudiera reflejarse en un cìrculo eterno de refracciones entre las estrellas del Universo . \n","Frase più lunga nel Dataset Target di Test             :  e però nel secondo giron convien che sanza pro si penta qualunque priva sè del vostro mondo , biscazza e fonde la sua facultade , e piange là dov' esser de' giocondo\n","\n","--------------------------------- EXAMPLE ---------------------------------------------\n","['zei']\n","tf.Tensor(\n","[[  101 10941 10116   102     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(1, 32), dtype=int32)\n","------------------------------------------------------------------\n","[' E io']\n","<tf.RaggedTensor [[2, 14, 44, 3]]>\n","\n","\n","['Sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal , sieh mal ! ']\n","tf.Tensor(\n","[[  101 11583 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237 15189   117 10632 10237 15189\n","    117 10632 10237 15189   117 10632 10237   102]], shape=(1, 128), dtype=int32)\n","------------------------------------------------------------------\n","[\" E già venìa su per le torbide onde un fracasso d' un suon , pien di spavento , per cui tremavano amendue le sponde , non altrimenti fatto che d' un vento impetüoso per li avversi ardori , che fier la selva e sanz' alcun rattento li rami schianta , abbatte e porta fori\"]\n","<tf.RaggedTensor [[2, 14, 74, 727, 68, 41, 47, 14536, 133, 60, 5018, 13, 5, 60, 725, 6,\n","  715, 39, 4036, 6, 41, 123, 14692, 732, 47, 14100, 6, 40, 774, 141, 36,\n","  13, 5, 60, 416, 10255, 41, 48, 4407, 4335, 6, 36, 1123, 37, 722, 14,\n","  481, 5, 196, 12626, 48, 1563, 3004, 6, 6438, 14, 280, 903, 3]]>\n"]}],"source":["train_dataset_dante, validation_dataset_dante, test_dataset = train_val_test_dataset(df=df,\n","                                                                                     filter_column='DANTE',\n","                                                                                     debug=debug)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"VH_aKPlV_AWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687081843223,"user_tz":-120,"elapsed":1683,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"outputId":"37a0d254-544f-4916-9ebb-68958d512e2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------- ENCODER  -------------------------------\n","Shape                    : {'input_word_ids': <tf.Tensor: shape=(64, 128), dtype=int32, numpy=\n","array([[  101, 18082, 10637, ...,     0,     0,     0],\n","       [  101, 10104, 10847, ...,     0,     0,     0],\n","       [  101, 10159, 22272, ...,     0,     0,     0],\n","       ...,\n","       [  101, 10159, 20111, ...,     0,     0,     0],\n","       [  101, 20328, 10120, ...,     0,     0,     0],\n","       [  101, 10282, 10911, ...,     0,     0,     0]], dtype=int32)>, 'input_mask': <tf.Tensor: shape=(64, 128), dtype=int32, numpy=\n","array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       ...,\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}\n","Shape                    : (64, 128)\n","Word Ids                 : [  101 18082 10637 10737 10196 60259 10246   177 49433 66558 59719 10116]\n","Input Mask               : [1 1 1 1 1 1 1 1 1 1 1 1]\n","--------------------- DECODER ----------------------------------\n","Shape it input           : (64, 128)\n","Example it input         : [   2  120  192  206  109    6   36   83 5582    7    0    0]\n","--------------------- TARGET -----------------------------------\n","Shape it input           : (64, 128)\n","Example it target        : [ 120  192  206  109    6   36   83 5582    7    3    0    0]\n"]}],"source":["# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for (enc_input, dec_input), target in train_dataset_dante.take(1):\n","  print('----------------------- ENCODER  -------------------------------')\n","  print(f'Shape                    : {enc_input}')\n","  print(f'Shape                    : {enc_input[\"input_word_ids\"].shape}')\n","  print(f'Word Ids                 : {enc_input[\"input_word_ids\"][0, :12]}')\n","  print(f'Input Mask               : {enc_input[\"input_mask\"][0, :12]}')\n","  print('--------------------- DECODER ----------------------------------')\n","  print(f'Shape it input           : {dec_input.shape}')\n","  print(f'Example it input         : {dec_input[0][:12]}')\n","  print('--------------------- TARGET -----------------------------------')\n","  print(f'Shape it input           : {target.shape}')\n","  print(f'Example it target        : {target[0][:12]}')"]},{"cell_type":"markdown","metadata":{"id":"8dtVuZGJpvXl"},"source":["## Encoder BERT\n","\n","Predispondo la classe necessaria per la costruzione di BERT\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"m7v9Y-Lep4CD","executionInfo":{"status":"ok","timestamp":1687081843224,"user_tz":-120,"elapsed":9,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["class EncoderBert(tf.keras.layers.Layer):\n","  def __init__(self, bert_encoder, name='EncoderBert', trainable=False):\n","    super(EncoderBert, self).__init__()\n","\n","    self.encoder = hub.KerasLayer(bert_encoder, name='BERT_encoder', trainable=trainable)\n","\n","  def call(self, x, debug=False):\n","\n","    if debug:\n","      print(f'****************** DEBUG ENCODER BERT ******************')\n","      print(f\"First example\")\n","      print(f'Keys                         : {list(x.keys())}')\n","      print(f'Shape                        : {x[\"input_word_ids\"].shape}')\n","      print(f'Word Ids                     : {x[\"input_word_ids\"][0, :16]}')\n","      print(f'Input Mask                   : {x[\"input_mask\"][0, :16]}')\n","\n","    x = self.encoder(x)['sequence_output']\n","\n","    if debug:\n","      print()\n","      print(f'Encoder Outputs BERT Shape   : {x.shape}')\n","      print(f'Encoder Outputs BERT Values  : {x[0, :1, :16]}')\n","      print('*********************************************************')\n","\n","    return x"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Q08luTkusEfn","executionInfo":{"status":"ok","timestamp":1687081859913,"user_tz":-120,"elapsed":16695,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df4c12a5-a616-4252-8df7-34b555253149"},"outputs":[{"output_type":"stream","name":"stdout","text":["****************** DEBUG ENCODER BERT ******************\n","First example\n","Keys                         : ['input_word_ids', 'input_mask']\n","Shape                        : (64, 128)\n","Word Ids                     : [  101 18082 10637 10737 10196 60259 10246   177 49433 66558 59719 10116\n","   119   102     0     0]\n","Input Mask                   : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n","\n","Encoder Outputs BERT Shape   : (64, 128, 768)\n","Encoder Outputs BERT Values  : [[-0.03197939  0.09959724 -0.11866057  0.41455346 -0.04438442  0.02337466\n","   0.18171097 -0.16623658  0.06744681  0.21266855  0.07381846  0.04000761\n","  -0.01157836 -0.00687177 -0.45601267  0.07824986]]\n","*********************************************************\n"]}],"source":["encoder_bert = EncoderBert(bert_encoder=tfhub_handle_encoder,\n","                           trainable=trainable)\n","\n","bert_outputs = encoder_bert(enc_input, debug)"]},{"cell_type":"markdown","metadata":{"id":"ReEQ5rX7aGtl"},"source":["## Decoder\n","\n","Predispondo la classe necessaria per la costruzione di un Layer di Decoder"]},{"cell_type":"markdown","metadata":{"id":"gAu1IXlRZzlq"},"source":["### TOKEN AND POSITION EMBEDDING\n","\n","Implementazione del blocco Embedding per l'utilizzo di vettori posizionali insieme ai vettori di token di parole tramite estensione della classe Layer di Keras."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"o9-RSKTqsmUC","executionInfo":{"status":"ok","timestamp":1687081859913,"user_tz":-120,"elapsed":30,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, maxlen, vocab_size, embed_dim):\n","    super(TokenAndPositionEmbedding, self).__init__()\n","    self.maxlen = maxlen\n","    self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim, name='Token_Embedding')\n","    self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim, name='Positional_Embedding')\n","\n","  def call(self, x, debug=False):\n","    x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=self.maxlen, padding='post')\n","    maxlen = tf.shape(x)[-1]\n","\n","    if debug:\n","      print('********** DEBUG TOKEN AND POSITION EMBEDDING ***********')\n","      print(f'Sequence Max len                          : {maxlen}')\n","      print(f'Sequence Shape                            : {tf.shape(x)}')\n","\n","    positions = tf.range(start=0, limit=maxlen, delta=1)\n","    positions = self.pos_emb(positions)\n","    x = self.token_emb(x)\n","    output = x + positions\n","\n","    if debug:\n","      print(f'Shape TokenAndPositionEmbedding           : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"rr_EWQUX8EWP","executionInfo":{"status":"ok","timestamp":1687081939245,"user_tz":-120,"elapsed":283,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bd8903dc-5582-41f2-bb78-c3da68747772"},"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 128\n","Sequence Shape                            : [ 64 128]\n","Shape TokenAndPositionEmbedding           : (64, 128, 128)\n","*********************************************************\n"]}],"source":["token_position_dante = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.dante.get_vocab_size(), EMBEDDING_DIM)\n","\n","inputs_decoder = token_position_dante(dec_input, debug)"]},{"cell_type":"markdown","metadata":{"id":"XdLv-6nidKGK"},"source":["### LAYER DECODER\n","\n","Implementazione di un blocco di DecoderTransformer tramite estensione della classe Layer di Keras"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"SO5rYsFpfFS_","executionInfo":{"status":"ok","timestamp":1687081949457,"user_tz":-120,"elapsed":286,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, embed_dim, num_heads, ff_dim, rate=0.5, name='DECODER', num_layers_name=1):\n","    super(Decoder, self).__init__()\n","    self.att1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='Decoder_MultiHeadAttention_input_decoder_Block_'+str(num_layers_name))\n","    self.att2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, name='Decoder_MultiHeadAttention_output_encoder_Block_'+str(num_layers_name))\n","    self.ffn = tf.keras.Sequential(\n","      [\n","       tf.keras.layers.Dense(ff_dim, activation='relu', name='Decoder_FFN_Dense_1_Block_'+str(num_layers_name)),\n","       tf.keras.layers.Dense(embed_dim, name='Decoder_FFN_Dense_2_Block_'+str(num_layers_name)),\n","      ]\n","    )\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(name='Decoder_LayerNormalization_1_Block_'+str(num_layers_name))\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(name='Decoder_LayerNormalization_2_Block_'+str(num_layers_name))\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(name='Decoder_LayerNormalization_3_Block_'+str(num_layers_name))\n","    self.dropout1 = tf.keras.layers.Dropout(rate, name='Decoder_Dropout_1_Block_'+str(num_layers_name))\n","    self.dropout2 = tf.keras.layers.Dropout(rate, name='Decoder_Dropout_2_Block_'+str(num_layers_name))\n","    self.dropout3 = tf.keras.layers.Dropout(rate, name='Decoder_Dropout_3_Block_'+str(num_layers_name))\n","\n","    self._name = name\n","\n","  def call(self, inputs, bert_outputs, training=False, debug=False):\n","    attn_output1 = self.att1(query=inputs,\n","                             value=inputs,\n","                             key=inputs,\n","                             use_causal_mask=True)\n","\n","    attn_output1 = self.dropout1(attn_output1)\n","    out1 = self.layernorm1(inputs + attn_output1)\n","\n","    attn_output2 = self.att2(key=bert_outputs,\n","                             value=bert_outputs,\n","                             query=out1)\n","\n","    attn_output2 = self.dropout2(attn_output2, training=training)\n","    out2 = self.layernorm2(out1 + attn_output2)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","\n","    output = self.layernorm3(out2 + ffn_output)\n","\n","    if debug:\n","      print('******************* DEBUG DECODER ***********************')\n","      print(f'Input Shape                       : {inputs.shape}')\n","      print(f'Shape Outputs Decoder             : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"yysVdkHH8EPH","executionInfo":{"status":"ok","timestamp":1687081951912,"user_tz":-120,"elapsed":1558,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"15e007ca-ca1b-4b28-a4e6-8017eeeaad42"},"outputs":[{"output_type":"stream","name":"stdout","text":["******************* DEBUG DECODER ***********************\n","Input Shape                       : (64, 128, 128)\n","Shape Outputs Decoder             : (64, 128, 128)\n","*********************************************************\n"]}],"source":["decoder = Decoder(EMBEDDING_DIM,\n","                  NUM_HEADS,\n","                  FF_DIM,\n","                  DROPUOT)\n","\n","outputs_decoder = decoder(inputs=inputs_decoder,\n","                          bert_outputs=bert_outputs,\n","                          training=training,\n","                          debug=debug)"]},{"cell_type":"markdown","metadata":{"id":"ne4zTOG_NKfV"},"source":["## TRANSFORMER\n","\n","Implementazione del blocco Transformer tramite estensione della classe Layer di Keras."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"lw2xMCAMC_4M","executionInfo":{"status":"ok","timestamp":1687081958205,"user_tz":-120,"elapsed":271,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["class TransformerBlock(tf.keras.Model):\n","  def __init__(self,\n","               num_layers,\n","               embed_dim,\n","               num_heads,\n","               ff_dim,\n","               max_len,\n","               vocab_size,\n","               tfhub_handle_encoder,\n","               trainable,\n","               rate=0.5):\n","\n","    super(TransformerBlock, self).__init__()\n","\n","    self.num_layers = num_layers\n","\n","    self.token_pos_dec = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n","\n","    self.encoder = EncoderBert(tfhub_handle_encoder, trainable=trainable)\n","    self.decoder = [Decoder(embed_dim, num_heads, ff_dim, rate, num_layers_name=i+1) for i in range(num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(rate, name='Transformer_Dropout_Pre_Final_Layer')\n","    self.final_layer = tf.keras.layers.Dense(vocab_size, name='Transformer_Final_Layer')\n","\n","  def call(self, inputs, training=False, debug=False):\n","    inputs_encoder, inputs_decoder  = inputs\n","\n","    encoder_output = self.encoder(inputs_encoder, debug=debug)\n","\n","    inputs_decoder = self.token_pos_dec(inputs_decoder, debug)\n","\n","    if debug:\n","      print(f'---------------- DEBUG TRANSFORMER BLOCK ----------------')\n","      print(f'inputs_encoder       : {inputs_encoder[\"input_word_ids\"].shape}')\n","      print(f'inputs_decoder       : {inputs_decoder.shape}')\n","\n","    transformer_output = inputs_decoder\n","\n","    for i in range(self.num_layers):\n","      transformer_output = self.decoder[i](inputs=transformer_output,\n","                                           bert_outputs=encoder_output,\n","                                           training=training,\n","                                           debug=debug)\n","\n","    transformer_output = self.dropout(transformer_output)\n","    transformer_output = self.final_layer(transformer_output)\n","\n","    if debug:\n","      print(f'Output Shape       : {transformer_output.shape}')\n","      print(f'Output Transformer : {transformer_output[0, :1, :12]}')\n","      print(f'---------------------------------------------------------')\n","\n","    return transformer_output"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"pr--G0ZZVAMi","executionInfo":{"status":"ok","timestamp":1687081965686,"user_tz":-120,"elapsed":6616,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e51267a4-0111-4d12-bc89-8bb5f9990e95"},"outputs":[{"output_type":"stream","name":"stdout","text":["****************** DEBUG ENCODER BERT ******************\n","First example\n","Keys                         : ['input_word_ids', 'input_mask']\n","Shape                        : (64, 128)\n","Word Ids                     : [  101 18082 10637 10737 10196 60259 10246   177 49433 66558 59719 10116\n","   119   102     0     0]\n","Input Mask                   : [1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0]\n","\n","Encoder Outputs BERT Shape   : (64, 128, 768)\n","Encoder Outputs BERT Values  : [[-0.03197939  0.09959724 -0.11866057  0.41455346 -0.04438442  0.02337466\n","   0.18171097 -0.16623658  0.06744681  0.21266855  0.07381846  0.04000761\n","  -0.01157836 -0.00687177 -0.45601267  0.07824986]]\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 128\n","Sequence Shape                            : [ 64 128]\n","Shape TokenAndPositionEmbedding           : (64, 128, 128)\n","*********************************************************\n","---------------- DEBUG TRANSFORMER BLOCK ----------------\n","inputs_encoder       : (64, 128)\n","inputs_decoder       : (64, 128, 128)\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (64, 128, 128)\n","Shape Outputs Decoder             : (64, 128, 128)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (64, 128, 128)\n","Shape Outputs Decoder             : (64, 128, 128)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (64, 128, 128)\n","Shape Outputs Decoder             : (64, 128, 128)\n","*********************************************************\n","Output Shape       : (64, 128, 15269)\n","Output Transformer : [[ 0.10614013  0.10924977 -0.06065574  0.21924189  0.03954092 -0.06803578\n","   0.10060378 -0.11568882 -0.06945544 -0.00641664 -0.14116772 -0.15754548]]\n","---------------------------------------------------------\n"]}],"source":["transformer = TransformerBlock(NUM_LAYERS,\n","                               EMBEDDING_DIM,\n","                               NUM_HEADS,\n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.dante.get_vocab_size(),\n","                               tfhub_handle_encoder,\n","                               trainable,\n","                               DROPUOT)\n","\n","transformer_output = transformer((enc_input, dec_input),\n","                                 training=training,\n","                                 debug=debug)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"0kYt6ehvh-8B","executionInfo":{"status":"ok","timestamp":1687081966059,"user_tz":-120,"elapsed":401,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b43ae67-89ea-417c-ce83-546f0171e592"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer_block\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," token_and_position_embeddin  multiple                 1970816   \n"," g_1 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," encoder_bert_1 (EncoderBert  multiple                 134734080 \n"," )                                                               \n","                                                                 \n"," DECODER (Decoder)           multiple                  1315456   \n","                                                                 \n"," DECODER (Decoder)           multiple                  1315456   \n","                                                                 \n"," DECODER (Decoder)           multiple                  1315456   \n","                                                                 \n"," Transformer_Dropout_Pre_Fin  multiple                 0         \n"," al_Layer (Dropout)                                              \n","                                                                 \n"," Transformer_Final_Layer (De  multiple                 1969701   \n"," nse)                                                            \n","                                                                 \n","=================================================================\n","Total params: 142,620,965\n","Trainable params: 7,886,885\n","Non-trainable params: 134,734,080\n","_________________________________________________________________\n"]}],"source":["transformer.summary()"]},{"cell_type":"markdown","metadata":{"id":"IFmcHTSDTvYk"},"source":["## Addestramento Modello"]},{"cell_type":"markdown","metadata":{"id":"-z6qj1uclHRa"},"source":["### Callbacks"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"3hurmpSjJ_dT","executionInfo":{"status":"ok","timestamp":1687081976173,"user_tz":-120,"elapsed":282,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 save_best_only=True)\n","\n","# Create a callback Tensorboard\n","log_dir =  os.path.abspath(os.path.join(PATH_LOG, 'dantenizzatore'))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n","\n","# Create a callback save the log history\n","json_logging_callback = tf.keras.callbacks.LambdaCallback(\n","  on_epoch_end=lambda epoch, logs: json_log.write(\n","    json.dumps({'epoch': epoch,\n","                'loss': logs['loss'],\n","                'masked_accuracy': logs['masked_accuracy'],\n","                'val_loss': logs['val_loss'],\n","                'val_masked_accuracy': logs['val_masked_accuracy']}) + '\\n'),\n","  on_train_end=lambda logs: json_log.close()\n",")"]},{"cell_type":"markdown","metadata":{"id":"tiuqPlHo0Z0n"},"source":["### Compilazione"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"4shRQA7ymnJB","executionInfo":{"status":"ok","timestamp":1687081978073,"user_tz":-120,"elapsed":283,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["def masked_loss(label, pred):\n","  mask = label != 0\n","  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","  loss = loss_object(label, pred)\n","\n","  mask = tf.cast(mask, dtype=loss.dtype)\n","  loss *= mask\n","\n","  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n","  return loss"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"lSSSIXd0fPD-","executionInfo":{"status":"ok","timestamp":1687081978074,"user_tz":-120,"elapsed":4,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["def masked_accuracy(label, pred):\n","  pred = tf.argmax(pred, axis=2)\n","  label = tf.cast(label, pred.dtype)\n","  match = label == pred\n","\n","  mask = label != 0\n","\n","  match = match & mask\n","\n","  match = tf.cast(match, dtype=tf.float32)\n","  mask = tf.cast(mask, dtype=tf.float32)\n","  return tf.reduce_sum(match)/tf.reduce_sum(mask)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"I_fbsEx6LkAg","executionInfo":{"status":"ok","timestamp":1687081978074,"user_tz":-120,"elapsed":3,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}}},"outputs":[],"source":["transformer.compile(\n","  loss=masked_loss,\n","  optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n","                                     beta_1=BETA_1,\n","                                     beta_2=BETA_2,\n","                                     epsilon=EPSILON),\n","  metrics=[masked_accuracy])"]},{"cell_type":"markdown","metadata":{"id":"GhBGzbvrh2Rw"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZ0iTUOXh70W"},"outputs":[],"source":["start = datetime.datetime.now()\n","\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_dante,\n","                initial_epoch=0,\n","                epochs=EPOCHS_DANTE,\n","                shuffle=True,\n","                validation_data=validation_dataset_dante,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback,\n","                           cp_callback\n","                           ])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ejTGcN8h4we"},"outputs":[],"source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)\n","\n","transformer.compile(\n","  loss=masked_loss,\n","  optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n","                                     beta_1=BETA_1,\n","                                     beta_2=BETA_2,\n","                                     epsilon=EPSILON),\n","  metrics=[masked_accuracy])"]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset_dante,\n","                initial_epoch=EPOCHS_DANTE,\n","                epochs=EPOCHS_DANTE+EPOCHS_DANTE,\n","                shuffle=True,\n","                validation_data=validation_dataset_dante,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback,\n","                           cp_callback\n","                           ])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"NGY1Ynw5DyLA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L0w4wF79UhAp"},"source":["## Valutazione dell'addestramento\n","Avendo in output il log ed i risultati dell'addestramento, possiamo visualizzare\n","queste informazioni relativamente alle metriche di interesse."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RpXR2p5VAdoG"},"outputs":[],"source":["df_history = pd.read_json(log_history, lines=True)\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n","\n","ax1.plot(df_history['loss'], label='Loss')\n","ax1.plot(df_history['val_loss'], label='Validation Loss')\n","ax1.set_title('Training Loss')\n","ax1.legend()\n","\n","ax2.plot(df_history['masked_accuracy'], label='Accuracy')\n","ax2.plot(df_history['val_masked_accuracy'], label='Validation Accuracy')\n","ax2.set_title('Training Accuracy')\n","ax2.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"O16Hx71zjnNA"},"source":["## Tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qmo7Pi3TaoFJ"},"outputs":[],"source":["!tensorboard dev upload --logdir drive/MyDrive/BERT/logs/fit/dantenizzatore/ \\\n","  --name \"Simple experiment with DANTE\" \\\n","  --description \"Training results\" \\\n","  --one_shot"]},{"cell_type":"markdown","metadata":{"id":"ReOkcBp2WHWW"},"source":["## Test del modello\n","La seguente cella permette di caricare l'ultimo checkpoint dell'addestramento\n","precedentemente salvato."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RN0mnV8Wd92H"},"outputs":[],"source":["trainable = False\n","\n","transformer = TransformerBlock(NUM_LAYERS,\n","                               EMBEDDING_DIM,\n","                               NUM_HEADS,\n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.ita.get_vocab_size(),\n","                               tfhub_handle_encoder,\n","                               trainable,\n","                               DROPUOT)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13980,"status":"ok","timestamp":1687033657896,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"5PIf_6-RSBb1","outputId":"212f6bb5-d0ff-4383-d2b3-48c7d0db8eb2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f78a2be88b0>"]},"metadata":{},"execution_count":43}],"source":["latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L2PEoJVb1V8x"},"outputs":[],"source":["class Translate:\n","  def __init__(self, transformer_block, tokenizers):\n","    self.transformer = transformer_block\n","    self.tokenizers = tokenizers\n","\n","  def predict(self, input_text, max_length):\n","    if input_text is None:\n","      input_text = (df[ORIGINAL_COLUMN].tolist())[np.random.choice(len(df[ORIGINAL_COLUMN].tolist()))]\n","      print(input_text)\n","\n","    inputs_bert = self.tokenizers.multilingual.tokenize(input_text)\n","\n","    start_end = self.tokenizers.ita.tokenize([''])[0]\n","    start = (start_end[0][tf.newaxis]).numpy()[0]\n","    end = (start_end[1][tf.newaxis]).numpy()[0]\n","\n","    output_array = tf.TensorArray(dtype=tf.int32, size=max_length, dynamic_size=True)\n","    output_array = output_array.write(0, tf.constant([start]))\n","\n","    out_words = []\n","\n","    for i in tf.range(max_length):\n","      # decodifica e recupero probabilità di output\n","      output = tf.transpose(output_array.stack())\n","\n","      transformer_output = transformer((inputs_bert, output),\n","                                        training=False,\n","                                        debug=False)\n","\n","      predictions = transformer_output[:, -1:, :]\n","\n","      # selezione della parola più probabile\n","      predict = tf.argmax(predictions, -1)\n","      pred_values = (tf.keras.backend.argmax(transformer_output, axis=-1)).numpy()\n","\n","      # inserimento della parola nella sequenza di output\n","      output_array = output_array.write(i+1, [pred_values[0][i]])\n","\n","      if pred_values[0][i] == end:\n","        break\n","\n","    output = tf.transpose(output_array.stack())\n","    text = tokenizers.ita.detokenize(output)[0]\n","    tokens = tokenizers.ita.lookup(output)[0]\n","\n","    return text, tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68025,"status":"ok","timestamp":1687033729168,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"udIjI2jZWR6g","outputId":"f91b8310-a608-4d3b-efb8-4accffd8e88d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input          : Vedete qui i fermagli che le manda , \n","Target         : Vedete qui fermagli , ch' e' le manda , \n","Prediction     : vedete qui fermagli , ch ' e ' le manda ,\n","---------------------------------------------\n","Input          : Don' t put your love in a poor man\n","Target         : In poveruon no metter già tu' amore\n","Prediction     : in poveruon no metter gia tu ' amore\n","---------------------------------------------\n","Input          : Mon arc , si je ne pouvais pas\n","Target         : I' 'l mi' brandone , sed i' te non potrei\n","Prediction     : i ' ' l mi ' brandone , sed i ' te non potrei\n","---------------------------------------------\n","Input          : der gute Grund sagte Du bist weit weg , \n","Target         : Ragion la bella e disse Tu ssè corso , \n","Prediction     : la buona anguilla nonn e gia pegiore ,\n","---------------------------------------------\n","Input          : Du weißt nicht , was schlecht es ist , bis du es probierst . \n","Target         : Non sa che mal si sia chi non asaggia\n","Prediction     : tu non sa che ' l ben a ccio ,\n","---------------------------------------------\n","Input          : Er lobte seine Manieren und sein Aussehen , \n","Target         : Lodando sua maniera , e sua fazone , \n","Prediction     : suo ' gioie e noie per lui fur ricievute ,\n","---------------------------------------------\n","Input          : Dann fuhr Francheza zu Pferd\n","Target         : Allor Francheza sì à cavalcato , \n","Prediction     : allor francheza si a cavalcato ,\n","---------------------------------------------\n","Input          : and so for many times ansimo\n","Target         : Sì che per molte volte ne sospiro , \n","Prediction     : e per molte volte merze , veco ch ' ella diverso\n","---------------------------------------------\n","Input          : Con quel tipo , avrai successo nella ricerca . \n","Target         : Con quel cotal fa buon intrar in caccia , \n","Prediction     : con quel cotal fa buon intrar in caccia ,\n","---------------------------------------------\n","Input          :  I heard you preach abstinence \n","Target         :  Predicar astinenza i' t' ò udito . \n","Prediction     : predicar astinenza i ' t ' o udito .\n","---------------------------------------------\n","Input          : Riportarle sulla retta via . \n","Target         : Per rimenargli a lor dritti camini . \n","Prediction     : per rimenargli a lor dritti camini .\n","---------------------------------------------\n","Input          : qu' elle ne l' accorderait qu' à parler\n","Target         : Ched ella gli degniasse pur parlare . \n","Prediction     : ch ' ella gli degniasse pur parlare .\n","---------------------------------------------\n","Input          : and pick them up from the pickpockets and the room . \n","Target         : E prendergli a' gheroni e a la sala;\n","Prediction     : e prendergli a ' gheroni e a la sala ;\n","---------------------------------------------\n","Input          : dass ich den Menschen Güte beweise\n","Target         : Ch' i' a le gienti mostri benvolere , \n","Prediction     : ch ' i ' a le gienti mostri benvolere ,\n","---------------------------------------------\n","Input          : Nè io , nè chi fa Dio d' Amore ! \n","Target         : Nè vo' , nè que' , che d' Amor si fa Dio ! . \n","Prediction     : ne vo ' , ne que ' , che d ' amor si fa dio ! .\n","---------------------------------------------\n","Input          : Ogni mio atto voglio rendere perfetto , \n","Target         : Ognie mi' fatto sì vo' far a sesta , \n","Prediction     : ognie mi ' fatto si vo ' far a sesta ,\n","---------------------------------------------\n","Input          : Appariscente e costoso lo conserverai . \n","Target         : A l' avenante caro il ti terrai , \n","Prediction     : a l ' avenante caro il ti terrai ,\n","---------------------------------------------\n","Input          : rifiutando di amare una donna\n","Target         : Di scondir del tu' amor tal damigiella\n","Prediction     : di scondir del tu ' amor tal damigiella\n","---------------------------------------------\n","Input          : A voler raccontare le sue sembianze\n","Target         : A voler racontar de' suo' senbianti , \n","Prediction     : a le tenpie a paura si s ' aferra .\n","---------------------------------------------\n","Input          : e astuzia sia i truffatori che le vittime . \n","Target         : E 'nganno ingannatori e ingannati . \n","Prediction     : e ' nganno ingannatori e ingannati .\n","---------------------------------------------\n","Input          : Et ils ont dit qu' ils avaient trouvè un accord . \n","Target         : E disser ch' e' trovavar d' acordanza\n","Prediction     : e disser ch ' e ' trovavar d ' acordanza\n","---------------------------------------------\n","Input          : Um der Liebe Jesu Christi willen , rette mich\n","Target         : E le dirai Per Giesocristo , tra' mi\n","Prediction     : per dio , gientil madonna , vi m ' avea\n","---------------------------------------------\n","Input          : And he threatened her with great force;\n","Target         : E molto forte mente la minaccia;\n","Prediction     : e molto forte mente la minaccia ;\n","---------------------------------------------\n","Input          : Noch immer sind liebenswerte Menschen vertrieben worden . \n","Target         : À 'ncor di gientil giente discacciata , \n","Prediction     : a ' ncor di gientil giente discacciata ,\n","---------------------------------------------\n","Input          : Of course I am , if God gives you life , \n","Target         : Ciertana son , se Dio ti dona vita , \n","Prediction     : ciertana son , se dio ti dona vita ,\n","---------------------------------------------\n","Input          : Because , everything he' s exposed to . \n","Target         : Chè , tutto ciò ch' egli avrà detto a l' una , \n","Prediction     : che , tutto cio ch ' egli avra detto a l ' una ,\n","---------------------------------------------\n","Input          : Je ne veux pas qu' on me mèprise en buvant de l' alcool . \n","Target         : Non vo' che 'l ber per ciò nesun disami , \n","Prediction     : non vo ' che ' l ber per cio nesun disami ,\n","---------------------------------------------\n","Input          : Fatherly prayer and a metal chain . \n","Target         : Di paternostri , e 'l laccio di fil iera;\n","Prediction     : e predicar dolze predicazione\n","---------------------------------------------\n","Input          : Sanft mit gebeugtem Kopf , \n","Target         : Umile mente l' ebi a capo chino , \n","Prediction     : umile mente l ' ebi a capo chino ,\n","---------------------------------------------\n","Input          : les quelques choses que je veux encore dire\n","Target         : Alquanti motti , ch' i' voglio ancor dire . \n","Prediction     : de ch ' io vego ch ' i ' ne vo ' assai ,\n","---------------------------------------------\n","Input          : Or that you get more satisfaction . \n","Target         : Nè che tti doni più di dilettanza . \n","Prediction     : o che ttu agie sugo di cipolle ,\n","---------------------------------------------\n","Input          : Ohne jemals unsere Gedanken auszudrücken . \n","Target         : Sanza dir cosa mai , che noi pensiamo . \n","Prediction     : sanza dir cosa mai , che noi pensiamo .\n","---------------------------------------------\n","Input          : parce qu' il ètait heureux de dire des calomnies . \n","Target         : Chè sì gli piaque dir ribalderia . \n","Prediction     : che si gli piaque dir ribalderia .\n","---------------------------------------------\n","Input          : Ich werde ihren Reichtum nicht minimieren . \n","Target         : Non recherò a poco il loro assai . \n","Prediction     : non rechero a poco il loro assai .\n","---------------------------------------------\n","Input          : wie es schmerzt , meinen Rang zu tun . \n","Target         : Ch' e' molto si penar di far mi' grado . \n","Prediction     : com ' e ' parea molto pensava ,\n","---------------------------------------------\n","Input          : and thought she was more beautiful than a fairy\n","Target         : E credes' esser più bella che fata , \n","Prediction     : e credes ' esser piu bella che fata ,\n","---------------------------------------------\n","Input          : Fausse prophètie et abstinence obligatoire\n","Target         : Che Falsenbiante e Costretta Astinenza\n","Prediction     : e falsenbiante e costretta astinenza\n","---------------------------------------------\n","Input          : Soyez amer et rigide au dèbut . \n","Target         : Ch' a cominciar si mostra acierbo e duro . \n","Prediction     : ch ' a cominciar si mostra acierbo e duro .\n","---------------------------------------------\n","Input          : For many occasions I have lied , \n","Target         : Per molte volte mi son pergiurato , \n","Prediction     : per molte volte fui a voi ,\n","---------------------------------------------\n","Input          : Und dann beleidigt er sie auf grässliche Weise \n","Target         : E poi villanamente la ranpogna , \n","Prediction     : e poi villanamente la ranpogna ,\n","---------------------------------------------\n","Input          : perchè non sa cosa le attirerà critiche\n","Target         : Chè non dotta che que' faccia blasmarla . \n","Prediction     : che non dotta che que ' faccia blasmarla .\n","---------------------------------------------\n","Input          : in una battaglia , in cui c' ero anche io . \n","Target         : In una battaglia , nella qual fu' io . \n","Prediction     : in una battaglia , nella qual fu ' io .\n","---------------------------------------------\n","Input          : Figlia mia , se Dio ti benedica , \n","Target         : Figliuola mia , se Dio ti benedica , \n","Prediction     : figliuola mia , se dio ti benedica ,\n","---------------------------------------------\n","Input          : Um die zu quälen , die sie so mochte . \n","Target         : Per crucciar que' , che tanto le piacea . \n","Prediction     : per crucciar que ' , che tanto le piacea .\n","---------------------------------------------\n","Input          : Diejenigen , die daran glauben , werden immer noch Unrecht haben . \n","Target         : Chi di ciò 'l crede , falleria ancore . \n","Prediction     : chi que ' , c ' a ' anima grieva .\n","---------------------------------------------\n","Input          : que je connais toute la leçon à l' esprit\n","Target         : Chèd i' so la lezion tratutta a mente , \n","Prediction     : ched i ' so la lezion tratutta a mente ,\n","---------------------------------------------\n","Input          : E ti do , come buon inizio , \n","Target         : E sì tti do , per buon cominciamento , \n","Prediction     : e si tti do , per ch ' i ' soccorra durante ,\n","---------------------------------------------\n","Input          : Das würde dir viel geben . \n","Target         : Che ricca mente t' avrebe donata\n","Prediction     : che ricca mente t ' avrebe donata\n","---------------------------------------------\n","Input          : Je me suis fait vite , je voulais te dire\n","Target         : Digiunar me ne fecie , a ver vo dire , \n","Prediction     : digiunar me ne fecie , a ver vo dire ,\n","---------------------------------------------\n","Input          : Now let' s go quickly and find that villain . \n","Target         : Or andiàn tosto , e troviàn quel villano , \n","Prediction     : or andian tosto , e trovian quel villano ,\n","---------------------------------------------\n"]}],"source":["translate = Translate(transformer_block=transformer,\n","                      tokenizers=tokenizers)\n","\n","# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for test_input_data, test_target_data in test_dataset.take(50):\n","  test_input_data = test_input_data.numpy().decode()\n","  test_target_data = test_target_data.numpy().decode()\n","\n","  text, token = translate.predict(tf.constant([test_input_data]), MAX_SEQ_LENGTH)\n","\n","  print(f'{\"Input\":15s}: {test_input_data}')\n","  print(f'{\"Target\":15s}: {test_target_data}')\n","  print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')\n","  print('---------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1207,"status":"ok","timestamp":1687033730338,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"s9Dun8NnGaXd","outputId":"7a192f71-7e77-48b9-f762-a21952311748"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input          : è bello divertirsi con gli amici\n","Prediction     : i ' si nonn e volontier di far a signiore ,\n","---------------------------------------------\n"]}],"source":["text_input_data = 'è bello divertirsi con gli amici'\n","\n","text, token = translate.predict(tf.constant([text_input_data]), MAX_SEQ_LENGTH)\n","print(f'{\"Input\":15s}: {text_input_data}')\n","print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')\n","print('---------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1102,"status":"ok","timestamp":1687033738304,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"},"user_tz":-120},"id":"hfjiujXc7Mzw","outputId":"25883ada-7959-4cc4-f3fd-3cc59e16d7de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input          : cocacola al bar\n","Prediction     : udendo quella nobile novella ,\n","---------------------------------------------\n"]}],"source":["text_input_data = 'cocacola al bar'\n","\n","text, token = translate.predict(tf.constant([text_input_data]), MAX_SEQ_LENGTH)\n","print(f'{\"Input\":15s}: {text_input_data}')\n","print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')\n","print('---------------------------------------------')"]},{"cell_type":"code","source":["text_input_data = 'sono andato al supermercato a comprare una birra'\n","\n","text, token = translate.predict(tf.constant([text_input_data]), MAX_SEQ_LENGTH)\n","print(f'{\"Input\":15s}: {text_input_data}')\n","print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')\n","print('---------------------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ilN2dMGclJhv","executionInfo":{"status":"ok","timestamp":1687033745718,"user_tz":-120,"elapsed":1390,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"outputId":"ee435f8b-8bbc-452e-a306-61b9f614122f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input          : sono andato al supermercato a comprare una birra\n","Prediction     : e ora si avea studiato l ' arte ,\n","---------------------------------------------\n"]}]},{"cell_type":"code","source":["text_input_data = 'quanta è figa la masterclass di Datamasters'\n","\n","text, token = translate.predict(tf.constant([text_input_data]), MAX_SEQ_LENGTH)\n","print(f'{\"Input\":15s}: {text_input_data}')\n","print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')\n","print('---------------------------------------------')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jg7zmT0jlVUf","executionInfo":{"status":"ok","timestamp":1687033758230,"user_tz":-120,"elapsed":3201,"user":{"displayName":"daniele badiali","userId":"08100546172874775759"}},"outputId":"ea632a98-cfa5-4411-a4da-248c589484e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Input          : quanta è figa la masterclass di Datamasters\n","Prediction     : che lla roba del fior le sorte\n","---------------------------------------------\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Azz9t8bTDDGO"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["0KUcCnjXVjt3","mICEGEzJVnvx","pKZxiQ5_Whmw","5QIDajkEsVU1","8dtVuZGJpvXl","-z6qj1uclHRa","tiuqPlHo0Z0n","L0w4wF79UhAp","O16Hx71zjnNA","ReOkcBp2WHWW"],"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}