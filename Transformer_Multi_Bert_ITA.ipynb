{"cells":[{"cell_type":"markdown","source":["## Pacchetti da installare"],"metadata":{"id":"yDKuSNBd92YI"}},{"cell_type":"code","source":["!pip install -q -U 'tensorflow-text==2.8.*'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NE4enZGpvMRX","executionInfo":{"status":"ok","timestamp":1678652751830,"user_tz":-60,"elapsed":66046,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"aca046e0-081b-4244-96f9-57798a8e5539"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 KB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q tf-models-official"],"metadata":{"id":"FPtWz_qHuofc","executionInfo":{"status":"ok","timestamp":1678652813810,"user_tz":-60,"elapsed":61985,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2dbe4328-13f8-4a61-d270-1a2e58d5402f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m630.1/630.1 KB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.9/238.9 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.2/439.2 KB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hJy-juNOpUOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678652833285,"user_tz":-60,"elapsed":19494,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"17277361-74c4-4296-9c2c-a94385b51fb5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Import notebook"],"metadata":{"id":"xXYm-Qqw-ANh"}},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"UaAiWsEuC_4K","executionInfo":{"status":"ok","timestamp":1678652840606,"user_tz":-60,"elapsed":7330,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"outputs":[],"source":["import os\n","import re\n","import datetime\n","import pathlib\n","import json\n","from pathlib import Path\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["tf.get_logger().setLevel('ERROR')\n","tf.config.run_functions_eagerly(True)"],"metadata":{"id":"uKEqRlKowOQS","executionInfo":{"status":"ok","timestamp":1678652840607,"user_tz":-60,"elapsed":24,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Variabili Globali"],"metadata":{"id":"HRe16D-rUBLA"}},{"cell_type":"code","source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'train_data.csv'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_data_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# PATH LOG Tensorboard\n","PATH_LOG = 'logs/fit/transformer_multi_bert_it'\n","PATH_LOG = os.path.abspath(os.path.join(root_folder, PATH_LOG))\n","log_dir =  os.path.abspath(os.path.join(PATH_LOG, datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))) \n","log_history = os.path.abspath(os.path.join(PATH_LOG, 'histrory.json'))\n","\n","# PATH WEIGHTS Tensorboard\n","PATH_WEIGHTS = 'weights/transformer_multi_bert_it'\n","PATH_WEIGHTS = os.path.abspath(os.path.join(root_folder, PATH_WEIGHTS))\n","checkpoint_path = os.path.abspath(os.path.join(PATH_WEIGHTS, 'cp.ckpt'))\n","\n","# VOCABOLARIO\n","vocab_folder = 'vocab'\n","multilingual_vocab_finalname = 'multilingual_vocab.txt'\n","ita_vocab_finalname = 'ita_vocab.txt'\n","\n","VOCAB_PATH = os.path.abspath(os.path.join(root_folder, vocab_folder))\n","multilingual_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, multilingual_vocab_finalname))\n","ita_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, ita_vocab_finalname))\n","\n","# parametri per il modello\n","ORIGINAL_COLUMN = 'Original'\n","TRANSLATE_COLUMN = 'Translate'"],"metadata":{"id":"ewLgCIuEpczO","executionInfo":{"status":"ok","timestamp":1678652840608,"user_tz":-60,"elapsed":24,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Iper Parametri Modello"],"metadata":{"id":"LCiP6wT05k6j"}},{"cell_type":"code","source":["TEST = 200\n","TEST_SIZE = 0.3\n","\n","MAX_VOCAB_SIZE = 30000 \n","EMBEDDING_DIM = 128\n","HIDDEN_DIM = 1024 # numero di celle nei layer ricorrenti nascosti\n","\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 2000\n","MAX_SEQ_LENGTH = 128\n","\n","NUM_LAYERS = 1 # Numero di layer di Decoder del Transformer\n","NUM_HEADS = 8 # Numero di meccanismi di multi-head attention\n","FF_DIM = 16 # Numero di celle dei Layer Feed Forward\n","DROPUOT = 0.5\n","\n","# Ottimizzatore Adam\n","LEARNING_RATE_ADAM = 1e-4\n","BETA_1 = 0.66\n","BETA_2 = 0.999\n","EPOCHS_ADAM = 5\n","\n","# IMPOSTO IL DEBUG A TRUE \n","debug = True\n","training = True"],"metadata":{"id":"8CN-4Uzoqbjl","executionInfo":{"status":"ok","timestamp":1678652840609,"user_tz":-60,"elapsed":24,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Parametri BERT"],"metadata":{"id":"BehZY4rETECN"}},{"cell_type":"code","source":["bert_model_name = 'distilbert_multi_cased_L-6_H-768_A-12/1'  \n","tfhub_handle_preprocess = 'https://tfhub.dev/jeongukjae/distilbert_multi_cased_preprocess/2'\n","tfhub_handle_encoder =  'https://tfhub.dev/jeongukjae/distilbert_multi_cased_L-6_H-768_A-12/1'\n","\n","if debug:\n","  print('BERT model name                    : ', bert_model_name)\n","  print('BERT model selected                : ', tfhub_handle_encoder)\n","  print('BERT preprocess                    : ', tfhub_handle_preprocess)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fodDcY6sm392","executionInfo":{"status":"ok","timestamp":1678652840610,"user_tz":-60,"elapsed":25,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"4f863234-4a26-4fec-a496-ee38d0087845"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT model name                    :  distilbert_multi_cased_L-6_H-768_A-12/1\n","BERT model selected                :  https://tfhub.dev/jeongukjae/distilbert_multi_cased_L-6_H-768_A-12/1\n","BERT preprocess                    :  https://tfhub.dev/jeongukjae/distilbert_multi_cased_preprocess/2\n"]}]},{"cell_type":"code","source":["'''\n","bert_model_name = 'bert_multi_cased_L-12_H-768_A-12/4'  \n","tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3'\n","tfhub_handle_encoder =  'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4'\n","\n","if debug:\n","  print('BERT model name                    : ', bert_model_name)\n","  print('BERT model selected                : ', tfhub_handle_encoder)\n","  print('BERT preprocess                    : ', tfhub_handle_preprocess)\n","\n","'''"],"metadata":{"id":"6tGAR51Nj0Eg","executionInfo":{"status":"ok","timestamp":1678652840610,"user_tz":-60,"elapsed":13,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"2bfc0cec-3629-4f9f-c826-b8572d2453b6"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"\\nbert_model_name = 'bert_multi_cased_L-12_H-768_A-12/4'  \\ntfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3'\\ntfhub_handle_encoder =  'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4'\\n\\nif debug:\\n  print('BERT model name                    : ', bert_model_name)\\n  print('BERT model selected                : ', tfhub_handle_encoder)\\n  print('BERT preprocess                    : ', tfhub_handle_preprocess)\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["## DATASET"],"metadata":{"id":"5DPeN9Vanbvv"}},{"cell_type":"markdown","source":["### Caricamento Dati"],"metadata":{"id":"LU7AorKXT8K7"}},{"cell_type":"code","source":["def preprocess_sentence(w):\n","  '''\n","  Preprocessing dei testi di input, impostando tutti i caratteri\n","  minuscoli, aggiungendo uno spazio prima di ogni punto e sostituendo\n","  qualsiasi carattere con uno spazio se non è compreso nel seguente elenco:\n","  (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\", \"’\")\n","  '''\n","  # inserimento di uno spazio tra ogni parola e il successivo punto,\n","  # punto esclamativo, punto interrogativo e virgola\n","  # esempio: \"ciao, come và?\" => \"ciao , come và ?\"\n","  w = re.sub(r\"([?.!,])\", r\" \\1 \", w) # inserimento di uno spazio\n","\n","  # sostituzione dei caratteri apostrofo\n","  w = re.sub(r\"([’]+)\", \"'\", w)\n","\n","  w = w.replace(\"á\", \"à\")\n","  w = w.replace(\"é\", \"è\")\n","  w = w.replace(\"í\", \"ì\")\n","  w = w.replace(\"ó\", \"ò\")\n","  w = w.replace(\"ú\", \"ù\")\n","  w = w.replace('\"', \" \")\n","  w = w.replace(':', \" \")\n","  w = w.replace('«', \" \")\n","  w = w.replace('»', \" \")\n","  w = w.replace('‘', \" \")\n","  w = w.replace('-', \" \")\n","  w = w.replace('[', \" \")\n","  w = w.replace(']', \" \")\n","  w = w.replace('(', \" \")\n","  w = w.replace(')', \" \")\n","  w = w.replace(\"•\", \" \")\n","  w = w.replace(\"..\", \".\")\n","  w = w.replace(\"...\", \".\")\n","  w = w.replace(\"\\xa0\", \" \")\n","  w = w.replace(\"   \", \" \")\n","  w = w.replace(\"–\", \" \")\n","  w = w.replace(\"“\", \" \")\n","  w = w.replace(\"”\", \" \")\n","  w = w.replace(\"„\", \" \")\n","  w = w.replace(\"─\", \" \")\n","  w = w.replace(\"♪\", \" \")\n","  w = w.replace(\"#\", \" \")\n","  w = w.replace(\"/\", \" \")\n","  w = w.replace(\"=\", \" \")\n","  w = w.replace(\">\", \" \")\n","  w = w.replace(\"\\\\\", \" \")\n","  w = w.replace(\"`\", \" \")\n","  w = w.replace(\"¡\", \" \")\n","  w = w.replace(\"¿\", \" \")\n","  w = w.replace(\"œ\", \" \")\n","\n","  # inserimento di uno spazio dopo apostrofo\n","  w = re.sub(r\"(['])\", r\"\\1 \", w) \n","\n","  w = w.replace(\" ' \", \" '\")\n","\n","  w = re.sub(r'[\" \"]+', \" \", w) # rimozione di più spazi consecutivi\n","  return w"],"metadata":{"id":"Jm_Up6gyOTgW","executionInfo":{"status":"ok","timestamp":1678652840611,"user_tz":-60,"elapsed":12,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\n","  train_data_filenamepath,\n","  usecols=[ORIGINAL_COLUMN, TRANSLATE_COLUMN]\n",")\n","\n","# Preprocessing dei dati di Input\n","input_data = df[ORIGINAL_COLUMN].apply(lambda x : preprocess_sentence(x)).tolist()\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","target_data = df[TRANSLATE_COLUMN].apply(lambda x : preprocess_sentence(x)).tolist()"],"metadata":{"id":"rY-2na-MkMMI","executionInfo":{"status":"ok","timestamp":1678652848646,"user_tz":-60,"elapsed":8047,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["train_input_data, validation_input_data, train_target_data, validation_target_data = train_test_split(\n","  input_data[:-TEST], \n","  target_data[:-TEST], \n","  test_size=TEST_SIZE, \n","  random_state=42,\n","  shuffle=True\n",") \n","\n","train_input_data = train_input_data[:(int((len(train_input_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","train_target_data = train_target_data[:(int((len(train_target_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","\n","validation_input_data = validation_input_data[:(int((len(validation_input_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","validation_target_data = validation_target_data[:(int((len(validation_target_data) / BATCH_SIZE)) * BATCH_SIZE)]\n","\n","test_input_data = input_data[len(train_input_data)+len(validation_input_data):]\n","test_target_data = target_data[len(train_target_data)+len(validation_target_data):]\n","\n","if debug:\n","  print(f'Dati totali presenti nel Dataset               : {len(df)}')\n","  print(f'Dati totali presenti nel Dataset di Train      : {len(train_input_data)}')\n","  print(f'Dati totali presenti nel Dataset di Validation : {len(validation_input_data)}')\n","  print(f'Dati totali presenti nel Dataset di Test       : {len(test_input_data)}\\n')\n","\n","\n","  print('----------------------------------- TRAIN SET -----------------------------------------')\n","  print(train_input_data[-4:])\n","  print(train_target_data[-4:])\n","  print('--------------------------------- VALIDATION SET --------------------------------------')\n","  print(validation_input_data[-4:])\n","  print(validation_target_data[-4:])\n","  print('----------------------------------- TEST SET ------------------------------------------')\n","  print(test_input_data[-4:])\n","  print(test_target_data[-4:])\n","\n","  print('-------------------------------- ANALISI DATI -----------------------------------------')\n","  print(f'Esempi nel Dataset di Train                            : {len(train_input_data)}')\n","  print(f'Frase più corta in nel Dataset di Train                : {min(train_input_data, key = len)}')\n","  print(f'Frase più corta in Italiano nel Dataset di Train       : {min(train_target_data, key = len)}')\n","  print(f'Frase più lunga in nel Dataset di Train                : {max(train_input_data, key = len)}')\n","  print(f'Frase più lunga in Italiano nel Dataset di Train       : {max(train_target_data, key = len)}')\n","  print('---------------------------------------------------------------------------------------')\n","  print(f'Esempi nel Dataset di Validation                       : {len(validation_input_data)}')\n","  print(f'Frase più corta in nel Dataset di Validation           : {min(validation_input_data, key = len)}')\n","  print(f'Frase più corta in Italiano nel Dataset di Validation  : {min(validation_target_data, key = len)}')\n","  print(f'Frase più lunga in nel Dataset di Validation           : {max(validation_input_data, key = len)}')\n","  print(f'Frase più lunga in Italiano nel Dataset di Validation  : {max(validation_target_data, key = len)}')\n","  print('---------------------------------------------------------------------------------------')\n","  print(f'Esempi nel Dataset di Test                             : {len(test_input_data)}')\n","  print(f'Frase più corta in nel Dataset di Test                 : {min(test_input_data, key = len)}')\n","  print(f'Frase più corta in Italiano nel Dataset di Test        : {min(test_target_data, key = len)}')\n","  print(f'Frase più lunga in nel Dataset di Test                 : {max(test_input_data, key = len)}')\n","  print(f'Frase più lunga in Italiano nel Dataset di Test        : {max(test_target_data, key = len)}')  "],"metadata":{"id":"Ga3s8F5pi46Z","executionInfo":{"status":"ok","timestamp":1678652848984,"user_tz":-60,"elapsed":375,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9b33023b-9ccf-4828-9e71-0ab970657316"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Dati totali presenti nel Dataset               : 95119\n","Dati totali presenti nel Dataset di Train      : 66432\n","Dati totali presenti nel Dataset di Validation : 28416\n","Dati totali presenti nel Dataset di Test       : 271\n","\n","----------------------------------- TRAIN SET -----------------------------------------\n","['Vous pouvez ècrire dans la langue que vous voulez . Sur Tatoeba , toutes les langues sont ègales . ', 'Prometiò venir a las cuatro . ', 'Yo no era desafortunado . ', 'We hebben ruim de tijd om onze trein te halen . ']\n","['Puoi scrivere in qualsiasi lingua desideri . Su Tatoeba tutte le lingue sono uguali . ', 'Ha promesso di venire alle quattro . ', 'Non ero sfigato . ', 'Noi abbiamo un sacco di tempo per prendere il nostro treno . ']\n","--------------------------------- VALIDATION SET --------------------------------------\n","[\"Passe moi la clef de 12 s' il te plaît . \", 'La salmonela es muy frecuente en los huevos crudos , por eso es importante no comerlos asì . ', \"Nous ne pouvions ouvrir la porte , car elle ètait fermèe à clè de l' intèrieur . \", 'Elles veulent dècouvrir qui a cassè la fenêtre . ']\n","['Passami la chiave da 12 , per favore . ', 'La salmonella è molto frequente nelle uova crude , per questo è importante non mangiarle così . ', \"Non potemmo aprire la porta , perchè era chiusa a chiave dall' interno . \", 'Vogliono scoprire chi ha rotto la finestra . ']\n","----------------------------------- TEST SET ------------------------------------------\n","['Ich bin noch zu schwach . ', 'Du bist nicht ersetzbar . ', 'Wir haben einen Vertrag . ', 'Ich habe nichts bezahlt . ']\n","['Sono ancora troppo debole . ', 'Non sei sostituibile . ', 'Abbiamo un contratto . ', 'Io non ho pagato niente . ']\n","-------------------------------- ANALISI DATI -----------------------------------------\n","Esempi nel Dataset di Train                            : 66432\n","Frase più corta in nel Dataset di Train                : Hoi . \n","Frase più corta in Italiano nel Dataset di Train       : No ? \n","Frase più lunga in nel Dataset di Train                : There is no such thing , at this stage of the world' s history in the United States , as an independent press . You know it and I know it . There is not one of you who dare write your honest opinions , and if you did , you know beforehand that it would never appear in print . I am paid weekly for keeping my honest opinions out of the paper I am connected with . Others of you are paid similar salaries for similar things , and any of you who would be foolish as to write honest opinions would be out on the streets looking for another job . If I allowed my honest opinions to appear in one issue of my papers , before twenty four hours my occupation would be gone . The business of the journalist is to destroy the truth , to lie outright , to pervert , to vilify , to fawn at the feet of Mammon , and to sell his country and his race for his daily bread . You know it and I know it , and what folly is this toasting an independent press ? We are the jumping jacks , they pull the strings and we dance . Our talents , our possibilities and our lives are all the property of other men . We are intellectual prostitutes . \n","Frase più lunga in Italiano nel Dataset di Train       : Non vi è nulla di simile , in questa fase della storia del mondo in America , come una stampa indipendente . Tu lo sai e io lo so . Non c' è nessuno di voi che osa scrivere le proprie opinioni oneste , e se l' avete fatto , si sa in anticipo che non apparirebbe mai in stampa Io sono pagato settimanalmente per tenere le mie opinioni oneste fuori dal giornale a cui sono collegato . Altri di voi sono pagati con stipendi simili per cose simili , e chiunque di voi che sarebbe sciocco da scrivere opinioni oneste sarebbe fuori per le strade in cerca di un altro lavoro . Se ho lasciato le mie opinioni oneste di apparire in una questione di mie carte , prima di 24 ore la mia occupazione sarebbe andato . L' attività del giornalista è quella di distruggere la verità , di mentire apertamente , di pervertire , di diffamare , di strisciare ai piedi di Mammona , e di vendere il suo paese e la sua razza per il suo pane quotidiano . Tu lo sai e io lo so , e che follia è questo brindare per una stampa indipendente ? Noi siamo i burattini , loro tirano i fili e noi balliamo . I nostri talenti , le nostre possibilità e le nostre vite sono tutte proprietà di altri uomini . Noi siamo delle prostitute intellettuali . \n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Validation                       : 28416\n","Frase più corta in nel Dataset di Validation           : Sì . \n","Frase più corta in Italiano nel Dataset di Validation  : Sì . \n","Frase più lunga in nel Dataset di Validation           : There is no such thing , at this stage of the world' s history in The United States of America , as an independent press . You know it and I know it . There is not one of you who dare write your honest opinions , and if you did , you know beforehand that it would never appear in print . I am paid weekly for keeping my honest opinions out of the paper I am connected with . Others of you are paid similar salaries for similar things , and any of you who would be foolish as to write honest opinions would be out on the streets looking for another job . If I allowed my honest opinions to appear in one issue of my papers , before twenty four hours my occupation would be gone . The business of the journalist is to destroy the truth , to lie outright , to pervert , to vilify , to fawn at the feet of Mammon , and to sell his country and his race for his daily bread . You know it and I know it , and what folly is this toasting an independent press ? We are the jumping jacks , they pull the strings and we dance . Our talents , our possibilities and our lives are all the property of other men . We are intellectual prostitutes . \n","Frase più lunga in Italiano nel Dataset di Validation  : Lasciando adunque indietro le cose circa un Principe immaginate , e discorrendo quelle che son vere , dico , che tutti gli uomini , quando se ne parla , e massime i Principi , per esser posti più alto , sono notati di alcune di queste qualità che arrecano loro o biasimo , o laude; e questo è che alcuno è tenuto liberale , alcuno misero , usando uno termine Toscano , perchè avaro in nostra lingua è ancor colui che per rapina desidera d' avere , e misero chiamiamo quello che si astiene dall' usare il suo alcuno è tenuto donatore , alcuno rapace; alcuno crudele , alcuno pietoso; l' uno fedifrago , l' altro fedele; l' uno effeminato e pusillanime , l' altro feroce e animoso; l' uno umano , l' altro superbo; l' uno lascivo , l' altro casto; l' uno intero , l' altro astuto; l' uno duro , l' altro facile; l' uno grave , l' altro leggiere; l' uno religioso , l' altro incredulo , e simili . Io so che ciascuno confesserà , che sarebbe laudabilissima cosa un Principe trovarsi di tutte le sopraddette qualità , quelle che sono tenute buone; ma perchè non si possono avere , nè interamente osservare per le condizioni umane che non lo consentono , gli è necessario essere tanto prudente , che sappia fuggire l' infamia di quelli vizi che li torrebbono lo Stato , e da quelli che non gliene tolgano , guardarsi , se egli è possibile; ma non potendosi , si può con minor rispetto lasciare andare . Ed ancora . . . \n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Test                             : 271\n","Frase più corta in nel Dataset di Test                 : Nicht die Polizei rufen ! \n","Frase più corta in Italiano nel Dataset di Test        : Mi evita . \n","Frase più lunga in nel Dataset di Test                 : Ich weiß , dass Tom lügt . \n","Frase più lunga in Italiano nel Dataset di Test        : Tom Clancy decedette a sessantasei anni . \n"]}]},{"cell_type":"markdown","source":["## Tokenizer\n","\n","Creo due differenti tokenizer che mi servizranno per la predisposizione dei dati di input:\n","\n","\n","*   EncTokenizer classe custom per la tokenizzazione dei dati di input al Layer di Encoder di Bert\n","*   DecTokenizer classe custom per la tokenizzazione dei dati di input al Layer di Decoder\n","\n"],"metadata":{"id":"njyY9RWlFMWu"}},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n","dataset = dataset.shuffle(len(input_data)).batch(BATCH_SIZE, drop_remainder=True)\n","\n","train_multilingual = dataset.map(lambda multilingual, ita: multilingual)\n","train_ita = dataset.map(lambda multilingual, ita: ita)"],"metadata":{"id":"fUG1fTAYekOy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678652855477,"user_tz":-60,"elapsed":6497,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"33a56061-3b1e-459b-d31a-c7cba42d0406"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"],"metadata":{"id":"xWO-LrXJe0cF","executionInfo":{"status":"ok","timestamp":1678652855478,"user_tz":-60,"elapsed":19,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def cleanup_text(reserved_tokens, token_txt):\n","\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result"],"metadata":{"id":"yGdsrOoKiYUK","executionInfo":{"status":"ok","timestamp":1678652855478,"user_tz":-60,"elapsed":18,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["tokenizers = tf.Module()"],"metadata":{"id":"qbKNS_uQhHLz","executionInfo":{"status":"ok","timestamp":1678652855478,"user_tz":-60,"elapsed":17,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["### Classe EncTokenizer\n","\n","Classe custom per la tokenizzazione dei dati di italiano e che crea i tre vettori necessari al layer di Encoder \n","Bert:\n","\n","\n","*   input_word_ids\n","*   input_type_ids\n","*   input_mask\n","\n","\n","\n"],"metadata":{"id":"0KUcCnjXVjt3"}},{"cell_type":"code","source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens = {\n","  'start_of_sequence_id': 101,\n","  'end_of_segment_id': 102,\n","  'padding_id': 0,\n","  'mask_id': 103\n","}\n","\n","bert_vocab_args = dict(\n","  # The target vocabulary size\n","  vocab_size = MAX_VOCAB_SIZE,\n","  # Reserved tokens that must be included in the vocabulary\n","  reserved_tokens=reserved_tokens,\n","  # Arguments for `text.BertTokenizer`\n","  bert_tokenizer_params=bert_tokenizer_params,\n","  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","  learn_params={},\n",")"],"metadata":{"id":"Yr0izOZLembx","executionInfo":{"status":"ok","timestamp":1678652855479,"user_tz":-60,"elapsed":17,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["exist_vocab = Path(multilingual_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  multilingual_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_multilingual.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(multilingual_vocab_filenamepath, multilingual_vocab)"],"metadata":{"id":"BwSKtlLSe7bH","executionInfo":{"status":"ok","timestamp":1678652855479,"user_tz":-60,"elapsed":15,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class EncTokenizer(tf.Module):\n","  def __init__(self, tfhub_handle_preprocess):\n","    self.preprocessor = hub.KerasLayer(tfhub_handle_preprocess)\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    return self.preprocessor(strings)"],"metadata":{"id":"WmsNdDLNf6vr","executionInfo":{"status":"ok","timestamp":1678652855845,"user_tz":-60,"elapsed":380,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["tokenizers.multilingual = EncTokenizer(tfhub_handle_preprocess)"],"metadata":{"id":"-4B-HWWcmsmz","executionInfo":{"status":"ok","timestamp":1678652873494,"user_tz":-60,"elapsed":17652,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["### Classe DecTokenizer\n","\n","Classe custom per la tokenizzazione dei dati in lingua italiana per il layer di Decoder\n"],"metadata":{"id":"mICEGEzJVnvx"}},{"cell_type":"code","source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens_vocab=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","  # The target vocabulary size\n","  vocab_size = MAX_VOCAB_SIZE,\n","  # Reserved tokens that must be included in the vocabulary\n","  reserved_tokens=reserved_tokens_vocab,\n","  # Arguments for `text.BertTokenizer`\n","  bert_tokenizer_params=bert_tokenizer_params,\n","  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","  learn_params={},\n",")"],"metadata":{"id":"abBEnJJGV0AD","executionInfo":{"status":"ok","timestamp":1678652873495,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["exist_vocab = Path(ita_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  ita_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_ita.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(ita_vocab_filenamepath, ita_vocab)"],"metadata":{"id":"dGsP1V4nVz6S","executionInfo":{"status":"ok","timestamp":1678652873496,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["START = tf.argmax(tf.constant(reserved_tokens_vocab) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens_vocab) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape(out_type=tf.int32)[0]\n","\n","  starts = tf.fill([count,1], START)\n","  starts = tf.cast(starts, tf.int32)\n","\n","  ends = tf.fill([count,1], END)\n","  ends = tf.cast(ends, tf.int32)\n","\n","  x = tf.concat([starts, ragged, ends], axis=1)\n","  return x"],"metadata":{"id":"BeaD2-uLWT50","executionInfo":{"status":"ok","timestamp":1678652873497,"user_tz":-60,"elapsed":10,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["class DecTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens_vocab, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True, token_out_type=tf.int32)\n","    self._reserved_tokens_vocab = reserved_tokens_vocab\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","    \n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int32))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int32))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int32))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens_vocab, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens_vocab)"],"metadata":{"id":"iaAW-xm5WT1_","executionInfo":{"status":"ok","timestamp":1678652873497,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["tokenizers.ita = DecTokenizer(reserved_tokens_vocab, ita_vocab_filenamepath)"],"metadata":{"id":"svlLobM4WTzC","executionInfo":{"status":"ok","timestamp":1678652877417,"user_tz":-60,"elapsed":3928,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["### Analisi Dati Tokenizzati"],"metadata":{"id":"pKZxiQ5_Whmw"}},{"cell_type":"code","source":["print(f'Vocabolario Italiano : {tokenizers.ita.get_vocab_size()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jrg6TwQzW5LN","executionInfo":{"status":"ok","timestamp":1678652877718,"user_tz":-60,"elapsed":12,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"465517c4-7c25-47bd-88d9-2ea66d8ab858"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabolario Italiano : 7188\n"]}]},{"cell_type":"code","source":["print(input_data[-2:])\n","print(tokenizers.multilingual.tokenize(input_data[-2:])['input_word_ids'][:, :])\n","print('------------------------------------------------------------------')\n","print(target_data[-2:])\n","print(tokenizers.ita.tokenize(target_data[-2:]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKEFeDdGFIGS","executionInfo":{"status":"ok","timestamp":1678652879895,"user_tz":-60,"elapsed":2183,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"5b24c432-1470-4c27-cb1c-a57a45a6f61e"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['Wir haben einen Vertrag . ', 'Ich habe nichts bezahlt . ']\n","tf.Tensor(\n","[[  101 51732 13289 10897 25239   119   102     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]\n"," [  101 21023 21404 38451 13863 78975 10123   119   102     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(2, 128), dtype=int32)\n","------------------------------------------------------------------\n","['Abbiamo un contratto . ', 'Io non ho pagato niente . ']\n","<tf.RaggedTensor [[2, 154, 85, 2430, 11, 3], [2, 93, 81, 92, 1477, 228, 11, 3]]>\n"]}]},{"cell_type":"code","source":["print([min(train_input_data, key = len)])\n","print(tokenizers.multilingual.tokenize([min(train_input_data, key = len)])['input_word_ids'][:, :32])\n","print('------------------------------------------------------------------')\n","print([min(train_target_data, key = len)])\n","print(tokenizers.ita.tokenize([min(train_target_data, key = len)]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O85BUq2INK5j","executionInfo":{"status":"ok","timestamp":1678652881440,"user_tz":-60,"elapsed":1559,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"277d19f6-951a-4a34-9216-7a4c6998c899"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["['Hoi . ']\n","tf.Tensor(\n","[[  101 20220 10116   119   102     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0]], shape=(1, 32), dtype=int32)\n","------------------------------------------------------------------\n","['No ? ']\n","<tf.RaggedTensor [[2, 382, 24, 3]]>\n"]}]},{"cell_type":"code","source":["print([max(train_input_data, key = len)])\n","print(tokenizers.multilingual.tokenize([max(train_input_data, key = len)])['input_word_ids'])\n","print('------------------------------------------------------------------')\n","print([max(train_target_data, key = len)])\n","print(tokenizers.ita.tokenize([max(train_target_data, key = len)]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JzJl4YGBPR3Z","executionInfo":{"status":"ok","timestamp":1678652882606,"user_tz":-60,"elapsed":1178,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"5bc2e954-ee41-4dcf-8d5d-7f6f01a3d4e8"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[\"There is no such thing , at this stage of the world' s history in the United States , as an independent press . You know it and I know it . There is not one of you who dare write your honest opinions , and if you did , you know beforehand that it would never appear in print . I am paid weekly for keeping my honest opinions out of the paper I am connected with . Others of you are paid similar salaries for similar things , and any of you who would be foolish as to write honest opinions would be out on the streets looking for another job . If I allowed my honest opinions to appear in one issue of my papers , before twenty four hours my occupation would be gone . The business of the journalist is to destroy the truth , to lie outright , to pervert , to vilify , to fawn at the feet of Mammon , and to sell his country and his race for his daily bread . You know it and I know it , and what folly is this toasting an independent press ? We are the jumping jacks , they pull the strings and we dance . Our talents , our possibilities and our lives are all the property of other men . We are intellectual prostitutes . \"]\n","tf.Tensor(\n","[[  101 11723 10124 10192 11049 40414   117 10160 10531 15365 10108 10105\n","  11356   112   187 11486 10106 10105 10609 10859   117 10146 10151 16584\n","  21040   119 11065 21852 10271 10111   146 21852 10271   119 11723 10124\n","  10472 10464 10108 13028 10479 45476 28685 20442 14923 13051 72773   117\n","  10111 12277 13028 12172   117 13028 21852 11360 41137 10189 10271 10894\n","  14794 22641 10106 31210   119   146 10392 25938 33159 10142 51318 15127\n","  14923 13051 72773 10950 10108 10105 17895   146 10392 26989 10169   119\n","  64738 10108 13028 10301 25938 13213 20509 15388 10142 13213 24682   117\n","  10111 11178 10108 13028 10479 10894 10347   174 47195 15529 10146 10114\n","  28685 14923 13051 72773 10894 10347 10950 10135 10105 41969 34279 10142\n","  12864 23627   119 14535   146 18162 15127   102]], shape=(1, 128), dtype=int32)\n","------------------------------------------------------------------\n","[\"Non vi è nulla di simile , in questa fase della storia del mondo in America , come una stampa indipendente . Tu lo sai e io lo so . Non c' è nessuno di voi che osa scrivere le proprie opinioni oneste , e se l' avete fatto , si sa in anticipo che non apparirebbe mai in stampa Io sono pagato settimanalmente per tenere le mie opinioni oneste fuori dal giornale a cui sono collegato . Altri di voi sono pagati con stipendi simili per cose simili , e chiunque di voi che sarebbe sciocco da scrivere opinioni oneste sarebbe fuori per le strade in cerca di un altro lavoro . Se ho lasciato le mie opinioni oneste di apparire in una questione di mie carte , prima di 24 ore la mia occupazione sarebbe andato . L' attività del giornalista è quella di distruggere la verità , di mentire apertamente , di pervertire , di diffamare , di strisciare ai piedi di Mammona , e di vendere il suo paese e la sua razza per il suo pane quotidiano . Tu lo sai e io lo so , e che follia è questo brindare per una stampa indipendente ? Noi siamo i burattini , loro tirano i fili e noi balliamo . I nostri talenti , le nostre possibilità e le nostre vite sono tutte proprietà di altri uomini . Noi siamo delle prostitute intellettuali . \"]\n","<tf.RaggedTensor [[2, 81, 255, 31, 250, 80, 1662, 10, 86, 111, 4009, 102, 326, 100, 201,\n","  86, 574, 10, 107, 89, 2178, 2569, 11, 139, 114, 478, 31, 93, 114, 195,\n","  11, 81, 29, 8, 31, 247, 80, 178, 84, 41, 552, 502, 94, 2880, 1576, 4037,\n","  10, 31, 104, 38, 8, 216, 145, 10, 96, 266, 86, 3004, 84, 81, 6806, 1039,\n","  134, 86, 2178, 93, 88, 1477, 264, 1285, 87, 1801, 94, 438, 1576, 4037,\n","  327, 200, 893, 27, 192, 88, 3191, 179, 11, 346, 80, 178, 88, 7000, 97,\n","  45, 183, 2744, 1839, 2143, 87, 233, 2143, 10, 31, 1286, 80, 178, 84,\n","  366, 6514, 99, 502, 1576, 4037, 366, 327, 87, 94, 1968, 86, 1158, 80,\n","  85, 245, 186, 11, 104, 92, 614, 94, 438, 1576, 4037, 80, 6806, 86, 89,\n","  898, 80, 438, 2380, 10, 144, 80, 6239, 485, 82, 112, 3480, 366, 348, 11,\n","  38, 8, 1647, 100, 2776, 31, 231, 80, 4185, 82, 397, 10, 80, 3864, 5399,\n","  10, 80, 87, 6229, 10, 80, 80, 3273, 2989, 10, 80, 45, 3417, 2190, 1139,\n","  267, 655, 80, 125, 767, 306, 10, 31, 80, 3268, 83, 118, 252, 31, 82,\n","  117, 3245, 87, 83, 118, 823, 5974, 11, 139, 114, 478, 31, 93, 114, 195,\n","  10, 31, 84, 3845, 31, 101, 28, 262, 1186, 149, 87, 89, 2178, 2569, 24,\n","  133, 189, 35, 28, 1632, 666, 294, 10, 115, 6036, 141, 35, 32, 3784, 31,\n","  133, 28, 2734, 5025, 11, 35, 553, 5606, 10, 94, 886, 994, 31, 94, 886,\n","  3398, 88, 295, 2406, 80, 346, 518, 11, 133, 189, 128, 42, 257, 636,\n","  4134, 160, 5163, 11, 3]]>\n"]}]},{"cell_type":"markdown","source":["## Creazione dataset\n","Utilizzo della libreria tf.data per la gestione del dataset da utilizzare.\n","Verranno creati batch di esempi che verranno utilizzati durante l'addestramento."],"metadata":{"id":"5QIDajkEsVU1"}},{"cell_type":"code","source":["def prepare_batch(multilingual, ita):\n","  zero = tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH], tf.int32)\n","\n","  # Tokenizzo l'input per l'Encoder\n","  encoder = tokenizers.multilingual.tokenize(multilingual)          \n","\n","  # Tokenizzo l'input per il Decder e creo la variabile Target\n","  ita = tokenizers.ita.tokenize(ita)\n","  decoder = ita[:, :-1].to_tensor()  # Drop the [END] tokens\n","  target = ita[:, 1:].to_tensor()   # Drop the [START] tokens\n","  \n","  decoder = tf.concat([decoder, zero], 1)\n","  decoder = decoder[:, :(MAX_SEQ_LENGTH)]\n","\n","  target = tf.concat([target, zero], 1)\n","  target = target[:, :(MAX_SEQ_LENGTH)]\n","\n","  return (encoder, decoder), target"],"metadata":{"id":"ccH3jHoABPzV","executionInfo":{"status":"ok","timestamp":1678652882607,"user_tz":-60,"elapsed":7,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def make_batches(ds):\n","  return (\n","    ds\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE)\n","    .map(prepare_batch, tf.data.AUTOTUNE)\n","    .prefetch(buffer_size=tf.data.AUTOTUNE))"],"metadata":{"id":"l_dswlCiBTdR","executionInfo":{"status":"ok","timestamp":1678652882607,"user_tz":-60,"elapsed":7,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# Definizione del dataset\n","# [from_tensor_slices] permette di recuperare batch\n","# di esempi dai dataset di riferimento\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_target_data))\n","validation_dataset = tf.data.Dataset.from_tensor_slices((validation_input_data, validation_target_data))\n","\n","# impostazione del recupero di esempi presi in maniera\n","# casuale in gruppi di [BATCH_SIZE] tra quelli disponibili\n","train_dataset = make_batches(train_dataset)\n","validation_dataset = make_batches(validation_dataset)"],"metadata":{"id":"tktJ5YuIsYe3","executionInfo":{"status":"ok","timestamp":1678652884908,"user_tz":-60,"elapsed":2307,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for (enc_input, dec_input), target in train_dataset.take(1):\n","  print('----------------------- ENCODER  -------------------------------')\n","  print(f'Shape                    : {enc_input[\"input_word_ids\"].shape}')\n","  print(f'Word Ids                 : {enc_input[\"input_word_ids\"][0, :MAX_SEQ_LENGTH]}')\n","  print(f'Input Mask               : {enc_input[\"input_mask\"][0, :MAX_SEQ_LENGTH]}')\n","  print('--------------------- DECODER ----------------------------------')\n","  print(f'Shape it input           : {dec_input.shape}')\n","  print(f'Example it input         : {dec_input[0]}')  \n","  print('--------------------- TARGET -----------------------------------')\n","  print(f'Shape it input           : {target.shape}')\n","  print(f'Example it target        : {target[0]}')  "],"metadata":{"id":"VH_aKPlV_AWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678652886670,"user_tz":-60,"elapsed":1765,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"87e50d85-29ac-4b27-f20b-9368ae4b7af5"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------- ENCODER  -------------------------------\n","Shape                    : (64, 128)\n","Word Ids                 : [   101  31349  10298  72374  16828  10269    117  10118  19802 101267\n","  11967    119    102      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0]\n","Input Mask               : [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","--------------------- DECODER ----------------------------------\n","Shape it input           : (64, 128)\n","Example it input         : [   2  164   31 3129 6337 1928   11    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0]\n","--------------------- TARGET -----------------------------------\n","Shape it input           : (64, 128)\n","Example it target        : [ 164   31 3129 6337 1928   11    3    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0]\n"]}]},{"cell_type":"markdown","source":["## Encoder BERT\n","\n","Predispondo la classe necessaria per la costruzione di BERT\n"],"metadata":{"id":"8dtVuZGJpvXl"}},{"cell_type":"code","source":["class EncoderBert(tf.keras.layers.Layer):\n","  def __init__(self, bert_encoder, embedding_dim, max_len):\n","    super(EncoderBert, self).__init__()\n","\n","    self.encoder = hub.KerasLayer(bert_encoder, name='BERT_encoder', trainable=False)\n","\n","    self.conv_1 = tf.keras.layers.Conv1D(embedding_dim * 4, 1, activation='relu') \n","    self.conv_2 = tf.keras.layers.Conv1D(embedding_dim, 1, activation='relu') \n","    self.lambda_layer = tf.keras.layers.Lambda(lambda x: x[:,:max_len])\n","    self.max_len = max_len\n","\n","  def call(self, x, debug=False):\n","\n","    if debug:\n","      print(f'****************** DEBUG ENCODER BERT ******************')\n","      print(f\"First example\")\n","      print(f'Keys                         : {list(x.keys())}')\n","      print(f'Shape                        : {x[\"input_word_ids\"].shape}')\n","      print(f'Word Ids                     : {x[\"input_word_ids\"][0, :16]}')\n","      print(f'Input Mask                   : {x[\"input_mask\"][0, :16]}')\n","      \n","    # x = self.encoder(x)['sequence_output'] \n","    # encoder_outputs stato intermedio di BERT prima che esegua la traduzione recuperare la metà della lunghezza\n","    x = self.encoder(x)['encoder_outputs'] \n","    x = x[int(len(x) / 2) - 1]\n","\n","    if debug:\n","      print()\n","      print(f'Encoder Outputs BERT Shape   : {x.shape}')\n","      print(f'Encoder Outputs BERT Values  : {x[0, :1, :16]}')\n","\n","    x = self.conv_1(x)\n","    if debug:\n","      print()\n","      print(f'Sequence Conv1 Shape         : {x.shape}')\n","\n","    x = self.conv_2(x)\n","    if debug:\n","      print(f'Sequence Conv2 Shape         : {x.shape}')\n","\n","    x = self.lambda_layer(x)\n","    if debug:\n","      print(f'Sequence Lambda Layer        : {x.shape}')\n","      print()\n","      print(f'Sequence Outputs Values      : {x[0, 0, :16]}')      \n","      print('*********************************************************') \n","\n","    return x"],"metadata":{"id":"m7v9Y-Lep4CD","executionInfo":{"status":"ok","timestamp":1678652886670,"user_tz":-60,"elapsed":7,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["encoder_bert = EncoderBert(tfhub_handle_encoder, \n","                           EMBEDDING_DIM, \n","                           MAX_SEQ_LENGTH)\n","\n","bert_outputs = encoder_bert(enc_input, debug) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q08luTkusEfn","executionInfo":{"status":"ok","timestamp":1678652903434,"user_tz":-60,"elapsed":16770,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"340104a5-c5c6-4429-d167-ed6efb99ac17"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["****************** DEBUG ENCODER BERT ******************\n","First example\n","Keys                         : ['input_mask', 'input_word_ids']\n","Shape                        : (64, 128)\n","Word Ids                     : [   101  31349  10298  72374  16828  10269    117  10118  19802 101267\n","  11967    119    102      0      0      0]\n","Input Mask                   : [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n","\n","Encoder Outputs BERT Shape   : (64, 128, 768)\n","Encoder Outputs BERT Values  : [[ 0.10977075  0.20603485 -0.06048406  0.06641597  0.60671943 -0.06845495\n","   0.17543602 -0.0774892  -0.0541014  -0.15081355  0.00659086 -0.0210262\n","  -0.6052111  -0.28018197  0.11806128  0.16583753]]\n","\n","Sequence Conv1 Shape         : (64, 128, 512)\n","Sequence Conv2 Shape         : (64, 128, 128)\n","Sequence Lambda Layer        : (64, 128, 128)\n","\n","Sequence Outputs Values      : [0.04035005 0.         0.02120987 0.         0.         0.\n"," 0.12940168 0.9227224  0.         0.41434366 0.         0.\n"," 0.17287062 0.39301676 0.26035973 0.        ]\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## Decoder\n","\n","Predispondo la classe necessaria per la costruzione di un Layer di Decoder"],"metadata":{"id":"ReEQ5rX7aGtl"}},{"cell_type":"markdown","source":["### TOKEN AND POSITION EMBEDDING\n","\n","Implementazione del blocco Embedding per l'utilizzo di vettori posizionali insieme ai vettori di token di parole tramite estensione della classe Layer di Keras. "],"metadata":{"id":"gAu1IXlRZzlq"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(tf.keras.layers.Layer):\n","  def __init__(self, maxlen, vocab_size, embed_dim):\n","    super(TokenAndPositionEmbedding, self).__init__()\n","    self.maxlen = maxlen\n","    self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","    self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","  def call(self, x, debug=False):\n","    x = tf.keras.preprocessing.sequence.pad_sequences(x, maxlen=self.maxlen, padding='post')\n","    maxlen = tf.shape(x)[-1]\n","\n","    if debug:\n","      print('********** DEBUG TOKEN AND POSITION EMBEDDING ***********')\n","      print(f'Sequence Max len                          : {maxlen}')\n","      print(f'Sequence Shape                            : {tf.shape(x)}')\n","\n","    positions = tf.range(start=0, limit=maxlen, delta=1)\n","    positions = self.pos_emb(positions)\n","    x = self.token_emb(x)\n","    output = x + positions\n","\n","    if debug:\n","      print(f'Shape TokenAndPositionEmbedding           : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"o9-RSKTqsmUC","executionInfo":{"status":"ok","timestamp":1678652903434,"user_tz":-60,"elapsed":28,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["token_position_it = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.ita.get_vocab_size(), EMBEDDING_DIM)\n","\n","inputs_decoder = token_position_it(dec_input, debug)"],"metadata":{"id":"rr_EWQUX8EWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678652903434,"user_tz":-60,"elapsed":25,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"0462c757-d507-4e9d-edcf-00ecec500363"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 128\n","Sequence Shape                            : [ 64 128]\n","Shape TokenAndPositionEmbedding           : (64, 128, 128)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### LAYER DECODER\n","\n","Implementazione di un blocco di DecoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"XdLv-6nidKGK"}},{"cell_type":"markdown","source":["#### DecodeBert\n","\n","Implmentazione di un blocco di  decodifica custom per decodificare l'output dal layer EncoderBert prima di passarlo al Decoder del Transformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"_iq7Y-d4eRd8"}},{"cell_type":"code","source":["class DecodeBert(tf.keras.layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DecodeBert'):\n","    super(DecodeBert, self).__init__()\n","    self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = tf.keras.Sequential(\n","      [tf.keras.layers.Dense(ff_dim, activation='relu'), \n","       tf.keras.layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = tf.keras.layers.LayerNormalization()\n","    self.layernorm2 = tf.keras.layers.LayerNormalization()\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, bert_outputs, training=False, debug=False):\n","    attn_output = self.att(query=bert_outputs,\n","                           value=bert_outputs, \n","                           key=bert_outputs)\n","    \n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(bert_outputs + attn_output)\n","\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","\n","    output = self.layernorm2(out1 + ffn_output)\n","\n","    if debug:\n","      print('********************* DEBUG DECODE-BERT *********************')\n","      print(f'Shape Input Layer Decode-Bert       : {bert_outputs.shape}')\n","      print(f'Shape Output Layer Decode-Bert      : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"joTBTlWF8ETD","executionInfo":{"status":"ok","timestamp":1678652903435,"user_tz":-60,"elapsed":19,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["encoder = DecodeBert(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_encoder = encoder(bert_outputs=bert_outputs,\n","                          training=training, \n","                          debug=debug)"],"metadata":{"id":"JaIzBxFCfKe9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678652904245,"user_tz":-60,"elapsed":828,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"51d5a14e-eccb-423c-b698-c4e609825029"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (64, 128, 128)\n","Shape Output Layer Decode-Bert      : (64, 128, 128)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["#### Layer Decoder"],"metadata":{"id":"dMTKLwd3dRw5"}},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DEC'):\n","    super(Decoder, self).__init__()\n","    self.decode_bert = DecodeBert(max_len=max_len, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, rate=rate)\n","    self.att1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.att2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = tf.keras.Sequential(\n","      [tf.keras.layers.Dense(ff_dim, activation='relu'), \n","       tf.keras.layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = tf.keras.layers.LayerNormalization()\n","    self.layernorm2 = tf.keras.layers.LayerNormalization()\n","    self.layernorm3 = tf.keras.layers.LayerNormalization()\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, inputs, bert_outputs, training=False, debug=False):\n","    attn_output1 = self.att1(query=inputs,\n","                             value=inputs, \n","                             key=inputs, \n","                             use_causal_mask=True)\n","    \n","    attn_output1 = self.dropout1(attn_output1)\n","    out1 = self.layernorm1(inputs + attn_output1)\n","\n","    dec_bert = self.decode_bert(bert_outputs=bert_outputs, training=training, debug=debug)\n","\n","    attn_output2 = self.att2(key=dec_bert, \n","                             value=dec_bert, \n","                             query=out1)\n","    \n","    attn_output2 = self.dropout2(attn_output2, training=training)\n","    out2 = self.layernorm2(out1 + attn_output2)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","\n","    output = self.layernorm3(out2 + ffn_output)\n","\n","    if debug:\n","      print('******************* DEBUG DECODER ***********************')\n","      print(f'Input Shape                       : {inputs.shape}')\n","      print(f'Shape Outputs Decoder             : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"SO5rYsFpfFS_","executionInfo":{"status":"ok","timestamp":1678652904246,"user_tz":-60,"elapsed":8,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_decoder = decoder(inputs=inputs_decoder, \n","                          bert_outputs=bert_outputs,  \n","                          training=training,\n","                          debug=debug)"],"metadata":{"id":"yysVdkHH8EPH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678652904246,"user_tz":-60,"elapsed":8,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"fce545a4-e134-4006-a4b9-38633b0db2d7"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (64, 128, 128)\n","Shape Output Layer Decode-Bert      : (64, 128, 128)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (64, 128, 128)\n","Shape Outputs Decoder             : (64, 128, 128)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## TRANSFORMER\n","\n","Implementazione del blocco Transformer tramite estensione della classe Layer di Keras."],"metadata":{"id":"ne4zTOG_NKfV"}},{"cell_type":"code","execution_count":42,"metadata":{"pycharm":{"name":"#%%\n"},"id":"lw2xMCAMC_4M","executionInfo":{"status":"ok","timestamp":1678652904646,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"outputs":[],"source":["class TransformerBlock(tf.keras.Model):\n","  def __init__(self, \n","               num_layers, \n","               embed_dim, \n","               num_heads, \n","               ff_dim, \n","               max_len,\n","               vocab_size,\n","               tfhub_handle_encoder,\n","               rate=0.5):\n","    \n","    super(TransformerBlock, self).__init__()\n","\n","    self.num_layers = num_layers\n","\n","    self.token_pos_dec = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n","\n","    self.encoder = EncoderBert(tfhub_handle_encoder, embed_dim, max_len)\n","    self.decoder = [Decoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","    self.final_layer = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, inputs, training=False, debug=False):\n","    inputs_encoder, inputs_decoder  = inputs\n","\n","    encoder_output = self.encoder(inputs_encoder, debug) \n","\n","    inputs_decoder = self.token_pos_dec(inputs_decoder, debug)\n","\n","    if debug:\n","      print(f'---------------- DEBUG TRANSFORMER BLOCK ----------------')\n","      print(f'inputs_encoder       : {inputs_encoder[\"input_word_ids\"].shape}')\n","      print(f'inputs_decoder       : {inputs_decoder.shape}')      \n","\n","    transformer_output = inputs_decoder\n","      \n","    for i in range(self.num_layers):\n","      transformer_output = self.decoder[i](inputs=transformer_output, \n","                                           bert_outputs=encoder_output, \n","                                           training=training,\n","                                           debug=debug)\n","\n","    transformer_output = self.dropout(transformer_output)\n","    logits = self.final_layer(transformer_output)\n","\n","    if debug:\n","      print(f'Output Shape       : {logits.shape}')\n","      print(f'Output Transformer : {logits[0, :1, :12]}')    \n","      print(f'---------------------------------------------------------')\n","\n","    return logits"]},{"cell_type":"code","source":["transformer = TransformerBlock(NUM_LAYERS, \n","                               EMBEDDING_DIM, \n","                               NUM_HEADS, \n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.ita.get_vocab_size(),\n","                               tfhub_handle_encoder,\n","                               DROPUOT)\n","\n","transformer_output = transformer((enc_input, dec_input), \n","                                 training=training,\n","                                 debug=debug)"],"metadata":{"id":"pr--G0ZZVAMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678652909073,"user_tz":-60,"elapsed":4435,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"2a17c69f-da3b-49a0-f018-222b2cb47d28"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["****************** DEBUG ENCODER BERT ******************\n","First example\n","Keys                         : ['input_mask', 'input_word_ids']\n","Shape                        : (64, 128)\n","Word Ids                     : [   101  31349  10298  72374  16828  10269    117  10118  19802 101267\n","  11967    119    102      0      0      0]\n","Input Mask                   : [1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n","\n","Encoder Outputs BERT Shape   : (64, 128, 768)\n","Encoder Outputs BERT Values  : [[ 0.10977075  0.20603485 -0.06048406  0.06641597  0.60671943 -0.06845495\n","   0.17543602 -0.0774892  -0.0541014  -0.15081355  0.00659086 -0.0210262\n","  -0.6052111  -0.28018197  0.11806128  0.16583753]]\n","\n","Sequence Conv1 Shape         : (64, 128, 512)\n","Sequence Conv2 Shape         : (64, 128, 128)\n","Sequence Lambda Layer        : (64, 128, 128)\n","\n","Sequence Outputs Values      : [0.21571425 0.2955563  0.12169532 0.4372349  0.         0.\n"," 0.19958243 0.33000088 0.56014323 0.         0.5786598  0.01277363\n"," 0.         0.7172626  0.43071565 0.9666981 ]\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 128\n","Sequence Shape                            : [ 64 128]\n","Shape TokenAndPositionEmbedding           : (64, 128, 128)\n","*********************************************************\n","---------------- DEBUG TRANSFORMER BLOCK ----------------\n","inputs_encoder       : (64, 128)\n","inputs_decoder       : (64, 128, 128)\n","********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (64, 128, 128)\n","Shape Output Layer Decode-Bert      : (64, 128, 128)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (64, 128, 128)\n","Shape Outputs Decoder             : (64, 128, 128)\n","*********************************************************\n","Output Shape       : (64, 128, 7188)\n","Output Transformer : [[-0.59353304 -0.05670902 -0.09403878  0.06468949  0.1043851  -0.37366864\n","   0.2737821   0.39395615  0.04289657  0.22835596  0.1975859  -0.18531178]]\n","---------------------------------------------------------\n"]}]},{"cell_type":"code","source":["transformer.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kYt6ehvh-8B","executionInfo":{"status":"ok","timestamp":1678652909409,"user_tz":-60,"elapsed":388,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"e608545b-783c-48fe-f421-7b37e3bde70f"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer_block\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," token_and_position_embeddin  multiple                 936448    \n"," g_1 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," encoder_bert_1 (EncoderBert  multiple                 135193472 \n"," )                                                               \n","                                                                 \n"," DEC (Decoder)               multiple                  1592224   \n","                                                                 \n"," dropout_16 (Dropout)        multiple                  0         \n","                                                                 \n"," dense_10 (Dense)            multiple                  927252    \n","                                                                 \n","=================================================================\n","Total params: 138,649,396\n","Trainable params: 3,915,316\n","Non-trainable params: 134,734,080\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Addestramento Modello"],"metadata":{"id":"IFmcHTSDTvYk"}},{"cell_type":"markdown","source":["### Compilazione"],"metadata":{"id":"tiuqPlHo0Z0n"}},{"cell_type":"code","source":["transformer.compile(\n","  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","  optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_ADAM, \n","                                     beta_1=BETA_1, \n","                                     beta_2=BETA_2),\n","  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"],"metadata":{"id":"bOyqCyjIr-L2","executionInfo":{"status":"ok","timestamp":1678652909410,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 save_best_only=True)\n","\n","# Create a callback Tensorboard\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n","\n","# Create a callback save the log history\n","json_logging_callback = tf.keras.callbacks.LambdaCallback(\n","  on_epoch_end=lambda epoch, logs: json_log.write(\n","    json.dumps({'epoch': epoch, \n","                'loss': logs['loss'],\n","                'sparse_categorical_accuracy': logs['sparse_categorical_accuracy'],\n","                'val_loss': logs['val_loss'],\n","                'val_sparse_categorical_accuracy': logs['val_sparse_categorical_accuracy']}) + '\\n'),\n","  on_train_end=lambda logs: json_log.close()\n",")"],"metadata":{"id":"3hurmpSjJ_dT","executionInfo":{"status":"ok","timestamp":1678652909410,"user_tz":-60,"elapsed":8,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["### Train"],"metadata":{"id":"Day7C7Qh0b4G"}},{"cell_type":"code","source":["start = datetime.datetime.now()\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset,\n","                initial_epoch=0,\n","                epochs=EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"etOGtBcer9yi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d0de85ad-678e-4f71-d48d-5057d6a62025"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/5\n","1038/1038 [==============================] - 1287s 1s/step - loss: 1.7642 - sparse_categorical_accuracy: 0.9081 - val_loss: 0.5394 - val_sparse_categorical_accuracy: 0.9262\n","Epoch 2/5\n","1038/1038 [==============================] - 1282s 1s/step - loss: 0.5172 - sparse_categorical_accuracy: 0.9282 - val_loss: 0.4691 - val_sparse_categorical_accuracy: 0.9323\n","Epoch 3/5\n","1038/1038 [==============================] - 1235s 1s/step - loss: 0.4608 - sparse_categorical_accuracy: 0.9332 - val_loss: 0.4258 - val_sparse_categorical_accuracy: 0.9358\n","Epoch 4/5\n"," 653/1038 [=================>............] - ETA: 6:36 - loss: 0.4287 - sparse_categorical_accuracy: 0.9359"]}]},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"R5WMR092fEFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","\n","json_log = open(log_history, mode='a', buffering=1, encoding='utf-8')\n","\n","transformer.fit(train_dataset,\n","                initial_epoch=EPOCHS_ADAM,\n","                epochs=EPOCHS_ADAM+EPOCHS_ADAM,\n","                shuffle=True,\n","                validation_data=validation_dataset,\n","                callbacks=[tensorboard_callback,\n","                           json_logging_callback, \n","                           cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"uQkacjBJfI7w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Valutazione dell'addestramento\n","Avendo in output il log ed i risultati dell'addestramento, possiamo visualizzare\n","queste informazioni relativamente alle metriche di interesse."],"metadata":{"id":"L0w4wF79UhAp"}},{"cell_type":"code","source":["# Recupero il log di addestramento\n","df_history = pd.read_json(log_history, lines=True)\n","\n","# visualizzazione andamento addestramento\n","# su un grafico composto da due sub-plot\n","# uno per il loss, l'altro per l'accuracy\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n","\n","# Errore durante l'addestramento\n","ax1.plot(df_history['loss'], label='Loss')\n","ax1.plot(df_history['val_loss'], label='Validation Loss')\n","ax1.set_title('Training Loss')\n","ax1.legend()\n","\n","# Accuratezza durante l'addestramento\n","ax2.plot(df_history['sparse_categorical_accuracy'], label='Accuracy')\n","ax2.plot(df_history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n","ax2.set_title('Training Accuracy')\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"RpXR2p5VAdoG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test del modello\n","La seguente cella permette di caricare l'ultimo checkpoint dell'addestramento\n","precedentemente salvato."],"metadata":{"id":"ReOkcBp2WHWW"}},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"5PIf_6-RSBb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Translate:\n","  def __init__(self, transformer_block, tokenizers):\n","    self.transformer = transformer_block\n","    self.tokenizers = tokenizers\n","\n","  def predict(self, input_text, max_length):\n","    if input_text is None:\n","      input_text = input_data[np.random.choice(len(input_data))]\n","      print(input_text)\n","\n","    inputs_bert = self.tokenizers.multilingual.tokenize(input_text)\n","\n","    start_end = self.tokenizers.ita.tokenize([''])[0]\n","    start = (start_end[0][tf.newaxis]).numpy()[0]\n","    end = (start_end[1][tf.newaxis]).numpy()[0]\n","\n","    output_array = tf.TensorArray(dtype=tf.int32, size=max_length, dynamic_size=True)\n","    output_array = output_array.write(0, tf.constant([start]))     \n","\n","    out_words = []\n","\n","    for i in tf.range(max_length):\n","      # decodifica e recupero probabilità di output\n","      output = tf.transpose(output_array.stack())\n","\n","      transformer_output = transformer((inputs_bert, output), \n","                                        training=False,\n","                                        debug=False)\n","\n","      predictions = transformer_output[:, -1:, :]\n","\n","      # selezione della parola più probabile\n","      predict = tf.argmax(predictions, -1)\n","      pred_values = (tf.keras.backend.argmax(transformer_output, axis=-1)).numpy()\n","    \n","      # inserimento della parola nella sequenza di output\n","      output_array = output_array.write(i+1, [pred_values[0][i]])\n","\n","      if pred_values[0][i] == end:\n","        break\n","\n","    output = tf.transpose(output_array.stack())\n","    text = tokenizers.ita.detokenize(output)[0]  \n","    tokens = tokenizers.ita.lookup(output)[0]\n","\n","    return text, tokens"],"metadata":{"id":"L2PEoJVb1V8x","executionInfo":{"status":"ok","timestamp":1678650680776,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["\n","test_sequences = [test_input_data[2], test_input_data[26], test_input_data[19], \n","                  test_input_data[34], test_input_data[45], test_input_data[58], \n","                  test_input_data[62], test_input_data[71], test_input_data[84],\n","                  test_input_data[90], test_input_data[99], test_input_data[0]]\n","\n","target_sequences = [test_target_data[2], test_target_data[26], test_target_data[19], \n","                    test_target_data[34], test_target_data[45], test_target_data[58], \n","                    test_target_data[62], test_target_data[71], test_target_data[84],\n","                    test_target_data[90], test_target_data[99], test_target_data[0]]\n","                    "],"metadata":{"id":"kwiBV43cMt_C","executionInfo":{"status":"ok","timestamp":1678650680778,"user_tz":-60,"elapsed":10,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["# test_sequences = test_input_data\n","\n","# target_sequences = test_target_data\n","\n","translate = Translate(transformer_block=transformer,\n","                      tokenizers=tokenizers)\n","\n","for test_sequence, target in zip(test_sequences, target_sequences):\n","  text, token = translate.predict(tf.constant([test_sequence]), MAX_SEQ_LENGTH)\n","\n","  print(f'{\"Input\":15s}: {test_sequence}')\n","  print(f'{\"Target\":15s}: {target}')\n","  print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')  \n","  print('---------------------------------------------')"],"metadata":{"id":"udIjI2jZWR6g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Qex8JVqvJxzp"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}