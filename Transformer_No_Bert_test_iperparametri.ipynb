{"cells":[{"cell_type":"code","source":["!pip install -q -U 'tensorflow-text==2.8.*'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NE4enZGpvMRX","executionInfo":{"status":"ok","timestamp":1670685044984,"user_tz":-60,"elapsed":71853,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"276ed6ed-082f-44fc-d256-eb354787fb77"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 26.0 MB/s \n","\u001b[K     |████████████████████████████████| 498.0 MB 12 kB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 62.2 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 72.7 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 59.5 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q tf-models-official"],"metadata":{"id":"FPtWz_qHuofc","executionInfo":{"status":"ok","timestamp":1670685123866,"user_tz":-60,"elapsed":78888,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3d000db-8ed1-4c9d-b781-22485210dae0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.4 MB 33.5 MB/s \n","\u001b[K     |████████████████████████████████| 662 kB 71.9 MB/s \n","\u001b[K     |████████████████████████████████| 238 kB 62.0 MB/s \n","\u001b[K     |████████████████████████████████| 2.3 MB 61.6 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 49.5 MB/s \n","\u001b[K     |████████████████████████████████| 588.3 MB 20 kB/s \n","\u001b[K     |████████████████████████████████| 352 kB 25.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 57.1 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n","\u001b[K     |████████████████████████████████| 118 kB 69.6 MB/s \n","\u001b[K     |████████████████████████████████| 38.2 MB 66.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 65.2 MB/s \n","\u001b[K     |████████████████████████████████| 6.0 MB 65.1 MB/s \n","\u001b[K     |████████████████████████████████| 439 kB 74.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.7 MB 64.1 MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hJy-juNOpUOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685159548,"user_tz":-60,"elapsed":35265,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"88297a09-2213-479d-adaf-341e9b213841"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"UaAiWsEuC_4K"},"outputs":[],"source":["import os\n","import re\n","import time\n","import unicodedata\n","import datetime\n","import pathlib\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","from keras import layers\n","\n","import tensorflow_hub as hub\n","import tensorflow_models as tfm\n","\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","source":["tf.get_logger().setLevel('ERROR')\n","tf.config.run_functions_eagerly(True)"],"metadata":{"id":"uKEqRlKowOQS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Variabili Globali"],"metadata":{"id":"HRe16D-rUBLA"}},{"cell_type":"code","source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'ita.txt'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# PATH LOG Tensorboard\n","PATH_LOG = 'logs/fit/transformer_no_bert_v5'\n","PATH_LOG = os.path.abspath(os.path.join(root_folder, PATH_LOG))\n","log_dir =  os.path.abspath(os.path.join(PATH_LOG, datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))) \n","\n","# PATH WEIGHTS Tensorboard\n","PATH_WEIGHTS = 'weights/transformer_nobert_v5'\n","PATH_WEIGHTS = os.path.abspath(os.path.join(root_folder, PATH_WEIGHTS))\n","\n","# MODELLO TOKENIZER\n","model_name = 'tokenizer_en_it_model'\n","tokenizer_folder_name = 'tokenizer'\n","\n","TOKEN_PATH = os.path.abspath(os.path.join(root_folder, tokenizer_folder_name))\n","tokenizer_filenamepath = os.path.abspath(os.path.join(TOKEN_PATH, model_name))"],"metadata":{"id":"ewLgCIuEpczO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# parametri per il modello\n","INPUT_COLUMN = 'input'\n","TARGET_COLUMN = 'target'\n","TARGET_FOR_INPUT = 'target_for_input'\n","NUM_SAMPLES = 350000 # portato da 10.000 a 100.000\n","TRAIN = 18016\n","VALIDATION = 6016\n","TEST = 100\n","\n","MAX_VOCAB_SIZE = 20000 # portato da 20.0000 a 200.000\n","EMBEDDING_DIM = 64  # --> 256  Densa non lineare relu --> 64  Densa non lineare relu (oppure Conv1D kernel=1)\n","HIDDEN_DIM = 1024 # numero di celle nei layer ricorrenti nascosti\n","\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 2000\n","EPOCHS = 70\n","MAX_SEQ_LENGTH = 128\n","\n","NUM_LAYERS = 1 # Numero di layer di Encoder e Decoder del Transformer\n","NUM_HEADS = 8 # Numero di meccanismi di multi-head attention\n","FF_DIM = 16 # Numero di celle dei Layer Feed Forward\n","DROPUOT = 0.5\n","\n","# LEARNING_RATE=0.01\n","\n","# IMPOSTO IL DEBUG A TRUE \n","debug = True\n","training = True"],"metadata":{"id":"8CN-4Uzoqbjl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Caricamento Dati"],"metadata":{"id":"LU7AorKXT8K7"}},{"cell_type":"code","source":["# Caricamento dataset: frasi in inglese, frasi in italiano\n","df = pd.read_csv(\n","    train_filenamepath,\n","    sep=\"\\t\",\n","    header=None,\n","    names=[INPUT_COLUMN, TARGET_COLUMN],\n","    usecols=[0,1],\n","    nrows=NUM_SAMPLES\n",")\n","\n","df = df[-(TRAIN+VALIDATION+TEST):].reset_index(drop=True)\n","\n","# Mischio il dataset in modo che sia più uniforme tra train e test\n","df = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n","\n","print(df.iloc[-4:], '\\n')\n","\n","# Preprocessing dei dati di Input\n","input_data = df[INPUT_COLUMN].tolist()\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","target_data = df[TARGET_COLUMN].tolist()\n","\n","\n","train_input_data = input_data[:TRAIN]\n","train_target_data = target_data[:TRAIN]\n","\n","validation_input_data = input_data[TRAIN:TRAIN+VALIDATION]\n","validation_target_data = target_data[TRAIN:TRAIN+VALIDATION]\n","\n","test_input_data = input_data[TRAIN+VALIDATION:]\n","test_target_data = target_data[TRAIN+VALIDATION:]\n","\n","print('-----------TRAIN SET--------------')\n","print(train_input_data[-4:])\n","print(train_target_data[-4:])\n","print('-----------VALIDATION SET---------------')\n","print(validation_input_data[-4:])\n","print(validation_target_data[-4:])\n","print('-----------TEST SET---------------')\n","print(test_input_data[-4:])\n","print(test_target_data[-4:])"],"metadata":{"id":"-K_qU8ouq5lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685170427,"user_tz":-60,"elapsed":4657,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"8222004e-de0e-4a79-dffc-204ac65d7254"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                                                   input  \\\n","20796         How did you two meet? \"It's a long story.\"   \n","20797  If you tell the truth, you don't have to remem...   \n","20798        Tom is young, rich, spoiled and egocentric.   \n","20799  Tom decided to give up skateboarding after his...   \n","\n","                                                  target  \n","20796  Come vi siete conosciute voi due? \"È una stori...  \n","20797       Se dice la verità, non deve ricordare nulla.  \n","20798      Tom è giovane, ricco, viziato ed egocentrico.  \n","20799  Tom decise di rinunciare ad andare in skateboa...   \n","\n","-----------TRAIN SET--------------\n","['He is, without question, the best man for the job.', 'I want to know how you got past the guards.', 'Tom saw something on the floor by the sofa.', 'It took a long time to accustom myself to the noise.']\n","[\"Lui è, senza dubbio, l'uomo migliore per il lavoro.\", 'Voglio sapere come hai superato le guardie.', 'Tom vide qualcosa sul pavimento accanto al divano.', 'Mi ci è voluto molto tempo per abituarmi al rumore.']\n","-----------TEST SET---------------\n","['How did you two meet? \"It\\'s a long story.\"', \"If you tell the truth, you don't have to remember anything.\", 'Tom is young, rich, spoiled and egocentric.', 'Tom decided to give up skateboarding after his accident.']\n","['Come vi siete conosciute voi due? \"È una storia lunga.\"', 'Se dice la verità, non deve ricordare nulla.', 'Tom è giovane, ricco, viziato ed egocentrico.', 'Tom decise di rinunciare ad andare in skateboard dopo il suo incidente.']\n"]}]},{"cell_type":"markdown","source":["### Analisi Dati"],"metadata":{"id":"KnDfwaOYa9fI"}},{"cell_type":"code","source":["print(f'Esempi nel Dataset di Train                            : {len(train_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Train        : {min(train_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Train       : {min(train_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Train        : {max(train_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Train       : {max(train_target_data, key = len)}')\n","print('---------------------------------------------------------------------------------------')\n","print(f'Esempi nel Dataset di Validation                       : {len(validation_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Validation   : {min(validation_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Validation  : {min(validation_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Validation   : {max(validation_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Validation  : {max(validation_target_data, key = len)}')\n","print('---------------------------------------------------------------------------------------')\n","print(f'Esempi nel Dataset di Test                             : {len(test_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Test         : {min(test_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Test        : {min(test_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Test         : {max(test_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Test        : {max(test_target_data, key = len)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YVgFmtSQdK-p","executionInfo":{"status":"ok","timestamp":1670685170428,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"5b78bd0c-9144-44ff-ab68-09ed40d80d4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Esempi nel Dataset di Train                       : 16000\n","Frase più corta in inglese nel Dataset di Train   : I'm not very good at this. \"Neither am I.\"\n","Frase più corta in italiano nel Dataset di Train  : Dove vai in vacanza?\n","Frase più lunga in inglese nel Dataset di Train   : The shoes were made of some soft stuff that looked like leather.\n","Frase più lunga in italiano nel Dataset di Train  : I lavoratori del settore dei trasporti organizzarono uno sciopero per protestare contro i tagli di paga.\n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Test                        : 4800\n","Frase più corta in inglese nel Dataset di Train   : How old is she? \"She is twelve years old.\"\n","Frase più corta in italiano nel Dataset di Train  : Deve aiutare Tom, lo sa.\n","Frase più lunga in inglese nel Dataset di Train   : I want to go to Australia once again before my passport expires.\n","Frase più lunga in italiano nel Dataset di Train  : I lavoratori del settore dei trasporti hanno organizzato uno sciopero per protestare contro i tagli di paga.\n"]}]},{"cell_type":"markdown","source":["### Tokenizer\n","\n","Carico il modello di tokenizer creato utilizzzando il set di dati a disposizione"],"metadata":{"id":"njyY9RWlFMWu"}},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n","dataset = dataset.shuffle(len(input_data)).batch(BATCH_SIZE, drop_remainder=True)"],"metadata":{"id":"c_YuEbcHjwvX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","    # The target vocabulary size\n","    vocab_size = MAX_VOCAB_SIZE,\n","    # Reserved tokens that must be included in the vocabulary\n","    reserved_tokens=reserved_tokens,\n","    # Arguments for `text.BertTokenizer`\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","    learn_params={},\n",")"],"metadata":{"id":"kUvYU-BkjwrA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_en = dataset.map(lambda en, it: en)\n","train_it = dataset.map(lambda en, it: it)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o-YrDiXOjwoK","executionInfo":{"status":"ok","timestamp":1670685173016,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"720bd382-2f80-436c-ee67-bb1ad53c8a85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["%%time\n","en_vocab = bert_vocab.bert_vocab_from_dataset(\n","    train_en.batch(10000).prefetch(2),\n","    **bert_vocab_args\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbWi5EM8jwlP","executionInfo":{"status":"ok","timestamp":1670685196643,"user_tz":-60,"elapsed":23633,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"4b30d508-bed5-47ff-9849-5a31ea0728a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 18.8 s, sys: 66.1 ms, total: 18.8 s\n","Wall time: 23.4 s\n"]}]},{"cell_type":"code","source":["%%time\n","it_vocab = bert_vocab.bert_vocab_from_dataset(\n","    train_it.batch(10000).prefetch(2),\n","    **bert_vocab_args\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vFZbI57IjwhZ","executionInfo":{"status":"ok","timestamp":1670685218491,"user_tz":-60,"elapsed":21865,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"3cea6965-c9b0-46c5-b412-a2d50b94ab42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 22.1 s, sys: 84.2 ms, total: 22.2 s\n","Wall time: 22.3 s\n"]}]},{"cell_type":"code","source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"],"metadata":{"id":"Ed69PBdKjweq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# VOCABOLARIO\n","vocab_folder = 'vocab'\n","en_vocab_finalname = 'en_vocab_1.txt'\n","it_vocab_finalname = 'it_vocab_1.txt'\n","\n","VOCAB_PATH = os.path.abspath(os.path.join(root_folder, vocab_folder))\n","en_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, en_vocab_finalname))\n","it_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, it_vocab_finalname))"],"metadata":{"id":"_zeykiwnkLeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["write_vocab_file(en_vocab_filenamepath, en_vocab)\n","write_vocab_file(it_vocab_filenamepath, it_vocab)"],"metadata":{"id":"fvw0IZ7jkCa9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["en_tokenizer = text.BertTokenizer(en_vocab_filenamepath, **bert_tokenizer_params)\n","it_tokenizer = text.BertTokenizer(it_vocab_filenamepath, **bert_tokenizer_params)"],"metadata":{"id":"QmOnTI15kCYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  x = tf.concat([starts, ragged, ends], axis=1)\n","  # x = keras.preprocessing.sequence.pad_sequences(x.numpy(), maxlen=MAX_SEQ_LENGTH, padding='post')\n","  return x"],"metadata":{"id":"8oGReoW6kCVv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result"],"metadata":{"id":"VjodYDstkCS2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","    \n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)"],"metadata":{"id":"ALs84-rOkCQF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizers = tf.Module()\n","tokenizers.en = CustomTokenizer(reserved_tokens, en_vocab_filenamepath)\n","tokenizers.it = CustomTokenizer(reserved_tokens, it_vocab_filenamepath)"],"metadata":{"id":"u0odE9Q9kCM6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f'Vocabolario Inglese  : {tokenizers.en.get_vocab_size()}')\n","print(f'Vocabolario Italiano : {tokenizers.it.get_vocab_size()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sGMiRqMrk34U","executionInfo":{"status":"ok","timestamp":1670685223157,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"93ab2c2f-5f18-4501-ad51-76d814aa644c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabolario Inglese  : 2119\n","Vocabolario Italiano : 2604\n"]}]},{"cell_type":"code","source":["# tokenizers = tf.saved_model.load(tokenizer_filenamepath)"],"metadata":{"id":"-4B-HWWcmsmz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(input_data[-1:])\n","print(tokenizers.en.tokenize(input_data[-1:]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize(input_data[-1:]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize(input_data[-1:])))\n","print('------------------------------------------------------------------')\n","print(target_data[-1:])\n","print(tokenizers.it.tokenize(target_data[-1:]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.it.tokenize(target_data[-1:]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.it.detokenize(tokenizers.it.tokenize(target_data[-1:])))"],"metadata":{"id":"rKEFeDdGFIGS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685224179,"user_tz":-60,"elapsed":1027,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"1201cc37-c47c-4607-9c29-f8ba0e9136e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Tom decided to give up skateboarding after his accident.']\n","<tf.RaggedTensor [[2, 54, 343, 52, 259, 139, 43, 1585, 801, 1054, 286, 1094, 121, 221, 72,\n","  317, 11, 3]]>\n","[[   2   54  343   52  259  139   43 1585  801 1054  286 1094  121  221\n","    72  317   11    3    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor([b'tom decided to give up skateboarding after his accident .'], shape=(1,), dtype=string)\n","------------------------------------------------------------------\n","['Tom decise di rinunciare ad andare in skateboard dopo il suo incidente.']\n","<tf.RaggedTensor [[2, 56, 28, 244, 1492, 53, 42, 2383, 760, 2396, 145, 95, 61, 43, 1865,\n","  104, 2594, 224, 301, 332, 682, 217, 57, 83, 348, 11, 3]]>\n","[[   2   56   28  244 1492   53   42 2383  760 2396  145   95   61   43\n","  1865  104 2594  224  301  332  682  217   57   83  348   11    3    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor([b'tom decise di rinunciare ad andare in skateboard dopo il suo incidente .'], shape=(1,), dtype=string)\n"]}]},{"cell_type":"code","source":["print([min(train_input_data, key = len)])\n","print(tokenizers.en.tokenize([min(train_input_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([min(train_input_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize([min(train_input_data, key = len)])))\n","print('------------------------------------------------------------------')\n","print([min(train_target_data, key = len)])\n","print(tokenizers.en.tokenize([min(train_target_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([min(train_target_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize([min(train_target_data, key = len)])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hkK4c0gUfH_c","executionInfo":{"status":"ok","timestamp":1670685224180,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"47662df5-8d14-483e-a820-7d72eb7e5c4b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['I\\'m not very good at this. \"Neither am I.\"']\n","<tf.RaggedTensor [[2, 33, 8, 37, 80, 116, 154, 78, 64, 11, 5, 1403, 246, 33, 11, 5, 3]]>\n","[[   2   33    8   37   80  116  154   78   64   11    5 1403  246   33\n","    11    5    3    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor([b'i \\' m not very good at this . \" neither am i . \"'], shape=(1,), dtype=string)\n","------------------------------------------------------------------\n","['Dove vai in vacanza?']\n","<tf.RaggedTensor [[2, 67, 1060, 46, 275, 591, 55, 46, 275, 2115, 422, 1998, 275, 24, 3]]>\n","[[   2   67 1060   46  275  591   55   46  275 2115  422 1998  275   24\n","     3    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor([b'dove vai in vacanza ?'], shape=(1,), dtype=string)\n"]}]},{"cell_type":"code","source":["print([max(train_input_data, key = len)])\n","print(tokenizers.en.tokenize([max(train_input_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([max(train_input_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize([max(train_input_data, key = len)])))\n","print('------------------------------------------------------------------')\n","print([max(train_target_data, key = len)])\n","print(tokenizers.en.tokenize([max(train_target_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([max(train_target_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize([max(train_target_data, key = len)])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6x7PMUugClF","executionInfo":{"status":"ok","timestamp":1670685224699,"user_tz":-60,"elapsed":524,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"2343a437-3f41-4abb-db90-5d82a734ef1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['The shoes were made of some soft stuff that looked like leather.']\n","<tf.RaggedTensor [[2, 51, 505, 107, 256, 56, 164, 115, 1817, 1359, 58, 471, 86, 36, 202,\n","  714, 1508, 11, 3]]>\n","[[   2   51  505  107  256   56  164  115 1817 1359   58  471   86   36\n","   202  714 1508   11    3    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor([b'the shoes were made of some soft stuff that looked like leather .'], shape=(1,), dtype=string)\n","------------------------------------------------------------------\n","['I lavoratori del settore dei trasporti organizzarono uno sciopero per protestare contro i tagli di paga.']\n","<tf.RaggedTensor [[2, 33, 36, 275, 2118, 429, 1673, 591, 28, 922, 1169, 203, 929, 28, 202,\n","  591, 44, 342, 570, 1247, 591, 159, 703, 422, 591, 1998, 1998, 578, 492,\n","  286, 45, 248, 286, 43, 2115, 591, 1971, 185, 286, 40, 185, 40, 342,\n","  1438, 590, 578, 202, 27, 492, 203, 342, 286, 33, 44, 275, 703, 491, 591,\n","  28, 591, 40, 275, 703, 275, 11, 3]]>\n","[[  36  275 2118  429 1673  591   28  922 1169  203  929   28  202  591\n","    44  342  570 1247  591  159  703  422  591 1998 1998  578  492  286\n","    45  248  286   43 2115  591 1971  185  286   40  185   40  342 1438\n","   590  578  202   27  492  203  342  286   33   44  275  703  491  591\n","    28  591   40  275  703  275   11    3]]\n","tf.Tensor([b'i lavoratori del settore dei trasporti organizzarono uno sciopero per protestare contro i tagli di paga .'], shape=(1,), dtype=string)\n"]}]},{"cell_type":"markdown","source":["### Creazione dataset\n","Utilizzo della libreria tf.data per la gestione del dataset da utilizzare.\n","Verranno creati batch di esempi che verranno utilizzati durante l'addestramento."],"metadata":{"id":"5QIDajkEsVU1"}},{"cell_type":"code","source":["def prepare_batch(en, it):\n","  zero = tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH], tf.int64)\n","  en = tokenizers.en.tokenize(en) # Output is ragged.\n","  en = tf.concat([en, zero], 1)\n","  en = en[:, :MAX_SEQ_LENGTH]     # Trim to MAX_TOKENS.\n","  en = en.to_tensor()             # Convert to 0-padded dense Tensor\n","\n","  it = tokenizers.it.tokenize(it)\n","  it_inputs = it[:, :-1].to_tensor()  # Drop the [END] tokens\n","  it_labels = it[:, 1:].to_tensor()   # Drop the [START] tokens\n","  \n","  it_inputs = tf.concat([it_inputs, zero], 1)\n","  it_inputs = it_inputs[:, :(MAX_SEQ_LENGTH)]\n","\n","  it_labels = tf.concat([it_labels, zero], 1)\n","  it_labels = it_labels[:, :(MAX_SEQ_LENGTH)]\n","\n","  return (en, it_inputs), it_labels"],"metadata":{"id":"ccH3jHoABPzV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_batches(ds):\n","  return (\n","      ds\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .map(prepare_batch, tf.data.AUTOTUNE)\n","      .prefetch(buffer_size=tf.data.AUTOTUNE))"],"metadata":{"id":"l_dswlCiBTdR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definizione del dataset\n","# [from_tensor_slices] permette di recuperare batch\n","# di esempi dai dataset di riferimento\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_target_data))\n","validation_dataset = tf.data.Dataset.from_tensor_slices((validation_input_data, validation_target_data))\n","\n","# impostazione del recupero di esempi presi in maniera\n","# casuale in gruppi di [BATCH_SIZE] tra quelli disponibili\n","train_dataset = make_batches(train_dataset)\n","validation_dataset = make_batches(validation_dataset)"],"metadata":{"id":"tktJ5YuIsYe3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for (en_input, it_input), it_target in train_dataset.take(1):\n","  print(f'Shape en input           : {en_input.shape}')\n","  print(f'Example en input         : {en_input[0]}')  \n","  print('-------------------------------------------------------')\n","  print(f'Shape it input           : {it_input.shape}')\n","  print(f'Example it input         : {it_input[0]}')  \n","  print(f'Shape it input           : {it_target.shape}')\n","  print(f'Example it target        : {it_target[0]}')  "],"metadata":{"id":"VH_aKPlV_AWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685227034,"user_tz":-60,"elapsed":496,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"63ade10d-52c3-4e16-e55f-a8f41edce423"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape en input           : (32, 64)\n","Example en input         : [  2  33   8  37 181  54  99  73 239  52 667 361  64 409  11   3   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0]\n","-------------------------------------------------------\n","Shape it input           : (32, 64)\n","Example it input         : [   2   65   63  282   54   56  236   61  245   53 2277   74  170   11\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n","Shape it input           : (32, 64)\n","Example it target        : [  65   63  282   54   56  236   61  245   53 2277   74  170   11    3\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n"]}]},{"cell_type":"markdown","source":["### Token and Position Embedding\n","\n","Implementazione del blocco Embedding per l'utilizzo di vettori posizionali insieme ai vettori di token di parole tramite estensione della classe Layer di Keras"],"metadata":{"id":"gAu1IXlRZzlq"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(layers.Layer):\n","  def __init__(self, maxlen, vocab_size, embed_dim):\n","    super(TokenAndPositionEmbedding, self).__init__()\n","    self.maxlen = maxlen\n","    self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","  def call(self, x, debug=False):\n","    x = keras.preprocessing.sequence.pad_sequences(x, maxlen=self.maxlen, padding='post')\n","    maxlen = tf.shape(x)[-1]\n","\n","    if debug:\n","      print('********** DEBUG TOKEN AND POSITION EMBEDDING ***********')\n","      print(f'Sequence Max len                          : {maxlen}')\n","      print(f'Sequence Shape                            : {tf.shape(x)}')\n","\n","    positions = tf.range(start=0, limit=maxlen, delta=1)\n","    positions = self.pos_emb(positions)\n","    x = self.token_emb(x)\n","    output = x + positions\n","\n","    if debug:\n","      print(f'Shape TokenAndPositionEmbedding           : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"o9-RSKTqsmUC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_position_en = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.en.get_vocab_size(), EMBEDDING_DIM)\n","token_position_it = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.it.get_vocab_size(), EMBEDDING_DIM)\n","\n","inputs_encoder = token_position_en(en_input, debug)\n","inputs_decoder = token_position_it(it_input, debug)"],"metadata":{"id":"rr_EWQUX8EWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685227035,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"06ba5d8e-fb9a-42d8-91e3-e3611fa4c0c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### Encoder\n","\n","Implmentazione di un blocco di EncoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"_iq7Y-d4eRd8"}},{"cell_type":"code","source":["class Encoder(layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='ENC'):\n","    super(Encoder, self).__init__()\n","    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","      [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization()\n","    self.layernorm2 = layers.LayerNormalization()\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, inputs, training=False, debug=False):\n","    attn_output = self.att(query=inputs,\n","                           value=inputs, \n","                           key=inputs)\n","    \n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(inputs + attn_output)\n","\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","\n","    output = self.layernorm2(out1 + ffn_output)\n","\n","    if debug:\n","      print('********************* DEBUG ENCODER *********************')\n","      print(f'Shape Input Layer Encoder       : {inputs.shape}')\n","      print(f'Shape Output Layer Encoder      : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"joTBTlWF8ETD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = Encoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_encoder = encoder(inputs=inputs_encoder,\n","                          training=training, \n","                          debug=debug)"],"metadata":{"id":"JaIzBxFCfKe9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685231394,"user_tz":-60,"elapsed":4365,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"e5d36983-8701-46be-8936-252af4c59d69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 64, 64)\n","Shape Output Layer Encoder      : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### Decoder\n","\n","Implementazione di un blocco di DecoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"grNE3Ww9e6Av"}},{"cell_type":"code","source":["class Decoder(layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DEC'):\n","    super(Decoder, self).__init__()\n","    self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","      [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization()\n","    self.layernorm2 = layers.LayerNormalization()\n","    self.layernorm3 = layers.LayerNormalization()\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","    self.dropout3 = layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, inputs, encoder_output, training=False, debug=False):\n","    attn_output1 = self.att1(query=inputs,\n","                             value=inputs, \n","                             key=inputs, \n","                             use_causal_mask=True)\n","    \n","    attn_output1 = self.dropout1(attn_output1)\n","    out1 = self.layernorm1(inputs + attn_output1)\n","\n","    attn_output2 = self.att2(key=encoder_output, \n","                             value=encoder_output, \n","                             query=out1)\n","    \n","    attn_output2 = self.dropout2(attn_output2, training=training)\n","    out2 = self.layernorm2(out1 + attn_output2)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","\n","    output = self.layernorm3(out2 + ffn_output)\n","\n","    if debug:\n","      print('******************* DEBUG DECODER ***********************')\n","      print(f'Input Shape                       : {inputs.shape}')\n","      print(f'Shape Outputs Decoder             : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"SO5rYsFpfFS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_decoder = decoder(inputs=inputs_decoder, \n","                          encoder_output=outputs_encoder,  \n","                          training=training,\n","                          debug=debug)"],"metadata":{"id":"yysVdkHH8EPH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685231395,"user_tz":-60,"elapsed":24,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"09d705cb-c8d9-45bb-da39-2099328d56de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 64, 64)\n","Shape Outputs Decoder             : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### Transformer\n","\n","Implementazione del blocco Transformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"ne4zTOG_NKfV"}},{"cell_type":"code","execution_count":null,"metadata":{"pycharm":{"name":"#%%\n"},"id":"lw2xMCAMC_4M"},"outputs":[],"source":["class TransformerBlock(keras.Model):\n","  def __init__(self, \n","               num_layers, \n","               embed_dim, \n","               num_heads, \n","               ff_dim, \n","               max_len,\n","               input_vocab_size,\n","               target_vocab_size,\n","               rate=0.5):\n","    \n","    super(TransformerBlock, self).__init__()\n","\n","    self.num_layers = num_layers\n","\n","    self.token_pos_enc = TokenAndPositionEmbedding(max_len, input_vocab_size, embed_dim)\n","    self.token_pos_dec = TokenAndPositionEmbedding(max_len, target_vocab_size, embed_dim)\n","\n","    self.encoder = [Encoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","    self.decoder = [Decoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","\n","    self.dropout = layers.Dropout(rate)\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","  def call(self, inputs, training=False, debug=False):\n","    inputs_encoder, inputs_decoder  = inputs\n","\n","    inputs_encoder = self.token_pos_enc(inputs_encoder, debug)\n","    inputs_decoder = self.token_pos_dec(inputs_decoder, debug)\n","\n","    if debug:\n","      print(f'---------------- DEBUG TRANSFORMER BLOCK ----------------')\n","      print(f'inputs_encoder       : {inputs_encoder.shape}')\n","      print(f'inputs_decoder       : {inputs_decoder.shape}')      \n","\n","    encoder_output = inputs_encoder\n","    transformer_output = inputs_decoder\n","\n","    for i in range(self.num_layers):\n","      encoder_output = self.encoder[i](inputs=encoder_output, \n","                                       training=training, \n","                                       debug=debug) \n","      \n","    for i in range(self.num_layers):\n","      transformer_output = self.decoder[i](inputs=transformer_output, \n","                                           encoder_output=encoder_output, \n","                                           training=training,\n","                                           debug=debug)\n","\n","    transformer_output = self.dropout(transformer_output)\n","    logits = self.final_layer(transformer_output)\n","\n","    if debug:\n","      print(f'Output Shape       : {logits.shape}')\n","      print(f'Output Transformer : {logits[0, :1, :12]}')    \n","      print(f'---------------------------------------------------------')\n","\n","    return logits"]},{"cell_type":"code","source":["transformer = TransformerBlock(NUM_LAYERS, \n","                               EMBEDDING_DIM, \n","                               NUM_HEADS, \n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.en.get_vocab_size(),\n","                               tokenizers.it.get_vocab_size(),\n","                               DROPUOT)\n","\n","transformer_output = transformer((en_input, it_input), \n","                                 training=training,\n","                                 debug=debug)"],"metadata":{"id":"pr--G0ZZVAMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685232654,"user_tz":-60,"elapsed":1276,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"23a1c76d-92d6-4241-9a08-488c9a4220aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n","---------------- DEBUG TRANSFORMER BLOCK ----------------\n","inputs_encoder       : (32, 64, 64)\n","inputs_decoder       : (32, 64, 64)\n","********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 64, 64)\n","Shape Output Layer Encoder      : (32, 64, 64)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 64, 64)\n","Shape Outputs Decoder             : (32, 64, 64)\n","*********************************************************\n","Output Shape       : (32, 64, 2604)\n","Output Transformer : [[-0.9368651  -0.17693846 -0.35368806 -0.14693826  0.11338224 -0.12913421\n","  -0.4269318  -0.0335535  -0.3041587   0.07293964  0.38176793 -0.76042503]]\n","---------------------------------------------------------\n"]}]},{"cell_type":"code","source":["transformer.summary()"],"metadata":{"id":"_kwqvJSu8liP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670685232654,"user_tz":-60,"elapsed":25,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"8119c351-5b23-4540-ef5c-2f274d91de6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer_block\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," token_and_position_embeddin  multiple                 139712    \n"," g_2 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," token_and_position_embeddin  multiple                 170752    \n"," g_3 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," ENC (Encoder)               multiple                  135056    \n","                                                                 \n"," DEC (Decoder)               multiple                  267856    \n","                                                                 \n"," dropout_13 (Dropout)        multiple                  0         \n","                                                                 \n"," dense_8 (Dense)             multiple                  169260    \n","                                                                 \n","=================================================================\n","Total params: 882,636\n","Trainable params: 882,636\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### Addestramento"],"metadata":{"id":"IFmcHTSDTvYk"}},{"cell_type":"code","source":["import json\n","\n","learning_rate = [3e-4]\n","beta_1 = [0.9, 0.75, 0.66]\n","beta_2 = [0.98, 0.99, 0.999]\n","\n","for lr in learning_rate:\n","  for b1 in beta_1:\n","    for b2 in beta_2:\n","      transformer = TransformerBlock(NUM_LAYERS, \n","                                EMBEDDING_DIM, \n","                                NUM_HEADS, \n","                                FF_DIM,\n","                                MAX_SEQ_LENGTH,\n","                                tokenizers.en.get_vocab_size(),\n","                                tokenizers.it.get_vocab_size(),\n","                                DROPUOT)\n","\n","      print('Parametri Addestramento AdamW : lr=' + str(lr) + ' b1=' + str(b1) + ', b2=' + str(b2))   \n","\n","      transformer.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                          optimizer=tf.keras.optimizers.experimental.AdamW(learning_rate=lr, beta_1=b1, beta_2=b2, epsilon=1e-9),\n","                          metrics=[keras.metrics.SparseCategoricalAccuracy()])\n","\n","      start = datetime.datetime.now()\n","      history = transformer.fit(train_dataset,\n","                                initial_epoch=0,\n","                                epochs=3,\n","                                shuffle=True,\n","                                validation_data=validation_dataset)\n","\n","      end = datetime.datetime.now()\n","      print(f'Tempo necessario per l\\'addestramento: {end - start}')\n"],"metadata":{"id":"7mtEEwtkQqxL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"713df6b0-2f88-4753-a336-8daa1eda1e1d","executionInfo":{"status":"ok","timestamp":1670690424189,"user_tz":-60,"elapsed":5170318,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parametri Addestramento AdamW : lr=0.0003 b1=0.9, b2=0.98\n","Epoch 1/3\n","500/500 [==============================] - 186s 371ms/step - loss: 2.5665 - sparse_categorical_accuracy: 0.7779 - val_loss: 1.1884 - val_sparse_categorical_accuracy: 0.8224\n","Epoch 2/3\n","500/500 [==============================] - 182s 363ms/step - loss: 1.1469 - sparse_categorical_accuracy: 0.8233 - val_loss: 1.0418 - val_sparse_categorical_accuracy: 0.8320\n","Epoch 3/3\n","500/500 [==============================] - 179s 357ms/step - loss: 1.0406 - sparse_categorical_accuracy: 0.8319 - val_loss: 0.9577 - val_sparse_categorical_accuracy: 0.8411\n","Tempo necessario per l'addestramento: 0:09:46.619769\n","Parametri Addestramento AdamW : lr=0.0003 b1=0.9, b2=0.99\n","Epoch 1/3\n","500/500 [==============================] - 185s 370ms/step - loss: 2.5180 - sparse_categorical_accuracy: 0.7831 - val_loss: 1.2036 - val_sparse_categorical_accuracy: 0.8234\n","Epoch 2/3\n","500/500 [==============================] - 182s 365ms/step - loss: 1.1589 - sparse_categorical_accuracy: 0.8235 - val_loss: 1.0475 - val_sparse_categorical_accuracy: 0.8326\n","Epoch 3/3\n","500/500 [==============================] - 173s 345ms/step - loss: 1.0410 - sparse_categorical_accuracy: 0.8321 - val_loss: 0.9567 - val_sparse_categorical_accuracy: 0.8403\n","Tempo necessario per l'addestramento: 0:10:06.412189\n","Parametri Addestramento AdamW : lr=0.0003 b1=0.9, b2=0.999\n","Epoch 1/3\n","500/500 [==============================] - 180s 359ms/step - loss: 2.5562 - sparse_categorical_accuracy: 0.7783 - val_loss: 1.2087 - val_sparse_categorical_accuracy: 0.8223\n","Epoch 2/3\n","500/500 [==============================] - 198s 397ms/step - loss: 1.1788 - sparse_categorical_accuracy: 0.8231 - val_loss: 1.0644 - val_sparse_categorical_accuracy: 0.8314\n","Epoch 3/3\n","500/500 [==============================] - 175s 349ms/step - loss: 1.0605 - sparse_categorical_accuracy: 0.8314 - val_loss: 0.9681 - val_sparse_categorical_accuracy: 0.8409\n","Tempo necessario per l'addestramento: 0:09:44.376987\n","Parametri Addestramento AdamW : lr=0.0003 b1=0.75, b2=0.98\n","Epoch 1/3\n","500/500 [==============================] - 180s 361ms/step - loss: 2.4703 - sparse_categorical_accuracy: 0.7835 - val_loss: 1.1835 - val_sparse_categorical_accuracy: 0.8220\n","Epoch 2/3\n","500/500 [==============================] - 196s 391ms/step - loss: 1.1437 - sparse_categorical_accuracy: 0.8235 - val_loss: 1.0387 - val_sparse_categorical_accuracy: 0.8330\n","Epoch 3/3\n","500/500 [==============================] - 174s 348ms/step - loss: 1.0369 - sparse_categorical_accuracy: 0.8325 - val_loss: 0.9539 - val_sparse_categorical_accuracy: 0.8411\n","Tempo necessario per l'addestramento: 0:09:11.600603\n","Parametri Addestramento AdamW : lr=0.0003 b1=0.75, b2=0.99\n","Epoch 1/3\n","500/500 [==============================] - 178s 356ms/step - loss: 2.4571 - sparse_categorical_accuracy: 0.7857 - val_loss: 1.1903 - val_sparse_categorical_accuracy: 0.8230\n","Epoch 2/3\n","500/500 [==============================] - 175s 351ms/step - loss: 1.1497 - sparse_categorical_accuracy: 0.8245 - val_loss: 1.0369 - val_sparse_categorical_accuracy: 0.8345\n","Epoch 3/3\n","500/500 [==============================] - 178s 356ms/step - loss: 1.0330 - sparse_categorical_accuracy: 0.8335 - val_loss: 0.9443 - val_sparse_categorical_accuracy: 0.8418\n","Tempo necessario per l'addestramento: 0:09:40.396950\n","Parametri Addestramento AdamW : lr=0.0003 b1=0.75, b2=0.999\n","Epoch 1/3\n","500/500 [==============================] - 176s 351ms/step - loss: 2.4868 - sparse_categorical_accuracy: 0.7840 - val_loss: 1.1984 - val_sparse_categorical_accuracy: 0.8238\n","Epoch 2/3\n","500/500 [==============================] - 174s 349ms/step - loss: 1.1679 - sparse_categorical_accuracy: 0.8248 - val_loss: 1.0508 - val_sparse_categorical_accuracy: 0.8339\n","Epoch 3/3\n","500/500 [==============================] - 172s 343ms/step - loss: 1.0491 - sparse_categorical_accuracy: 0.8333 - val_loss: 0.9566 - val_sparse_categorical_accuracy: 0.8424\n","Tempo necessario per l'addestramento: 0:09:10.445906\n","Parametri Addestramento AdamW : lr=0.0003 b1=0.66, b2=0.98\n","Epoch 1/3\n","500/500 [==============================] - 180s 360ms/step - loss: 2.4821 - sparse_categorical_accuracy: 0.7832 - val_loss: 1.1827 - val_sparse_categorical_accuracy: 0.8228\n","Epoch 2/3\n","500/500 [==============================] - 172s 344ms/step - loss: 1.1439 - sparse_categorical_accuracy: 0.8241 - val_loss: 1.0388 - val_sparse_categorical_accuracy: 0.8337\n","Epoch 3/3\n","500/500 [==============================] - 179s 359ms/step - loss: 1.0388 - sparse_categorical_accuracy: 0.8329 - val_loss: 0.9556 - val_sparse_categorical_accuracy: 0.8414\n","Tempo necessario per l'addestramento: 0:09:44.796433\n","Parametri Addestramento AdamW : lr=0.0003 b1=0.66, b2=0.99\n","Epoch 1/3\n","500/500 [==============================] - 173s 346ms/step - loss: 2.4982 - sparse_categorical_accuracy: 0.7837 - val_loss: 1.1889 - val_sparse_categorical_accuracy: 0.8246\n","Epoch 2/3\n","500/500 [==============================] - 177s 354ms/step - loss: 1.1515 - sparse_categorical_accuracy: 0.8243 - val_loss: 1.0428 - val_sparse_categorical_accuracy: 0.8322\n","Epoch 3/3\n","500/500 [==============================] - 174s 348ms/step - loss: 1.0356 - sparse_categorical_accuracy: 0.8329 - val_loss: 0.9478 - val_sparse_categorical_accuracy: 0.8424\n","Tempo necessario per l'addestramento: 0:09:38.606109\n","Parametri Addestramento AdamW : lr=0.0003 b1=0.66, b2=0.999\n","Epoch 1/3\n","500/500 [==============================] - 171s 342ms/step - loss: 2.4503 - sparse_categorical_accuracy: 0.7862 - val_loss: 1.2012 - val_sparse_categorical_accuracy: 0.8228\n","Epoch 2/3\n","500/500 [==============================] - 177s 354ms/step - loss: 1.1706 - sparse_categorical_accuracy: 0.8242 - val_loss: 1.0527 - val_sparse_categorical_accuracy: 0.8336\n","Epoch 3/3\n","500/500 [==============================] - 172s 344ms/step - loss: 1.0522 - sparse_categorical_accuracy: 0.8329 - val_loss: 0.9581 - val_sparse_categorical_accuracy: 0.8418\n","Tempo necessario per l'addestramento: 0:09:05.770945\n"]}]},{"cell_type":"code","source":["import json\n","\n","learning_rate = [1e-4]\n","beta_1 = [0.9, 0.75, 0.66]\n","beta_2 = [0.98, 0.99, 0.999]\n","\n","for lr in learning_rate:\n","  for b1 in beta_1:\n","    for b2 in beta_2:\n","      # learning_rate = CustomSchedule(EMBEDDING_DIM)\n","      \n","      transformer = TransformerBlock(NUM_LAYERS, \n","                                EMBEDDING_DIM, \n","                                NUM_HEADS, \n","                                FF_DIM,\n","                                MAX_SEQ_LENGTH,\n","                                tokenizers.en.get_vocab_size(),\n","                                tokenizers.it.get_vocab_size(),\n","                                DROPUOT)\n","\n","      print('Parametri Addestramento AdamW : lr=' + str(lr) + ' b1=' + str(b1) + ', b2=' + str(b2))   \n","\n","      transformer.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","                          optimizer=tf.keras.optimizers.experimental.AdamW(learning_rate=lr, beta_1=b1, beta_2=b2),\n","                          metrics=[keras.metrics.SparseCategoricalAccuracy()])\n","\n","      start = datetime.datetime.now()\n","      history = transformer.fit(train_dataset,\n","                                initial_epoch=0,\n","                                epochs=3,\n","                                shuffle=True,\n","                                validation_data=validation_dataset)\n","\n","      end = datetime.datetime.now()\n","      print(f'Tempo necessario per l\\'addestramento: {end - start}')\n"],"metadata":{"id":"sy7L21wmctAe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670695547916,"user_tz":-60,"elapsed":5123747,"user":{"displayName":"Daniele Badiali","userId":"14842026096794501907"}},"outputId":"0b1ae51f-e374-46a7-9d36-c468e9daaeda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parametri Addestramento AdamW : lr=0.0001 b1=0.9, b2=0.98\n","Epoch 1/3\n","500/500 [==============================] - 176s 352ms/step - loss: 4.6224 - sparse_categorical_accuracy: 0.7411 - val_loss: 2.0970 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 172s 344ms/step - loss: 1.6005 - sparse_categorical_accuracy: 0.7919 - val_loss: 1.2701 - val_sparse_categorical_accuracy: 0.8109\n","Epoch 3/3\n","500/500 [==============================] - 176s 353ms/step - loss: 1.2592 - sparse_categorical_accuracy: 0.8139 - val_loss: 1.1670 - val_sparse_categorical_accuracy: 0.8237\n","Tempo necessario per l'addestramento: 0:09:11.382883\n","Parametri Addestramento AdamW : lr=0.0001 b1=0.9, b2=0.99\n","Epoch 1/3\n","500/500 [==============================] - 171s 342ms/step - loss: 4.6712 - sparse_categorical_accuracy: 0.7409 - val_loss: 2.1400 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 178s 356ms/step - loss: 1.6377 - sparse_categorical_accuracy: 0.7937 - val_loss: 1.2834 - val_sparse_categorical_accuracy: 0.8115\n","Epoch 3/3\n","500/500 [==============================] - 168s 336ms/step - loss: 1.2711 - sparse_categorical_accuracy: 0.8138 - val_loss: 1.1706 - val_sparse_categorical_accuracy: 0.8234\n","Tempo necessario per l'addestramento: 0:09:35.958554\n","Parametri Addestramento AdamW : lr=0.0001 b1=0.9, b2=0.999\n","Epoch 1/3\n","500/500 [==============================] - 169s 337ms/step - loss: 4.4088 - sparse_categorical_accuracy: 0.7475 - val_loss: 1.9290 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 187s 373ms/step - loss: 1.6147 - sparse_categorical_accuracy: 0.7933 - val_loss: 1.3110 - val_sparse_categorical_accuracy: 0.8113\n","Epoch 3/3\n","500/500 [==============================] - 173s 347ms/step - loss: 1.3147 - sparse_categorical_accuracy: 0.8132 - val_loss: 1.1956 - val_sparse_categorical_accuracy: 0.8236\n","Tempo necessario per l'addestramento: 0:09:37.914336\n","Parametri Addestramento AdamW : lr=0.0001 b1=0.75, b2=0.98\n","Epoch 1/3\n","500/500 [==============================] - 180s 359ms/step - loss: 4.5033 - sparse_categorical_accuracy: 0.7447 - val_loss: 2.0235 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 182s 365ms/step - loss: 1.5789 - sparse_categorical_accuracy: 0.7946 - val_loss: 1.2659 - val_sparse_categorical_accuracy: 0.8104\n","Epoch 3/3\n","500/500 [==============================] - 197s 394ms/step - loss: 1.2547 - sparse_categorical_accuracy: 0.8134 - val_loss: 1.1658 - val_sparse_categorical_accuracy: 0.8226\n","Tempo necessario per l'addestramento: 0:09:25.278238\n","Parametri Addestramento AdamW : lr=0.0001 b1=0.75, b2=0.99\n","Epoch 1/3\n","500/500 [==============================] - 176s 350ms/step - loss: 4.4759 - sparse_categorical_accuracy: 0.7474 - val_loss: 2.0294 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 173s 346ms/step - loss: 1.6133 - sparse_categorical_accuracy: 0.7919 - val_loss: 1.2867 - val_sparse_categorical_accuracy: 0.8114\n","Epoch 3/3\n","500/500 [==============================] - 177s 354ms/step - loss: 1.2724 - sparse_categorical_accuracy: 0.8139 - val_loss: 1.1697 - val_sparse_categorical_accuracy: 0.8235\n","Tempo necessario per l'addestramento: 0:09:41.806849\n","Parametri Addestramento AdamW : lr=0.0001 b1=0.75, b2=0.999\n","Epoch 1/3\n","500/500 [==============================] - 180s 360ms/step - loss: 4.3837 - sparse_categorical_accuracy: 0.7472 - val_loss: 1.9434 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 180s 360ms/step - loss: 1.6234 - sparse_categorical_accuracy: 0.7917 - val_loss: 1.3158 - val_sparse_categorical_accuracy: 0.8098\n","Epoch 3/3\n","500/500 [==============================] - 176s 353ms/step - loss: 1.3177 - sparse_categorical_accuracy: 0.8140 - val_loss: 1.1968 - val_sparse_categorical_accuracy: 0.8244\n","Tempo necessario per l'addestramento: 0:09:19.196525\n","Parametri Addestramento AdamW : lr=0.0001 b1=0.66, b2=0.98\n","Epoch 1/3\n","500/500 [==============================] - 179s 358ms/step - loss: 4.6384 - sparse_categorical_accuracy: 0.7341 - val_loss: 2.1062 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 176s 351ms/step - loss: 1.6094 - sparse_categorical_accuracy: 0.7924 - val_loss: 1.2768 - val_sparse_categorical_accuracy: 0.8090\n","Epoch 3/3\n","500/500 [==============================] - 197s 394ms/step - loss: 1.2614 - sparse_categorical_accuracy: 0.8127 - val_loss: 1.1699 - val_sparse_categorical_accuracy: 0.8229\n","Tempo necessario per l'addestramento: 0:09:44.090651\n","Parametri Addestramento AdamW : lr=0.0001 b1=0.66, b2=0.99\n","Epoch 1/3\n","500/500 [==============================] - 174s 348ms/step - loss: 4.4774 - sparse_categorical_accuracy: 0.7462 - val_loss: 2.0062 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 183s 367ms/step - loss: 1.6026 - sparse_categorical_accuracy: 0.7928 - val_loss: 1.2808 - val_sparse_categorical_accuracy: 0.8103\n","Epoch 3/3\n","500/500 [==============================] - 185s 370ms/step - loss: 1.2705 - sparse_categorical_accuracy: 0.8137 - val_loss: 1.1732 - val_sparse_categorical_accuracy: 0.8228\n","Tempo necessario per l'addestramento: 0:09:31.120009\n","Parametri Addestramento AdamW : lr=0.0001 b1=0.66, b2=0.999\n","Epoch 1/3\n","500/500 [==============================] - 173s 346ms/step - loss: 4.6570 - sparse_categorical_accuracy: 0.7349 - val_loss: 2.0608 - val_sparse_categorical_accuracy: 0.7794\n","Epoch 2/3\n","500/500 [==============================] - 179s 357ms/step - loss: 1.6505 - sparse_categorical_accuracy: 0.7954 - val_loss: 1.3123 - val_sparse_categorical_accuracy: 0.8117\n","Epoch 3/3\n","500/500 [==============================] - 181s 361ms/step - loss: 1.3163 - sparse_categorical_accuracy: 0.8133 - val_loss: 1.1942 - val_sparse_categorical_accuracy: 0.8238\n","Tempo necessario per l'addestramento: 0:09:17.359658\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"collapsed_sections":["YJf4hjv4PMAJ"]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}